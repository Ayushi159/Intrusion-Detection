{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intrusion Detection System.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fGty1G3ylOSd"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all the datasets\n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\")\n",
        "df2=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
        "df3=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\")\n",
        "df4=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
        "df5=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
        "df6=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
        "df7=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
        "df8=pd.read_csv(\"/content/drive/MyDrive/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\")"
      ],
      "metadata": {
        "id": "y-ucsjECo4pS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeRvYG3gjpfs",
        "outputId": "e7722eec-822d-419a-a4a5-4ccdfacbac21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate all the datasets\n",
        "\n",
        "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8])"
      ],
      "metadata": {
        "id": "IUaqlIIPsH8E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "m0b4R0Y7sKsA",
        "outputId": "c932e86c-a41f-42d7-b2eb-16e9d70a82f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
              "0              49188               4                   2   \n",
              "1              49188               1                   2   \n",
              "2              49188               1                   2   \n",
              "3              49188               1                   2   \n",
              "4              49486               3                   2   \n",
              "\n",
              "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
              "0                        0                           12   \n",
              "1                        0                           12   \n",
              "2                        0                           12   \n",
              "3                        0                           12   \n",
              "4                        0                           12   \n",
              "\n",
              "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
              "0                             0                       6   \n",
              "1                             0                       6   \n",
              "2                             0                       6   \n",
              "3                             0                       6   \n",
              "4                             0                       6   \n",
              "\n",
              "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
              "0                       6                      6.0                     0.0   \n",
              "1                       6                      6.0                     0.0   \n",
              "2                       6                      6.0                     0.0   \n",
              "3                       6                      6.0                     0.0   \n",
              "4                       6                      6.0                     0.0   \n",
              "\n",
              "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
              "0  ...                     20          0.0          0.0            0   \n",
              "1  ...                     20          0.0          0.0            0   \n",
              "2  ...                     20          0.0          0.0            0   \n",
              "3  ...                     20          0.0          0.0            0   \n",
              "4  ...                     20          0.0          0.0            0   \n",
              "\n",
              "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
              "0            0        0.0        0.0          0          0  BENIGN  \n",
              "1            0        0.0        0.0          0          0  BENIGN  \n",
              "2            0        0.0        0.0          0          0  BENIGN  \n",
              "3            0        0.0        0.0          0          0  BENIGN  \n",
              "4            0        0.0        0.0          0          0  BENIGN  \n",
              "\n",
              "[5 rows x 79 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48ab62e3-1b91-4f51-8bb2-a9a028a2d2e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>...</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49188</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49486</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 79 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48ab62e3-1b91-4f51-8bb2-a9a028a2d2e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48ab62e3-1b91-4f51-8bb2-a9a028a2d2e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48ab62e3-1b91-4f51-8bb2-a9a028a2d2e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[' Label'].value_counts()  # there are 15 different classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO7XHzJFdNbU",
        "outputId": "cb8f4c09-2854-4438-d100-b1ebe217cac5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        2273097\n",
              "DoS Hulk                       231073\n",
              "PortScan                       158930\n",
              "DDoS                           128027\n",
              "DoS GoldenEye                   10293\n",
              "FTP-Patator                      7938\n",
              "SSH-Patator                      5897\n",
              "DoS slowloris                    5796\n",
              "DoS Slowhttptest                 5499\n",
              "Bot                              1966\n",
              "Web Attack � Brute Force         1507\n",
              "Web Attack � XSS                  652\n",
              "Infiltration                       36\n",
              "Web Attack � Sql Injection         21\n",
              "Heartbleed                         11\n",
              "Name:  Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp6gVBHrdWwj",
        "outputId": "d7fc552d-4512-4828-9198-11f0d6c98cb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " Destination Port              0\n",
              " Flow Duration                 0\n",
              " Total Fwd Packets             0\n",
              " Total Backward Packets        0\n",
              "Total Length of Fwd Packets    0\n",
              "                              ..\n",
              "Idle Mean                      0\n",
              " Idle Std                      0\n",
              " Idle Max                      0\n",
              " Idle Min                      0\n",
              " Label                         0\n",
              "Length: 79, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed some of the classes from dataset to have wide varity data samples.\n",
        "\n",
        "df.drop(df.loc[df[' Label']=='DDoS'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='PortScan'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='Bot'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='Web Attack � Brute Force'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='Web Attack � XSS'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='Web Attack � Sql Injection'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='FTP-Patator'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='DoS Slowhttptest'].index, inplace=True)\n",
        "df.drop(df.loc[df[' Label']=='DoS GoldenEye'].index, inplace=True)"
      ],
      "metadata": {
        "id": "3-Zf5YwEdwOt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[' Label'].value_counts()   # Using only 6 data classes"
      ],
      "metadata": {
        "id": "T1WrTvkngwA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54dbe61e-653b-46aa-8345-8fd2dbaf8b53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN           1170722\n",
              "DoS Hulk           70619\n",
              "SSH-Patator         2922\n",
              "DoS slowloris       2332\n",
              "Heartbleed            11\n",
              "Infiltration           4\n",
              "Name:  Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "classes = ['Benign', 'DoS Hulk', 'DoS Slowloris', 'Heartbleed', 'Infiltration', 'SSH-Patator']\n",
        "values = [1170772, 70619, 2332, 11, 4, 2992]\n",
        "\n",
        "px.bar(df, x=classes, y=values, labels={'x': 'Network Class Type', 'y':'No. of Samples'}, log_y=True, text=values, title='Training Data')"
      ],
      "metadata": {
        "id": "tAQB-r9QkHAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "dfce54a1-a0e2-42e9-862a-a8547d0d9b6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"6049e5eb-f77a-4961-bafc-311a2e677b5b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6049e5eb-f77a-4961-bafc-311a2e677b5b\")) {                    Plotly.newPlot(                        \"6049e5eb-f77a-4961-bafc-311a2e677b5b\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Network Class Type=%{x}<br>No. of Samples=%{y}<br>text=%{text}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[1170772.0,70619.0,2332.0,11.0,4.0,2992.0],\"textposition\":\"auto\",\"x\":[\"Benign\",\"DoS Hulk\",\"DoS Slowloris\",\"Heartbleed\",\"Infiltration\",\"SSH-Patator\"],\"xaxis\":\"x\",\"y\":[1170772,70619,2332,11,4,2992],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Network Class Type\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"No. of Samples\"},\"type\":\"log\"},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Training Data\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6049e5eb-f77a-4961-bafc-311a2e677b5b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
        "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
        "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
        "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
        "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
        "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
        "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
        "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
        "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
        "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
        "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
        "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
        "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
        "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
        "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
        "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
        "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
        "       ' Label']"
      ],
      "metadata": {
        "id": "LUBOaapwl-7s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPeAcJiygN7L",
        "outputId": "857ad32f-9f7d-4b41-f2cd-dfa352306b8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
              "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
              "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
              "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
              "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
              "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
              "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
              "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
              "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
              "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
              "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
              "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
              "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
              "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
              "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
              "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
              "       ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[features].copy()"
      ],
      "metadata": {
        "id": "Ag1As5RLo1zY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "Se_-pgSKpPTb",
        "outputId": "033dfd3b-6ff3-4dbf-cb26-743662640640"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Destination Port   Flow Duration   Total Fwd Packets  \\\n",
              "0                   49188               4                   2   \n",
              "1                   49188               1                   2   \n",
              "2                   49188               1                   2   \n",
              "3                   49188               1                   2   \n",
              "4                   49486               3                   2   \n",
              "...                   ...             ...                 ...   \n",
              "186641                443           32423                   2   \n",
              "186642                 80        35673258                   4   \n",
              "186643                 53           84941                   1   \n",
              "186644                 53           85421                   4   \n",
              "187031                443       116938572                  13   \n",
              "\n",
              "         Total Backward Packets  Total Length of Fwd Packets  \\\n",
              "0                             0                           12   \n",
              "1                             0                           12   \n",
              "2                             0                           12   \n",
              "3                             0                           12   \n",
              "4                             0                           12   \n",
              "...                         ...                          ...   \n",
              "186641                        2                           12   \n",
              "186642                        5                          209   \n",
              "186643                        1                           57   \n",
              "186644                        2                          184   \n",
              "187031                       12                          623   \n",
              "\n",
              "         Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
              "0                                  0                       6   \n",
              "1                                  0                       6   \n",
              "2                                  0                       6   \n",
              "3                                  0                       6   \n",
              "4                                  0                       6   \n",
              "...                              ...                     ...   \n",
              "186641                            12                       6   \n",
              "186642                          1656                     197   \n",
              "186643                           161                      57   \n",
              "186644                           300                      46   \n",
              "187031                          4083                     201   \n",
              "\n",
              "         Fwd Packet Length Min   Fwd Packet Length Mean  \\\n",
              "0                            6                 6.000000   \n",
              "1                            6                 6.000000   \n",
              "2                            6                 6.000000   \n",
              "3                            6                 6.000000   \n",
              "4                            6                 6.000000   \n",
              "...                        ...                      ...   \n",
              "186641                       6                 6.000000   \n",
              "186642                       0                52.250000   \n",
              "186643                      57                57.000000   \n",
              "186644                      46                46.000000   \n",
              "187031                       0                47.923077   \n",
              "\n",
              "         Fwd Packet Length Std  ...   min_seg_size_forward  Active Mean  \\\n",
              "0                     0.000000  ...                     20          0.0   \n",
              "1                     0.000000  ...                     20          0.0   \n",
              "2                     0.000000  ...                     20          0.0   \n",
              "3                     0.000000  ...                     20          0.0   \n",
              "4                     0.000000  ...                     20          0.0   \n",
              "...                        ...  ...                    ...          ...   \n",
              "186641                0.000000  ...                     20          0.0   \n",
              "186642               96.541442  ...                     20          0.0   \n",
              "186643                0.000000  ...                     32          0.0   \n",
              "186644                0.000000  ...                     32          0.0   \n",
              "187031               65.609275  ...                     20     168524.0   \n",
              "\n",
              "         Active Std   Active Max   Active Min   Idle Mean     Idle Std  \\\n",
              "0            0.0000            0            0         0.0       0.0000   \n",
              "1            0.0000            0            0         0.0       0.0000   \n",
              "2            0.0000            0            0         0.0       0.0000   \n",
              "3            0.0000            0            0         0.0       0.0000   \n",
              "4            0.0000            0            0         0.0       0.0000   \n",
              "...             ...          ...          ...         ...          ...   \n",
              "186641       0.0000            0            0         0.0       0.0000   \n",
              "186642       0.0000            0            0         0.0       0.0000   \n",
              "186643       0.0000            0            0         0.0       0.0000   \n",
              "186644       0.0000            0            0         0.0       0.0000   \n",
              "187031  106039.1471       243505        93543  58300000.0  408402.9565   \n",
              "\n",
              "         Idle Max   Idle Min   Label  \n",
              "0               0          0  BENIGN  \n",
              "1               0          0  BENIGN  \n",
              "2               0          0  BENIGN  \n",
              "3               0          0  BENIGN  \n",
              "4               0          0  BENIGN  \n",
              "...           ...        ...     ...  \n",
              "186641          0          0  BENIGN  \n",
              "186642          0          0  BENIGN  \n",
              "186643          0          0  BENIGN  \n",
              "186644          0          0  BENIGN  \n",
              "187031   58500000   58000000  BENIGN  \n",
              "\n",
              "[1246610 rows x 79 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a057079-6c88-4b83-8ebe-c5e691aa07d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>...</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49188</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49486</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186641</th>\n",
              "      <td>443</td>\n",
              "      <td>32423</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186642</th>\n",
              "      <td>80</td>\n",
              "      <td>35673258</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>209</td>\n",
              "      <td>1656</td>\n",
              "      <td>197</td>\n",
              "      <td>0</td>\n",
              "      <td>52.250000</td>\n",
              "      <td>96.541442</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186643</th>\n",
              "      <td>53</td>\n",
              "      <td>84941</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "      <td>161</td>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186644</th>\n",
              "      <td>53</td>\n",
              "      <td>85421</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>184</td>\n",
              "      <td>300</td>\n",
              "      <td>46</td>\n",
              "      <td>46</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187031</th>\n",
              "      <td>443</td>\n",
              "      <td>116938572</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>623</td>\n",
              "      <td>4083</td>\n",
              "      <td>201</td>\n",
              "      <td>0</td>\n",
              "      <td>47.923077</td>\n",
              "      <td>65.609275</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>168524.0</td>\n",
              "      <td>106039.1471</td>\n",
              "      <td>243505</td>\n",
              "      <td>93543</td>\n",
              "      <td>58300000.0</td>\n",
              "      <td>408402.9565</td>\n",
              "      <td>58500000</td>\n",
              "      <td>58000000</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1246610 rows × 79 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a057079-6c88-4b83-8ebe-c5e691aa07d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a057079-6c88-4b83-8ebe-c5e691aa07d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a057079-6c88-4b83-8ebe-c5e691aa07d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  \n",
        "# Dropping all the rows with nan values\n",
        "data.dropna(inplace=True)\n",
        "  \n",
        "# Printing df\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "bgd0hxmMjkWz",
        "outputId": "bdf5e8a0-021e-4a6c-80a2-f812c5ff05f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Destination Port   Flow Duration   Total Fwd Packets  \\\n",
              "0                   49188               4                   2   \n",
              "1                   49188               1                   2   \n",
              "2                   49188               1                   2   \n",
              "3                   49188               1                   2   \n",
              "4                   49486               3                   2   \n",
              "...                   ...             ...                 ...   \n",
              "186641                443           32423                   2   \n",
              "186642                 80        35673258                   4   \n",
              "186643                 53           84941                   1   \n",
              "186644                 53           85421                   4   \n",
              "187031                443       116938572                  13   \n",
              "\n",
              "         Total Backward Packets  Total Length of Fwd Packets  \\\n",
              "0                             0                           12   \n",
              "1                             0                           12   \n",
              "2                             0                           12   \n",
              "3                             0                           12   \n",
              "4                             0                           12   \n",
              "...                         ...                          ...   \n",
              "186641                        2                           12   \n",
              "186642                        5                          209   \n",
              "186643                        1                           57   \n",
              "186644                        2                          184   \n",
              "187031                       12                          623   \n",
              "\n",
              "         Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
              "0                                  0                       6   \n",
              "1                                  0                       6   \n",
              "2                                  0                       6   \n",
              "3                                  0                       6   \n",
              "4                                  0                       6   \n",
              "...                              ...                     ...   \n",
              "186641                            12                       6   \n",
              "186642                          1656                     197   \n",
              "186643                           161                      57   \n",
              "186644                           300                      46   \n",
              "187031                          4083                     201   \n",
              "\n",
              "         Fwd Packet Length Min   Fwd Packet Length Mean  \\\n",
              "0                            6                 6.000000   \n",
              "1                            6                 6.000000   \n",
              "2                            6                 6.000000   \n",
              "3                            6                 6.000000   \n",
              "4                            6                 6.000000   \n",
              "...                        ...                      ...   \n",
              "186641                       6                 6.000000   \n",
              "186642                       0                52.250000   \n",
              "186643                      57                57.000000   \n",
              "186644                      46                46.000000   \n",
              "187031                       0                47.923077   \n",
              "\n",
              "         Fwd Packet Length Std  ...   min_seg_size_forward  Active Mean  \\\n",
              "0                     0.000000  ...                     20          0.0   \n",
              "1                     0.000000  ...                     20          0.0   \n",
              "2                     0.000000  ...                     20          0.0   \n",
              "3                     0.000000  ...                     20          0.0   \n",
              "4                     0.000000  ...                     20          0.0   \n",
              "...                        ...  ...                    ...          ...   \n",
              "186641                0.000000  ...                     20          0.0   \n",
              "186642               96.541442  ...                     20          0.0   \n",
              "186643                0.000000  ...                     32          0.0   \n",
              "186644                0.000000  ...                     32          0.0   \n",
              "187031               65.609275  ...                     20     168524.0   \n",
              "\n",
              "         Active Std   Active Max   Active Min   Idle Mean     Idle Std  \\\n",
              "0            0.0000            0            0         0.0       0.0000   \n",
              "1            0.0000            0            0         0.0       0.0000   \n",
              "2            0.0000            0            0         0.0       0.0000   \n",
              "3            0.0000            0            0         0.0       0.0000   \n",
              "4            0.0000            0            0         0.0       0.0000   \n",
              "...             ...          ...          ...         ...          ...   \n",
              "186641       0.0000            0            0         0.0       0.0000   \n",
              "186642       0.0000            0            0         0.0       0.0000   \n",
              "186643       0.0000            0            0         0.0       0.0000   \n",
              "186644       0.0000            0            0         0.0       0.0000   \n",
              "187031  106039.1471       243505        93543  58300000.0  408402.9565   \n",
              "\n",
              "         Idle Max   Idle Min   Label  \n",
              "0               0          0  BENIGN  \n",
              "1               0          0  BENIGN  \n",
              "2               0          0  BENIGN  \n",
              "3               0          0  BENIGN  \n",
              "4               0          0  BENIGN  \n",
              "...           ...        ...     ...  \n",
              "186641          0          0  BENIGN  \n",
              "186642          0          0  BENIGN  \n",
              "186643          0          0  BENIGN  \n",
              "186644          0          0  BENIGN  \n",
              "187031   58500000   58000000  BENIGN  \n",
              "\n",
              "[1245362 rows x 79 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd3f4502-2c15-4cc7-a993-f58b5582110a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>...</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49188</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49188</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>49486</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186641</th>\n",
              "      <td>443</td>\n",
              "      <td>32423</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186642</th>\n",
              "      <td>80</td>\n",
              "      <td>35673258</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>209</td>\n",
              "      <td>1656</td>\n",
              "      <td>197</td>\n",
              "      <td>0</td>\n",
              "      <td>52.250000</td>\n",
              "      <td>96.541442</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186643</th>\n",
              "      <td>53</td>\n",
              "      <td>84941</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "      <td>161</td>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186644</th>\n",
              "      <td>53</td>\n",
              "      <td>85421</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>184</td>\n",
              "      <td>300</td>\n",
              "      <td>46</td>\n",
              "      <td>46</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187031</th>\n",
              "      <td>443</td>\n",
              "      <td>116938572</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>623</td>\n",
              "      <td>4083</td>\n",
              "      <td>201</td>\n",
              "      <td>0</td>\n",
              "      <td>47.923077</td>\n",
              "      <td>65.609275</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>168524.0</td>\n",
              "      <td>106039.1471</td>\n",
              "      <td>243505</td>\n",
              "      <td>93543</td>\n",
              "      <td>58300000.0</td>\n",
              "      <td>408402.9565</td>\n",
              "      <td>58500000</td>\n",
              "      <td>58000000</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1245362 rows × 79 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd3f4502-2c15-4cc7-a993-f58b5582110a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd3f4502-2c15-4cc7-a993-f58b5582110a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd3f4502-2c15-4cc7-a993-f58b5582110a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Labels=data.iloc[:,-1]"
      ],
      "metadata": {
        "id": "3N4zKr8G4WWm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features=data.iloc[:,:-1]"
      ],
      "metadata": {
        "id": "QXvuqPgk4Ysf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30RMER8E-mwF",
        "outputId": "bb37b660-567b-4175-ad10-f2886536d4f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X=np.array(Features)\n",
        "y=labelencoder_X.fit_transform(Labels)"
      ],
      "metadata": {
        "id": "C68q-bSM4kx9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset to train and test set"
      ],
      "metadata": {
        "id": "OnZds_mMyMYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# split into 70:30 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
      ],
      "metadata": {
        "id": "i1JwfSoiyFy4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "do9K8rpT_KRJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance"
      ],
      "metadata": {
        "id": "gf2eWGf1hpGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train, columns = Features.columns)\n"
      ],
      "metadata": {
        "id": "4Bdh4CqDrIEC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc=RandomForestClassifier();\n",
        "\n",
        "rfc.fit(X_train, y_train);\n",
        "\n",
        "# extract important features\n",
        "score = np.round(rfc.feature_importances_,3)\n",
        "importances = pd.DataFrame({'feature':X_train.columns,'importance':score})\n",
        "importances = importances.sort_values('importance',ascending=False).set_index('feature')"
      ],
      "metadata": {
        "id": "RaMh-n1lpbZH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot importances\n",
        "plt.rcParams['figure.figsize'] = (11, 4)\n",
        "importances.plot.bar();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "jpUONhfJg_AO",
        "outputId": "f3888143-f512-4c60-f7e9-852c40601c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 792x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "importances=importances.head(10)"
      ],
      "metadata": {
        "id": "x9Tulm7Ho4wV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances"
      ],
      "metadata": {
        "id": "75CH6LW74PbW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1e79d079-a263-416c-f555-5011aee6f16b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         importance\n",
              "feature                            \n",
              " Destination Port             0.084\n",
              " Bwd Packet Length Std        0.077\n",
              "Init_Win_bytes_forward        0.055\n",
              "Bwd Packet Length Max         0.043\n",
              " Bwd Packet Length Mean       0.041\n",
              " Avg Bwd Segment Size         0.039\n",
              " Packet Length Std            0.039\n",
              " Fwd IAT Std                  0.031\n",
              " Packet Length Mean           0.029\n",
              " Average Packet Size          0.028"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c30dfe5-25c2-4025-be6e-b14a67fb0ed7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feature</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Destination Port</th>\n",
              "      <td>0.084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <td>0.077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <td>0.043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <td>0.041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <td>0.039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Packet Length Std</th>\n",
              "      <td>0.039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <td>0.031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <td>0.029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Average Packet Size</th>\n",
              "      <td>0.028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c30dfe5-25c2-4025-be6e-b14a67fb0ed7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c30dfe5-25c2-4025-be6e-b14a67fb0ed7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c30dfe5-25c2-4025-be6e-b14a67fb0ed7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_final= [\" Destination Port\", \" Bwd Packet Length Std\",\"Init_Win_bytes_forward\", \"Bwd Packet Length Max\",\n",
        "                 \" Bwd Packet Length Mean\", \" Avg Bwd Segment Size\",\" Packet Length Std\",\" Fwd IAT Std\",\" Packet Length Mean\",\n",
        "                 \" Average Packet Size\",\" Label\"]"
      ],
      "metadata": {
        "id": "zj6hAx26ufO4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data[features_final].copy()"
      ],
      "metadata": {
        "id": "gvMFlGsxiMwy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Labels=dataset.iloc[:,-1]"
      ],
      "metadata": {
        "id": "u7p2Z_ksilZz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features=dataset.iloc[:,:-1]"
      ],
      "metadata": {
        "id": "QdHrasBni3wr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X = LabelEncoder()\n",
        "X=np.array(Features)\n",
        "y=labelencoder_X.fit_transform(Labels)"
      ],
      "metadata": {
        "id": "B-Wxvcyri4co"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# split into 70:30 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
      ],
      "metadata": {
        "id": "NQe2nyLJi9Qf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wxoLDsm_jAh8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding of categorical data\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "dataset[' Label'] = label_encoder.fit_transform(dataset[' Label'])"
      ],
      "metadata": {
        "id": "ETCL_zyRQjTk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benign : 0\n",
        "# DoS Hulk : 1\n",
        "# DoS Slowloris :2\n",
        "# Heartbleed: 3\n",
        "# Infiltration: 4\n",
        "# SSH-Patator: 5\n",
        "\n",
        "dataset[' Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IgL5xk9RfxF",
        "outputId": "01813c15-75b6-4911-e106-ad184ff7000d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1169791\n",
              "1      70302\n",
              "5       2922\n",
              "2       2332\n",
              "3         11\n",
              "4          4\n",
              "Name:  Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the data in csv file for deep learning models\n",
        "\n",
        "dataset.to_csv(r'drive/My Drive/data.csv', index = False)"
      ],
      "metadata": {
        "id": "qB8l0X9vSh9u"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# data must be in dataframe to apply function, so convert\n",
        "y_train = pd.DataFrame(y_train)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# apply function\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "N9pDv_guYA35"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb4hg6A3Ydvk",
        "outputId": "803bf075-1295-45ce-a147-bd49297f550c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(373609, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8MpeUDJkrhV",
        "outputId": "2d715b02-dcc4-46aa-9843-adb35d034140"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(373609, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the classifier model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "#def baseline_model():\n",
        "  # create model\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=10, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Ed682t2mYrFZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 1024, epochs = 25, verbose = 2, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoSyQTyeZ7qZ",
        "outputId": "d0f690a7-ebbb-4b1a-eecc-6762cbbd944e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "682/682 - 4s - loss: 0.2797 - accuracy: 0.9485 - val_loss: 0.0982 - val_accuracy: 0.9686 - 4s/epoch - 5ms/step\n",
            "Epoch 2/25\n",
            "682/682 - 2s - loss: 0.0875 - accuracy: 0.9690 - val_loss: 0.0802 - val_accuracy: 0.9687 - 2s/epoch - 3ms/step\n",
            "Epoch 3/25\n",
            "682/682 - 3s - loss: 0.0767 - accuracy: 0.9693 - val_loss: 0.0735 - val_accuracy: 0.9690 - 3s/epoch - 4ms/step\n",
            "Epoch 4/25\n",
            "682/682 - 2s - loss: 0.0715 - accuracy: 0.9694 - val_loss: 0.0691 - val_accuracy: 0.9689 - 2s/epoch - 4ms/step\n",
            "Epoch 5/25\n",
            "682/682 - 2s - loss: 0.0676 - accuracy: 0.9694 - val_loss: 0.0645 - val_accuracy: 0.9690 - 2s/epoch - 2ms/step\n",
            "Epoch 6/25\n",
            "682/682 - 2s - loss: 0.0638 - accuracy: 0.9700 - val_loss: 0.0613 - val_accuracy: 0.9704 - 2s/epoch - 2ms/step\n",
            "Epoch 7/25\n",
            "682/682 - 2s - loss: 0.0606 - accuracy: 0.9728 - val_loss: 0.0590 - val_accuracy: 0.9792 - 2s/epoch - 2ms/step\n",
            "Epoch 8/25\n",
            "682/682 - 2s - loss: 0.0576 - accuracy: 0.9746 - val_loss: 0.0565 - val_accuracy: 0.9707 - 2s/epoch - 2ms/step\n",
            "Epoch 9/25\n",
            "682/682 - 2s - loss: 0.0551 - accuracy: 0.9756 - val_loss: 0.0531 - val_accuracy: 0.9786 - 2s/epoch - 2ms/step\n",
            "Epoch 10/25\n",
            "682/682 - 2s - loss: 0.0528 - accuracy: 0.9765 - val_loss: 0.0512 - val_accuracy: 0.9795 - 2s/epoch - 2ms/step\n",
            "Epoch 11/25\n",
            "682/682 - 2s - loss: 0.0510 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9790 - 2s/epoch - 2ms/step\n",
            "Epoch 12/25\n",
            "682/682 - 2s - loss: 0.0491 - accuracy: 0.9762 - val_loss: 0.0477 - val_accuracy: 0.9726 - 2s/epoch - 2ms/step\n",
            "Epoch 13/25\n",
            "682/682 - 2s - loss: 0.0472 - accuracy: 0.9776 - val_loss: 0.0459 - val_accuracy: 0.9790 - 2s/epoch - 2ms/step\n",
            "Epoch 14/25\n",
            "682/682 - 2s - loss: 0.0462 - accuracy: 0.9775 - val_loss: 0.0457 - val_accuracy: 0.9793 - 2s/epoch - 2ms/step\n",
            "Epoch 15/25\n",
            "682/682 - 2s - loss: 0.0450 - accuracy: 0.9785 - val_loss: 0.0440 - val_accuracy: 0.9796 - 2s/epoch - 2ms/step\n",
            "Epoch 16/25\n",
            "682/682 - 2s - loss: 0.0440 - accuracy: 0.9792 - val_loss: 0.0468 - val_accuracy: 0.9711 - 2s/epoch - 2ms/step\n",
            "Epoch 17/25\n",
            "682/682 - 2s - loss: 0.0432 - accuracy: 0.9796 - val_loss: 0.0450 - val_accuracy: 0.9689 - 2s/epoch - 2ms/step\n",
            "Epoch 18/25\n",
            "682/682 - 2s - loss: 0.0425 - accuracy: 0.9803 - val_loss: 0.0419 - val_accuracy: 0.9825 - 2s/epoch - 2ms/step\n",
            "Epoch 19/25\n",
            "682/682 - 2s - loss: 0.0417 - accuracy: 0.9807 - val_loss: 0.0422 - val_accuracy: 0.9824 - 2s/epoch - 2ms/step\n",
            "Epoch 20/25\n",
            "682/682 - 2s - loss: 0.0415 - accuracy: 0.9809 - val_loss: 0.0443 - val_accuracy: 0.9691 - 2s/epoch - 2ms/step\n",
            "Epoch 21/25\n",
            "682/682 - 2s - loss: 0.0410 - accuracy: 0.9814 - val_loss: 0.0399 - val_accuracy: 0.9833 - 2s/epoch - 2ms/step\n",
            "Epoch 22/25\n",
            "682/682 - 2s - loss: 0.0404 - accuracy: 0.9819 - val_loss: 0.0399 - val_accuracy: 0.9830 - 2s/epoch - 2ms/step\n",
            "Epoch 23/25\n",
            "682/682 - 2s - loss: 0.0402 - accuracy: 0.9818 - val_loss: 0.0393 - val_accuracy: 0.9835 - 2s/epoch - 2ms/step\n",
            "Epoch 24/25\n",
            "682/682 - 2s - loss: 0.0398 - accuracy: 0.9823 - val_loss: 0.0396 - val_accuracy: 0.9811 - 2s/epoch - 2ms/step\n",
            "Epoch 25/25\n",
            "682/682 - 2s - loss: 0.0395 - accuracy: 0.9824 - val_loss: 0.0392 - val_accuracy: 0.9823 - 2s/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_test, y_test,verbose=2, batch_size= 256)\n",
        "\n",
        "print('test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEKyG_zlZ_Yw",
        "outputId": "0ac53b40-339d-48be-fafc-d693698238d9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1460/1460 - 2s - loss: 0.0393 - accuracy: 0.9819 - 2s/epoch - 1ms/step\n",
            "test accuracy: 0.9818981885910034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_probs= model.predict(X_test)\n",
        "yhat_classes= np.argmax(yhat_probs, axis=1)"
      ],
      "metadata": {
        "id": "0ICZT91Zaafd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_yhat_classes = pd.DataFrame(yhat_classes)\n",
        "df_yhat_classes.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRZ15x_nbKyA",
        "outputId": "48fa4e81-64c3-4bf5-8f35-e5c4dfb066cd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    346757\n",
              "1     26157\n",
              "5       433\n",
              "2       254\n",
              "3         8\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_rounded = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "Kdyxsx3KbQ66"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test_rounded, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "precision = precision_score(y_test_rounded, yhat_classes, average='weighted', zero_division=0)\n",
        "print('Precision: %f' % precision)\n",
        "recall = recall_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('Recall: %f' % recall)\n",
        "f1 = f1_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4NYc2TibYr9",
        "outputId": "c2d4694b-9fe3-4118-f583-9b05b8b8ab8a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.981898\n",
            "Precision: 0.984629\n",
            "Recall: 0.981898\n",
            "F1 score: 0.982251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "# confusion matrix\n",
        "CM = confusion_matrix(y_test_rounded, yhat_classes)\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(6, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "Rz3CYYwNbcna",
        "outputId": "d9d64e45-c2ab-4c61-a7e2-2059905ea692"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing GAN model**"
      ],
      "metadata": {
        "id": "Dw7xyEb-gnBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data.csv')"
      ],
      "metadata": {
        "id": "HZJ6kUKEd_G1"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfk18Q_Bf83a",
        "outputId": "807b5807-2871-4b33-c601-13a65aa26042"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
              "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
              "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
              "       ' Packet Length Mean', ' Average Packet Size', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[' Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmwUdb6RmRnM",
        "outputId": "009b9cd1-6a92-4f1f-f429-d347303f0f6e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1169791\n",
              "1      70302\n",
              "5       2922\n",
              "2       2332\n",
              "3         11\n",
              "4          4\n",
              "Name:  Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_benign = df.loc[df[' Label']==0]\n",
        "df_dosHulk = df.loc[df[' Label']==1]\n",
        "df_sshPatator = df.loc[df[' Label']==2]\n",
        "df_dosSlowloris = df.loc[df[' Label']==3]\n",
        "df_heartbleed = df.loc[df[' Label']==4]\n",
        "df_infiltration = df.loc[df[' Label']==5]"
      ],
      "metadata": {
        "id": "xSuTHZPEmUPd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_benign = df_benign.drop([' Label'], axis=1)\n",
        "df_dosHulk = df_dosHulk.drop([' Label'], axis=1)\n",
        "df_sshPatator = df_sshPatator.drop([' Label'], axis=1)\n",
        "df_dosSlowloris = df_dosSlowloris.drop([' Label'], axis=1)\n",
        "df_heartbleed = df_heartbleed.drop([' Label'], axis=1)\n",
        "df_infiltration = df_infiltration.drop([' Label'], axis=1)"
      ],
      "metadata": {
        "id": "_91D3N_NmvBz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
        "\n",
        "pwt_benign = pw.fit_transform(df_benign[df_benign.columns])\n",
        "pwt_dosHulk = pw.fit_transform(df_dosHulk[df_dosHulk.columns])\n",
        "pwt_sshPatator = pw.fit_transform(df_sshPatator[df_sshPatator.columns])\n",
        "pwt_dosSlowloris = pw.fit_transform(df_dosSlowloris[df_dosSlowloris.columns])\n",
        "pwt_heartbleed = pw.fit_transform(df_heartbleed[df_heartbleed.columns])\n",
        "pwt_infiltration = pw.fit_transform(df_infiltration[df_infiltration.columns])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da2D67Esm1HU",
        "outputId": "dbd7c3e7-7a35-4f75-af94-4ea8f20b0b9b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pwt_benign = pd.DataFrame(pwt_benign)\n",
        "df_pwt_dosHulk = pd.DataFrame(pwt_dosHulk)\n",
        "df_pwt_sshPatator = pd.DataFrame(pwt_sshPatator)\n",
        "df_pwt_dosSlowloris = pd.DataFrame(pwt_dosSlowloris)\n",
        "df_pwt_heartbleed = pd.DataFrame(pwt_heartbleed)\n",
        "df_pwt_infiltration = pd.DataFrame(pwt_infiltration)"
      ],
      "metadata": {
        "id": "w3fScpOknN5F"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pwt_benign.columns = [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']\n",
        "\n",
        "df_pwt_dosHulk.columns = [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']\n",
        "\n",
        "df_pwt_sshPatator.columns =[' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']\n",
        "\n",
        "df_pwt_dosSlowloris.columns = [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']\n",
        "\n",
        "df_pwt_heartbleed.columns = [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']\n",
        "\n",
        "df_pwt_infiltration.columns = [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']"
      ],
      "metadata": {
        "id": "Aaj-g1QKn2Hm"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pwt_benign.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "woo_7Z5tn6XD",
        "outputId": "64a119c6-cf89-479b-9f4c-4b8b07ef98e7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Destination Port   Bwd Packet Length Std  Init_Win_bytes_forward  \\\n",
              "0           1.647164               -0.489545                0.722691   \n",
              "1           1.647164               -0.489545                0.722691   \n",
              "2           1.647164               -0.489545                0.722691   \n",
              "3           1.647164               -0.489545                0.722691   \n",
              "4           1.647736               -0.489545                0.667633   \n",
              "\n",
              "   Bwd Packet Length Max   Bwd Packet Length Mean   Avg Bwd Segment Size  \\\n",
              "0              -1.421426                -1.432506              -1.432506   \n",
              "1              -1.421426                -1.432506              -1.432506   \n",
              "2              -1.421426                -1.432506              -1.432506   \n",
              "3              -1.421426                -1.432506              -1.432506   \n",
              "4              -1.421426                -1.432506              -1.432506   \n",
              "\n",
              "    Packet Length Std   Fwd IAT Std   Packet Length Mean   Average Packet Size  \n",
              "0           -1.364039     -0.610658            -0.945836             -0.899631  \n",
              "1           -1.364039     -0.610658            -0.945836             -0.899631  \n",
              "2           -1.364039     -0.610658            -0.945836             -0.899631  \n",
              "3           -1.364039     -0.610658            -0.945836             -0.899631  \n",
              "4           -1.364039     -0.610658            -0.945836             -0.899631  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-883f688b-2e97-42d3-a737-dc79ef9e0f90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Average Packet Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.647164</td>\n",
              "      <td>-0.489545</td>\n",
              "      <td>0.722691</td>\n",
              "      <td>-1.421426</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.364039</td>\n",
              "      <td>-0.610658</td>\n",
              "      <td>-0.945836</td>\n",
              "      <td>-0.899631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.647164</td>\n",
              "      <td>-0.489545</td>\n",
              "      <td>0.722691</td>\n",
              "      <td>-1.421426</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.364039</td>\n",
              "      <td>-0.610658</td>\n",
              "      <td>-0.945836</td>\n",
              "      <td>-0.899631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.647164</td>\n",
              "      <td>-0.489545</td>\n",
              "      <td>0.722691</td>\n",
              "      <td>-1.421426</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.364039</td>\n",
              "      <td>-0.610658</td>\n",
              "      <td>-0.945836</td>\n",
              "      <td>-0.899631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.647164</td>\n",
              "      <td>-0.489545</td>\n",
              "      <td>0.722691</td>\n",
              "      <td>-1.421426</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.364039</td>\n",
              "      <td>-0.610658</td>\n",
              "      <td>-0.945836</td>\n",
              "      <td>-0.899631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.647736</td>\n",
              "      <td>-0.489545</td>\n",
              "      <td>0.667633</td>\n",
              "      <td>-1.421426</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.432506</td>\n",
              "      <td>-1.364039</td>\n",
              "      <td>-0.610658</td>\n",
              "      <td>-0.945836</td>\n",
              "      <td>-0.899631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-883f688b-2e97-42d3-a737-dc79ef9e0f90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-883f688b-2e97-42d3-a737-dc79ef9e0f90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-883f688b-2e97-42d3-a737-dc79ef9e0f90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class GAN():\n",
        "    \n",
        "    def __init__(self, gan_args):\n",
        "        [self.batch_size, lr, self.noise_dim,\n",
        "         self.data_dim, layers_dim] = gan_args\n",
        "\n",
        "        self.generator = Generator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
        "\n",
        "        self.discriminator = Discriminator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
        "\n",
        "        optimizer = Adam(lr, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # The generator takes noise as input and generates samples\n",
        "        z = Input(shape=(self.noise_dim,))\n",
        "        record = self.generator(z)\n",
        "\n",
        "        # For the combined model only the generator is trained\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated samples as input and determines validity\n",
        "        validity = self.discriminator(record)\n",
        "\n",
        "        # The combined model \n",
        "        # Trains the generator to try and create samples that will fool the discriminator \n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def get_data_batch(self, train, batch_size, seed=0):\n",
        "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
        "        # np.random.seed(seed)\n",
        "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
        "        # iterate through shuffled indices, so every sample gets covered evenly\n",
        "\n",
        "        start_i = (batch_size * seed) % len(train)\n",
        "        stop_i = start_i + batch_size\n",
        "        shuffle_seed = (batch_size * seed) // len(train)\n",
        "        np.random.seed(shuffle_seed)\n",
        "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
        "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
        "        x = train.loc[train_ix[start_i: stop_i]].values\n",
        "        return np.reshape(x, (batch_size, -1))\n",
        "        \n",
        "    def train(self, data, train_arguments):\n",
        "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
        "        \n",
        "        data_cols = data.columns\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):    \n",
        "            # Discriminator\n",
        "            batch_data = self.get_data_batch(data, self.batch_size)\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "\n",
        "            # Generate a batch of new samples\n",
        "            gen_data = self.generator.predict(noise)\n",
        "    \n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "            # Generator\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    \n",
        "    \n",
        "class Generator():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "    def build_model(self, input_shape, dim, data_dim):\n",
        "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim, activation='relu')(input)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dense(dim * 4, activation='relu')(x)\n",
        "        x = Dense(data_dim)(x)\n",
        "        return Model(inputs=input, outputs=x)\n",
        "\n",
        "class Discriminator():\n",
        "    def __init__(self,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "    \n",
        "    def build_model(self, input_shape, dim):\n",
        "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim * 4, activation='relu')(input)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim, activation='relu')(x)\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "        return Model(inputs=input, outputs=x)"
      ],
      "metadata": {
        "id": "TCcpiULsn8B8"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_columns = df_pwt_benign.columns\n",
        "df_pwt_benign[data_columns] = df_pwt_benign[data_columns]\n",
        "\n",
        "# set generator parametes\n",
        "batch_size = 512\n",
        "learning_rate=5e-4\n",
        "noise_shape=32\n",
        "input_shape=10\n",
        "dim=128\n",
        " \n",
        "# set training parameters \n",
        "epochs = 2500\n",
        "log_step = 100\n",
        "\n",
        "# assign to variable\n",
        "generator_parameters = [batch_size, learning_rate, noise_shape, input_shape, dim]\n",
        "training_parameters = ['', epochs, log_step]"
      ],
      "metadata": {
        "id": "nir1hM3QoCeP"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "synthesizer = GAN(generator_parameters)\n",
        "# change passed in dataframe to select which class is to be generated\n",
        "synthesizer.train(df_pwt_benign, training_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g8m1qo-oGhw",
        "outputId": "feb80734-2f22-4189-e8f3-14a1665d66d1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (512, 32) for input KerasTensor(type_spec=TensorSpec(shape=(512, 32), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (32, 32).\n",
            "0 [D loss: 0.692708, acc.: 37.60%] [G loss: 0.677189]\n",
            "1 [D loss: 0.638763, acc.: 50.00%] [G loss: 0.670299]\n",
            "2 [D loss: 0.606341, acc.: 50.00%] [G loss: 0.659803]\n",
            "3 [D loss: 0.589169, acc.: 50.00%] [G loss: 0.647253]\n",
            "4 [D loss: 0.598647, acc.: 50.00%] [G loss: 0.626264]\n",
            "5 [D loss: 0.607471, acc.: 50.00%] [G loss: 0.646748]\n",
            "6 [D loss: 0.582687, acc.: 50.00%] [G loss: 0.739607]\n",
            "7 [D loss: 0.533651, acc.: 57.91%] [G loss: 0.888309]\n",
            "8 [D loss: 0.480016, acc.: 88.87%] [G loss: 1.039231]\n",
            "9 [D loss: 0.448799, acc.: 94.14%] [G loss: 1.143333]\n",
            "10 [D loss: 0.455264, acc.: 91.50%] [G loss: 1.130863]\n",
            "11 [D loss: 0.475950, acc.: 84.57%] [G loss: 1.091550]\n",
            "12 [D loss: 0.495551, acc.: 80.08%] [G loss: 1.070535]\n",
            "13 [D loss: 0.504811, acc.: 78.61%] [G loss: 1.075399]\n",
            "14 [D loss: 0.507321, acc.: 83.59%] [G loss: 1.111097]\n",
            "15 [D loss: 0.499245, acc.: 81.64%] [G loss: 1.137988]\n",
            "16 [D loss: 0.486224, acc.: 79.00%] [G loss: 1.123188]\n",
            "17 [D loss: 0.482578, acc.: 80.66%] [G loss: 1.070949]\n",
            "18 [D loss: 0.472773, acc.: 86.23%] [G loss: 1.021147]\n",
            "19 [D loss: 0.462384, acc.: 92.19%] [G loss: 0.989107]\n",
            "20 [D loss: 0.448514, acc.: 94.82%] [G loss: 0.964046]\n",
            "21 [D loss: 0.435078, acc.: 95.61%] [G loss: 0.952218]\n",
            "22 [D loss: 0.428730, acc.: 92.97%] [G loss: 0.938444]\n",
            "23 [D loss: 0.450781, acc.: 83.30%] [G loss: 0.923092]\n",
            "24 [D loss: 0.492783, acc.: 72.36%] [G loss: 0.926140]\n",
            "25 [D loss: 0.486622, acc.: 73.34%] [G loss: 1.033850]\n",
            "26 [D loss: 0.477264, acc.: 83.20%] [G loss: 1.141103]\n",
            "27 [D loss: 0.475568, acc.: 87.99%] [G loss: 1.173559]\n",
            "28 [D loss: 0.449145, acc.: 92.29%] [G loss: 1.181239]\n",
            "29 [D loss: 0.424897, acc.: 92.19%] [G loss: 1.202730]\n",
            "30 [D loss: 0.392609, acc.: 94.53%] [G loss: 1.251450]\n",
            "31 [D loss: 0.384079, acc.: 94.73%] [G loss: 1.257250]\n",
            "32 [D loss: 0.388516, acc.: 94.24%] [G loss: 1.235496]\n",
            "33 [D loss: 0.399482, acc.: 87.99%] [G loss: 1.209670]\n",
            "34 [D loss: 0.436217, acc.: 80.57%] [G loss: 1.148842]\n",
            "35 [D loss: 0.448365, acc.: 76.86%] [G loss: 1.130432]\n",
            "36 [D loss: 0.438938, acc.: 78.91%] [G loss: 1.175810]\n",
            "37 [D loss: 0.376592, acc.: 89.94%] [G loss: 1.329379]\n",
            "38 [D loss: 0.345679, acc.: 93.36%] [G loss: 1.459617]\n",
            "39 [D loss: 0.354331, acc.: 90.92%] [G loss: 1.478135]\n",
            "40 [D loss: 0.439927, acc.: 79.59%] [G loss: 1.479627]\n",
            "41 [D loss: 0.516887, acc.: 73.14%] [G loss: 1.456181]\n",
            "42 [D loss: 0.585344, acc.: 67.87%] [G loss: 1.559289]\n",
            "43 [D loss: 0.615339, acc.: 70.41%] [G loss: 1.598444]\n",
            "44 [D loss: 0.647384, acc.: 72.95%] [G loss: 1.568564]\n",
            "45 [D loss: 0.632427, acc.: 74.90%] [G loss: 1.537018]\n",
            "46 [D loss: 0.574165, acc.: 74.90%] [G loss: 1.526686]\n",
            "47 [D loss: 0.525983, acc.: 74.90%] [G loss: 1.443505]\n",
            "48 [D loss: 0.478207, acc.: 74.90%] [G loss: 1.328294]\n",
            "49 [D loss: 0.444865, acc.: 75.59%] [G loss: 1.222245]\n",
            "50 [D loss: 0.425958, acc.: 85.16%] [G loss: 1.131409]\n",
            "51 [D loss: 0.416658, acc.: 89.84%] [G loss: 1.095543]\n",
            "52 [D loss: 0.411830, acc.: 90.33%] [G loss: 1.077474]\n",
            "53 [D loss: 0.420329, acc.: 88.77%] [G loss: 1.066758]\n",
            "54 [D loss: 0.437069, acc.: 86.13%] [G loss: 1.066265]\n",
            "55 [D loss: 0.460485, acc.: 80.18%] [G loss: 1.072056]\n",
            "56 [D loss: 0.494889, acc.: 69.14%] [G loss: 1.094800]\n",
            "57 [D loss: 0.526683, acc.: 65.72%] [G loss: 1.114720]\n",
            "58 [D loss: 0.554881, acc.: 67.58%] [G loss: 1.189197]\n",
            "59 [D loss: 0.553848, acc.: 73.93%] [G loss: 1.284348]\n",
            "60 [D loss: 0.539481, acc.: 76.27%] [G loss: 1.377124]\n",
            "61 [D loss: 0.523389, acc.: 76.95%] [G loss: 1.409630]\n",
            "62 [D loss: 0.508624, acc.: 76.95%] [G loss: 1.403643]\n",
            "63 [D loss: 0.490782, acc.: 77.15%] [G loss: 1.352812]\n",
            "64 [D loss: 0.472521, acc.: 77.15%] [G loss: 1.295799]\n",
            "65 [D loss: 0.458003, acc.: 77.15%] [G loss: 1.241189]\n",
            "66 [D loss: 0.453240, acc.: 78.22%] [G loss: 1.197314]\n",
            "67 [D loss: 0.444841, acc.: 79.39%] [G loss: 1.165495]\n",
            "68 [D loss: 0.446428, acc.: 79.10%] [G loss: 1.149998]\n",
            "69 [D loss: 0.455104, acc.: 78.22%] [G loss: 1.163779]\n",
            "70 [D loss: 0.465466, acc.: 76.86%] [G loss: 1.170408]\n",
            "71 [D loss: 0.471732, acc.: 76.37%] [G loss: 1.191097]\n",
            "72 [D loss: 0.485975, acc.: 76.37%] [G loss: 1.176311]\n",
            "73 [D loss: 0.508224, acc.: 73.44%] [G loss: 1.148250]\n",
            "74 [D loss: 0.542204, acc.: 70.51%] [G loss: 1.113249]\n",
            "75 [D loss: 0.564350, acc.: 69.14%] [G loss: 1.061588]\n",
            "76 [D loss: 0.591425, acc.: 67.58%] [G loss: 0.997454]\n",
            "77 [D loss: 0.644684, acc.: 63.38%] [G loss: 1.035918]\n",
            "78 [D loss: 0.584278, acc.: 65.04%] [G loss: 1.187169]\n",
            "79 [D loss: 0.607437, acc.: 65.92%] [G loss: 1.254177]\n",
            "80 [D loss: 0.655878, acc.: 60.55%] [G loss: 1.211800]\n",
            "81 [D loss: 0.660758, acc.: 58.50%] [G loss: 1.147244]\n",
            "82 [D loss: 0.702882, acc.: 55.66%] [G loss: 1.143386]\n",
            "83 [D loss: 0.705682, acc.: 53.03%] [G loss: 1.165762]\n",
            "84 [D loss: 0.685057, acc.: 53.81%] [G loss: 1.162335]\n",
            "85 [D loss: 0.661643, acc.: 54.98%] [G loss: 1.172048]\n",
            "86 [D loss: 0.622667, acc.: 57.81%] [G loss: 1.186798]\n",
            "87 [D loss: 0.616338, acc.: 59.08%] [G loss: 1.182121]\n",
            "88 [D loss: 0.612303, acc.: 59.57%] [G loss: 1.190344]\n",
            "89 [D loss: 0.625370, acc.: 61.43%] [G loss: 1.149054]\n",
            "90 [D loss: 0.656555, acc.: 59.57%] [G loss: 1.099479]\n",
            "91 [D loss: 0.679008, acc.: 57.81%] [G loss: 1.124410]\n",
            "92 [D loss: 0.672676, acc.: 61.91%] [G loss: 1.132396]\n",
            "93 [D loss: 0.664099, acc.: 63.18%] [G loss: 1.112196]\n",
            "94 [D loss: 0.634673, acc.: 64.26%] [G loss: 1.112372]\n",
            "95 [D loss: 0.590722, acc.: 65.04%] [G loss: 1.206757]\n",
            "96 [D loss: 0.541647, acc.: 69.82%] [G loss: 1.286820]\n",
            "97 [D loss: 0.534508, acc.: 71.19%] [G loss: 1.289928]\n",
            "98 [D loss: 0.533868, acc.: 71.29%] [G loss: 1.275779]\n",
            "99 [D loss: 0.562108, acc.: 69.53%] [G loss: 1.227664]\n",
            "100 [D loss: 0.574999, acc.: 69.92%] [G loss: 1.200544]\n",
            "101 [D loss: 0.621561, acc.: 65.92%] [G loss: 1.170571]\n",
            "102 [D loss: 0.624191, acc.: 64.16%] [G loss: 1.151542]\n",
            "103 [D loss: 0.643719, acc.: 61.43%] [G loss: 1.138333]\n",
            "104 [D loss: 0.641654, acc.: 59.57%] [G loss: 1.128538]\n",
            "105 [D loss: 0.636961, acc.: 59.18%] [G loss: 1.118649]\n",
            "106 [D loss: 0.630318, acc.: 59.28%] [G loss: 1.153278]\n",
            "107 [D loss: 0.601088, acc.: 62.11%] [G loss: 1.188211]\n",
            "108 [D loss: 0.589478, acc.: 62.60%] [G loss: 1.183577]\n",
            "109 [D loss: 0.562439, acc.: 64.16%] [G loss: 1.177313]\n",
            "110 [D loss: 0.557448, acc.: 65.04%] [G loss: 1.139633]\n",
            "111 [D loss: 0.544135, acc.: 66.70%] [G loss: 1.113249]\n",
            "112 [D loss: 0.537763, acc.: 69.04%] [G loss: 1.102659]\n",
            "113 [D loss: 0.533070, acc.: 70.80%] [G loss: 1.086457]\n",
            "114 [D loss: 0.539006, acc.: 71.88%] [G loss: 1.027245]\n",
            "115 [D loss: 0.554479, acc.: 72.36%] [G loss: 0.987656]\n",
            "116 [D loss: 0.572402, acc.: 69.82%] [G loss: 0.960237]\n",
            "117 [D loss: 0.582631, acc.: 68.16%] [G loss: 0.972329]\n",
            "118 [D loss: 0.590252, acc.: 66.80%] [G loss: 0.981997]\n",
            "119 [D loss: 0.612401, acc.: 64.06%] [G loss: 0.959008]\n",
            "120 [D loss: 0.622140, acc.: 63.28%] [G loss: 0.949280]\n",
            "121 [D loss: 0.626907, acc.: 63.87%] [G loss: 0.953502]\n",
            "122 [D loss: 0.616991, acc.: 63.38%] [G loss: 0.978703]\n",
            "123 [D loss: 0.614110, acc.: 63.87%] [G loss: 1.007909]\n",
            "124 [D loss: 0.600086, acc.: 64.06%] [G loss: 1.047374]\n",
            "125 [D loss: 0.595736, acc.: 64.45%] [G loss: 1.097339]\n",
            "126 [D loss: 0.580950, acc.: 66.41%] [G loss: 1.112900]\n",
            "127 [D loss: 0.565775, acc.: 69.73%] [G loss: 1.118665]\n",
            "128 [D loss: 0.560074, acc.: 70.51%] [G loss: 1.093334]\n",
            "129 [D loss: 0.600619, acc.: 62.79%] [G loss: 1.104137]\n",
            "130 [D loss: 0.597849, acc.: 63.77%] [G loss: 1.141689]\n",
            "131 [D loss: 0.594662, acc.: 66.02%] [G loss: 1.127956]\n",
            "132 [D loss: 0.605546, acc.: 64.26%] [G loss: 1.144410]\n",
            "133 [D loss: 0.595864, acc.: 66.02%] [G loss: 1.166875]\n",
            "134 [D loss: 0.584673, acc.: 65.82%] [G loss: 1.177014]\n",
            "135 [D loss: 0.573822, acc.: 66.31%] [G loss: 1.193985]\n",
            "136 [D loss: 0.557005, acc.: 66.89%] [G loss: 1.204310]\n",
            "137 [D loss: 0.539534, acc.: 68.36%] [G loss: 1.223798]\n",
            "138 [D loss: 0.513519, acc.: 68.65%] [G loss: 1.235091]\n",
            "139 [D loss: 0.495847, acc.: 70.12%] [G loss: 1.262682]\n",
            "140 [D loss: 0.473177, acc.: 71.68%] [G loss: 1.275681]\n",
            "141 [D loss: 0.463473, acc.: 78.52%] [G loss: 1.295645]\n",
            "142 [D loss: 0.487302, acc.: 81.64%] [G loss: 1.313762]\n",
            "143 [D loss: 0.505461, acc.: 83.40%] [G loss: 1.361480]\n",
            "144 [D loss: 0.507807, acc.: 82.32%] [G loss: 1.456608]\n",
            "145 [D loss: 0.616212, acc.: 75.20%] [G loss: 1.493306]\n",
            "146 [D loss: 0.726918, acc.: 67.29%] [G loss: 1.504095]\n",
            "147 [D loss: 0.578535, acc.: 76.56%] [G loss: 1.769080]\n",
            "148 [D loss: 0.608807, acc.: 72.07%] [G loss: 1.550490]\n",
            "149 [D loss: 0.583018, acc.: 72.75%] [G loss: 1.543110]\n",
            "150 [D loss: 0.602327, acc.: 69.43%] [G loss: 1.526921]\n",
            "151 [D loss: 0.612941, acc.: 68.36%] [G loss: 1.471426]\n",
            "152 [D loss: 0.593385, acc.: 71.19%] [G loss: 1.422130]\n",
            "153 [D loss: 0.572663, acc.: 75.88%] [G loss: 1.402859]\n",
            "154 [D loss: 0.552921, acc.: 77.25%] [G loss: 1.388445]\n",
            "155 [D loss: 0.540299, acc.: 76.56%] [G loss: 1.464434]\n",
            "156 [D loss: 0.521491, acc.: 77.83%] [G loss: 1.428187]\n",
            "157 [D loss: 0.512314, acc.: 78.52%] [G loss: 1.411479]\n",
            "158 [D loss: 0.526774, acc.: 77.93%] [G loss: 1.490113]\n",
            "159 [D loss: 0.524102, acc.: 76.76%] [G loss: 1.492267]\n",
            "160 [D loss: 0.511667, acc.: 77.34%] [G loss: 1.579429]\n",
            "161 [D loss: 0.522177, acc.: 74.90%] [G loss: 1.631819]\n",
            "162 [D loss: 0.480557, acc.: 79.39%] [G loss: 1.728825]\n",
            "163 [D loss: 0.459382, acc.: 80.18%] [G loss: 1.853480]\n",
            "164 [D loss: 0.475147, acc.: 80.27%] [G loss: 1.859483]\n",
            "165 [D loss: 0.491282, acc.: 79.30%] [G loss: 1.738981]\n",
            "166 [D loss: 0.496417, acc.: 80.18%] [G loss: 1.669883]\n",
            "167 [D loss: 0.503398, acc.: 81.25%] [G loss: 1.501870]\n",
            "168 [D loss: 0.546723, acc.: 78.91%] [G loss: 1.367708]\n",
            "169 [D loss: 0.604363, acc.: 72.27%] [G loss: 1.219094]\n",
            "170 [D loss: 0.678827, acc.: 61.52%] [G loss: 1.146996]\n",
            "171 [D loss: 0.698819, acc.: 56.54%] [G loss: 1.182441]\n",
            "172 [D loss: 0.638571, acc.: 60.94%] [G loss: 1.349214]\n",
            "173 [D loss: 0.569855, acc.: 69.73%] [G loss: 1.492788]\n",
            "174 [D loss: 0.552471, acc.: 69.63%] [G loss: 1.562551]\n",
            "175 [D loss: 0.586915, acc.: 68.26%] [G loss: 1.601843]\n",
            "176 [D loss: 0.670394, acc.: 67.48%] [G loss: 1.628864]\n",
            "177 [D loss: 0.678793, acc.: 57.13%] [G loss: 1.614868]\n",
            "178 [D loss: 0.752627, acc.: 57.42%] [G loss: 1.437381]\n",
            "179 [D loss: 0.740934, acc.: 63.87%] [G loss: 1.449478]\n",
            "180 [D loss: 0.726140, acc.: 63.48%] [G loss: 1.385219]\n",
            "181 [D loss: 0.700896, acc.: 64.06%] [G loss: 1.322432]\n",
            "182 [D loss: 0.681603, acc.: 61.13%] [G loss: 1.243740]\n",
            "183 [D loss: 0.650584, acc.: 61.82%] [G loss: 1.171826]\n",
            "184 [D loss: 0.655511, acc.: 60.35%] [G loss: 1.168820]\n",
            "185 [D loss: 0.638296, acc.: 60.94%] [G loss: 1.142425]\n",
            "186 [D loss: 0.634165, acc.: 60.74%] [G loss: 1.110888]\n",
            "187 [D loss: 0.631452, acc.: 61.33%] [G loss: 1.112192]\n",
            "188 [D loss: 0.611232, acc.: 62.21%] [G loss: 1.121231]\n",
            "189 [D loss: 0.604521, acc.: 62.11%] [G loss: 1.130678]\n",
            "190 [D loss: 0.597954, acc.: 62.21%] [G loss: 1.125363]\n",
            "191 [D loss: 0.590672, acc.: 62.89%] [G loss: 1.105836]\n",
            "192 [D loss: 0.587976, acc.: 63.87%] [G loss: 1.102160]\n",
            "193 [D loss: 0.584313, acc.: 64.94%] [G loss: 1.096884]\n",
            "194 [D loss: 0.586042, acc.: 65.33%] [G loss: 1.099283]\n",
            "195 [D loss: 0.588614, acc.: 66.50%] [G loss: 1.073744]\n",
            "196 [D loss: 0.592506, acc.: 66.50%] [G loss: 1.056679]\n",
            "197 [D loss: 0.595781, acc.: 66.99%] [G loss: 1.051139]\n",
            "198 [D loss: 0.597372, acc.: 66.31%] [G loss: 1.019281]\n",
            "199 [D loss: 0.604177, acc.: 64.36%] [G loss: 1.006403]\n",
            "200 [D loss: 0.600740, acc.: 64.55%] [G loss: 0.994205]\n",
            "201 [D loss: 0.608571, acc.: 61.62%] [G loss: 0.974818]\n",
            "202 [D loss: 0.610441, acc.: 60.55%] [G loss: 0.970194]\n",
            "203 [D loss: 0.599266, acc.: 62.79%] [G loss: 0.967660]\n",
            "204 [D loss: 0.599484, acc.: 61.62%] [G loss: 0.956058]\n",
            "205 [D loss: 0.589102, acc.: 62.89%] [G loss: 0.987488]\n",
            "206 [D loss: 0.575827, acc.: 65.92%] [G loss: 1.001191]\n",
            "207 [D loss: 0.569543, acc.: 66.50%] [G loss: 1.007298]\n",
            "208 [D loss: 0.564312, acc.: 66.80%] [G loss: 1.011986]\n",
            "209 [D loss: 0.566468, acc.: 66.80%] [G loss: 0.993149]\n",
            "210 [D loss: 0.569521, acc.: 67.77%] [G loss: 0.992417]\n",
            "211 [D loss: 0.564862, acc.: 67.58%] [G loss: 0.991125]\n",
            "212 [D loss: 0.567280, acc.: 68.16%] [G loss: 0.985380]\n",
            "213 [D loss: 0.566331, acc.: 68.16%] [G loss: 1.002322]\n",
            "214 [D loss: 0.566767, acc.: 68.55%] [G loss: 1.013548]\n",
            "215 [D loss: 0.570143, acc.: 67.48%] [G loss: 1.020899]\n",
            "216 [D loss: 0.570963, acc.: 68.26%] [G loss: 1.016423]\n",
            "217 [D loss: 0.567278, acc.: 68.16%] [G loss: 1.007272]\n",
            "218 [D loss: 0.572401, acc.: 67.68%] [G loss: 1.008868]\n",
            "219 [D loss: 0.574787, acc.: 67.29%] [G loss: 1.022736]\n",
            "220 [D loss: 0.571544, acc.: 67.29%] [G loss: 1.006601]\n",
            "221 [D loss: 0.569538, acc.: 67.48%] [G loss: 0.999604]\n",
            "222 [D loss: 0.576856, acc.: 66.21%] [G loss: 1.003689]\n",
            "223 [D loss: 0.567040, acc.: 67.19%] [G loss: 1.005129]\n",
            "224 [D loss: 0.571521, acc.: 66.80%] [G loss: 1.002665]\n",
            "225 [D loss: 0.565608, acc.: 67.19%] [G loss: 0.992241]\n",
            "226 [D loss: 0.565079, acc.: 67.19%] [G loss: 0.988707]\n",
            "227 [D loss: 0.561569, acc.: 67.87%] [G loss: 0.982960]\n",
            "228 [D loss: 0.553315, acc.: 69.24%] [G loss: 0.985472]\n",
            "229 [D loss: 0.547844, acc.: 69.24%] [G loss: 0.987209]\n",
            "230 [D loss: 0.545265, acc.: 70.41%] [G loss: 0.997455]\n",
            "231 [D loss: 0.542274, acc.: 70.80%] [G loss: 0.998149]\n",
            "232 [D loss: 0.541033, acc.: 72.17%] [G loss: 1.001358]\n",
            "233 [D loss: 0.537759, acc.: 72.56%] [G loss: 1.006392]\n",
            "234 [D loss: 0.534994, acc.: 73.83%] [G loss: 1.009739]\n",
            "235 [D loss: 0.536257, acc.: 73.63%] [G loss: 1.016305]\n",
            "236 [D loss: 0.539170, acc.: 74.02%] [G loss: 1.023817]\n",
            "237 [D loss: 0.542543, acc.: 72.17%] [G loss: 1.044779]\n",
            "238 [D loss: 0.544357, acc.: 70.80%] [G loss: 1.050291]\n",
            "239 [D loss: 0.560474, acc.: 67.09%] [G loss: 1.064520]\n",
            "240 [D loss: 0.581613, acc.: 64.36%] [G loss: 1.043720]\n",
            "241 [D loss: 0.595385, acc.: 61.91%] [G loss: 1.067997]\n",
            "242 [D loss: 0.604172, acc.: 60.35%] [G loss: 1.061354]\n",
            "243 [D loss: 0.616607, acc.: 60.45%] [G loss: 1.035816]\n",
            "244 [D loss: 0.631990, acc.: 59.08%] [G loss: 1.022635]\n",
            "245 [D loss: 0.642110, acc.: 57.03%] [G loss: 1.021198]\n",
            "246 [D loss: 0.657535, acc.: 56.93%] [G loss: 1.043135]\n",
            "247 [D loss: 0.649593, acc.: 57.23%] [G loss: 1.097426]\n",
            "248 [D loss: 0.611291, acc.: 60.94%] [G loss: 1.201820]\n",
            "249 [D loss: 0.588218, acc.: 63.67%] [G loss: 1.315455]\n",
            "250 [D loss: 0.568482, acc.: 66.89%] [G loss: 1.459062]\n",
            "251 [D loss: 0.564676, acc.: 66.11%] [G loss: 1.613903]\n",
            "252 [D loss: 0.605372, acc.: 61.72%] [G loss: 1.814046]\n",
            "253 [D loss: 0.664393, acc.: 57.13%] [G loss: 1.954917]\n",
            "254 [D loss: 0.781280, acc.: 55.96%] [G loss: 1.736454]\n",
            "255 [D loss: 0.668557, acc.: 65.04%] [G loss: 1.747718]\n",
            "256 [D loss: 0.515231, acc.: 82.13%] [G loss: 1.739330]\n",
            "257 [D loss: 0.446735, acc.: 89.84%] [G loss: 1.555908]\n",
            "258 [D loss: 0.488042, acc.: 82.81%] [G loss: 1.272225]\n",
            "259 [D loss: 0.561031, acc.: 71.68%] [G loss: 1.125000]\n",
            "260 [D loss: 0.632251, acc.: 63.18%] [G loss: 0.933165]\n",
            "261 [D loss: 0.644638, acc.: 61.62%] [G loss: 0.871035]\n",
            "262 [D loss: 0.607663, acc.: 55.96%] [G loss: 1.033113]\n",
            "263 [D loss: 0.535290, acc.: 71.97%] [G loss: 1.273708]\n",
            "264 [D loss: 0.513708, acc.: 70.61%] [G loss: 1.395377]\n",
            "265 [D loss: 0.507680, acc.: 71.78%] [G loss: 1.413880]\n",
            "266 [D loss: 0.509035, acc.: 71.88%] [G loss: 1.366549]\n",
            "267 [D loss: 0.500788, acc.: 72.46%] [G loss: 1.302335]\n",
            "268 [D loss: 0.491663, acc.: 73.83%] [G loss: 1.229718]\n",
            "269 [D loss: 0.492481, acc.: 73.05%] [G loss: 1.196792]\n",
            "270 [D loss: 0.492983, acc.: 72.75%] [G loss: 1.179525]\n",
            "271 [D loss: 0.501192, acc.: 73.44%] [G loss: 1.163162]\n",
            "272 [D loss: 0.502170, acc.: 73.54%] [G loss: 1.138304]\n",
            "273 [D loss: 0.521433, acc.: 72.95%] [G loss: 1.125826]\n",
            "274 [D loss: 0.521151, acc.: 73.54%] [G loss: 1.126400]\n",
            "275 [D loss: 0.530621, acc.: 72.36%] [G loss: 1.135251]\n",
            "276 [D loss: 0.529934, acc.: 72.56%] [G loss: 1.188311]\n",
            "277 [D loss: 0.529113, acc.: 73.14%] [G loss: 1.296563]\n",
            "278 [D loss: 0.537865, acc.: 72.27%] [G loss: 1.269200]\n",
            "279 [D loss: 0.554066, acc.: 70.70%] [G loss: 1.334445]\n",
            "280 [D loss: 0.569467, acc.: 71.00%] [G loss: 1.249655]\n",
            "281 [D loss: 0.568372, acc.: 70.90%] [G loss: 1.198646]\n",
            "282 [D loss: 0.571876, acc.: 71.39%] [G loss: 1.128887]\n",
            "283 [D loss: 0.582745, acc.: 70.80%] [G loss: 1.099378]\n",
            "284 [D loss: 0.602459, acc.: 70.90%] [G loss: 1.111394]\n",
            "285 [D loss: 0.587026, acc.: 66.60%] [G loss: 1.101177]\n",
            "286 [D loss: 0.582203, acc.: 67.09%] [G loss: 1.077590]\n",
            "287 [D loss: 0.571281, acc.: 69.43%] [G loss: 1.052880]\n",
            "288 [D loss: 0.570984, acc.: 69.14%] [G loss: 1.071778]\n",
            "289 [D loss: 0.564494, acc.: 67.68%] [G loss: 1.096418]\n",
            "290 [D loss: 0.545726, acc.: 67.87%] [G loss: 1.057130]\n",
            "291 [D loss: 0.564581, acc.: 66.89%] [G loss: 1.036049]\n",
            "292 [D loss: 0.557666, acc.: 66.89%] [G loss: 1.093077]\n",
            "293 [D loss: 0.530859, acc.: 68.55%] [G loss: 1.123966]\n",
            "294 [D loss: 0.532123, acc.: 68.85%] [G loss: 1.087735]\n",
            "295 [D loss: 0.526417, acc.: 69.14%] [G loss: 1.107816]\n",
            "296 [D loss: 0.515140, acc.: 71.78%] [G loss: 1.125576]\n",
            "297 [D loss: 0.505551, acc.: 73.05%] [G loss: 1.125090]\n",
            "298 [D loss: 0.519543, acc.: 72.75%] [G loss: 1.127676]\n",
            "299 [D loss: 0.506353, acc.: 74.22%] [G loss: 1.145501]\n",
            "300 [D loss: 0.516638, acc.: 73.44%] [G loss: 1.136967]\n",
            "301 [D loss: 0.525172, acc.: 73.24%] [G loss: 1.100690]\n",
            "302 [D loss: 0.549017, acc.: 70.80%] [G loss: 1.109641]\n",
            "303 [D loss: 0.554129, acc.: 69.43%] [G loss: 1.088459]\n",
            "304 [D loss: 0.582697, acc.: 65.33%] [G loss: 1.151351]\n",
            "305 [D loss: 0.569615, acc.: 68.07%] [G loss: 1.217190]\n",
            "306 [D loss: 0.561742, acc.: 68.85%] [G loss: 1.245230]\n",
            "307 [D loss: 0.581903, acc.: 66.99%] [G loss: 1.348521]\n",
            "308 [D loss: 0.536579, acc.: 72.17%] [G loss: 1.376179]\n",
            "309 [D loss: 0.534239, acc.: 72.07%] [G loss: 1.351914]\n",
            "310 [D loss: 0.527320, acc.: 72.36%] [G loss: 1.351661]\n",
            "311 [D loss: 0.511787, acc.: 74.71%] [G loss: 1.298455]\n",
            "312 [D loss: 0.525935, acc.: 74.51%] [G loss: 1.248364]\n",
            "313 [D loss: 0.555787, acc.: 73.05%] [G loss: 1.191809]\n",
            "314 [D loss: 0.567684, acc.: 72.75%] [G loss: 1.152417]\n",
            "315 [D loss: 0.588331, acc.: 71.00%] [G loss: 1.124735]\n",
            "316 [D loss: 0.609944, acc.: 68.07%] [G loss: 1.108438]\n",
            "317 [D loss: 0.611163, acc.: 64.45%] [G loss: 1.077289]\n",
            "318 [D loss: 0.636231, acc.: 54.59%] [G loss: 1.044219]\n",
            "319 [D loss: 0.633835, acc.: 55.08%] [G loss: 1.056036]\n",
            "320 [D loss: 0.623794, acc.: 56.05%] [G loss: 1.183116]\n",
            "321 [D loss: 0.553921, acc.: 66.60%] [G loss: 1.468953]\n",
            "322 [D loss: 0.457303, acc.: 78.61%] [G loss: 1.830779]\n",
            "323 [D loss: 0.408469, acc.: 83.30%] [G loss: 2.088626]\n",
            "324 [D loss: 0.409358, acc.: 86.43%] [G loss: 2.220163]\n",
            "325 [D loss: 0.472234, acc.: 82.32%] [G loss: 2.443413]\n",
            "326 [D loss: 0.547440, acc.: 77.05%] [G loss: 2.540989]\n",
            "327 [D loss: 0.750359, acc.: 66.50%] [G loss: 2.389943]\n",
            "328 [D loss: 0.770257, acc.: 73.34%] [G loss: 2.370858]\n",
            "329 [D loss: 0.692013, acc.: 78.81%] [G loss: 2.293404]\n",
            "330 [D loss: 0.554523, acc.: 80.96%] [G loss: 2.227743]\n",
            "331 [D loss: 0.439385, acc.: 83.20%] [G loss: 2.136460]\n",
            "332 [D loss: 0.388080, acc.: 87.60%] [G loss: 2.098863]\n",
            "333 [D loss: 0.372926, acc.: 88.67%] [G loss: 1.946930]\n",
            "334 [D loss: 0.423372, acc.: 85.55%] [G loss: 1.716236]\n",
            "335 [D loss: 0.484144, acc.: 80.47%] [G loss: 1.559931]\n",
            "336 [D loss: 0.520137, acc.: 77.15%] [G loss: 1.591642]\n",
            "337 [D loss: 0.513084, acc.: 77.73%] [G loss: 1.683529]\n",
            "338 [D loss: 0.513068, acc.: 78.81%] [G loss: 1.689595]\n",
            "339 [D loss: 0.527332, acc.: 77.54%] [G loss: 1.867337]\n",
            "340 [D loss: 0.539237, acc.: 76.95%] [G loss: 1.928641]\n",
            "341 [D loss: 0.605810, acc.: 71.48%] [G loss: 1.908156]\n",
            "342 [D loss: 0.629243, acc.: 69.63%] [G loss: 1.945344]\n",
            "343 [D loss: 0.599724, acc.: 72.46%] [G loss: 1.923996]\n",
            "344 [D loss: 0.572970, acc.: 72.17%] [G loss: 1.922142]\n",
            "345 [D loss: 0.556614, acc.: 71.29%] [G loss: 1.927698]\n",
            "346 [D loss: 0.588638, acc.: 66.89%] [G loss: 2.020597]\n",
            "347 [D loss: 0.583501, acc.: 72.27%] [G loss: 1.887913]\n",
            "348 [D loss: 0.608461, acc.: 73.24%] [G loss: 1.799421]\n",
            "349 [D loss: 0.582690, acc.: 74.22%] [G loss: 1.679883]\n",
            "350 [D loss: 0.558456, acc.: 75.39%] [G loss: 1.663515]\n",
            "351 [D loss: 0.531220, acc.: 75.68%] [G loss: 1.645296]\n",
            "352 [D loss: 0.506545, acc.: 76.07%] [G loss: 1.625287]\n",
            "353 [D loss: 0.499761, acc.: 76.17%] [G loss: 1.564833]\n",
            "354 [D loss: 0.488695, acc.: 76.46%] [G loss: 1.517506]\n",
            "355 [D loss: 0.480014, acc.: 76.95%] [G loss: 1.467318]\n",
            "356 [D loss: 0.471080, acc.: 77.44%] [G loss: 1.424353]\n",
            "357 [D loss: 0.464003, acc.: 77.44%] [G loss: 1.437384]\n",
            "358 [D loss: 0.456983, acc.: 78.12%] [G loss: 1.422401]\n",
            "359 [D loss: 0.462487, acc.: 78.61%] [G loss: 1.436446]\n",
            "360 [D loss: 0.462666, acc.: 78.71%] [G loss: 1.387042]\n",
            "361 [D loss: 0.478199, acc.: 77.54%] [G loss: 1.349840]\n",
            "362 [D loss: 0.492343, acc.: 74.71%] [G loss: 1.360834]\n",
            "363 [D loss: 0.499171, acc.: 74.61%] [G loss: 1.367828]\n",
            "364 [D loss: 0.516481, acc.: 71.39%] [G loss: 1.503873]\n",
            "365 [D loss: 0.482135, acc.: 74.90%] [G loss: 1.609130]\n",
            "366 [D loss: 0.460391, acc.: 76.07%] [G loss: 1.685175]\n",
            "367 [D loss: 0.444295, acc.: 78.61%] [G loss: 1.678483]\n",
            "368 [D loss: 0.450658, acc.: 79.59%] [G loss: 1.611387]\n",
            "369 [D loss: 0.474272, acc.: 77.83%] [G loss: 1.516200]\n",
            "370 [D loss: 0.487966, acc.: 77.73%] [G loss: 1.446100]\n",
            "371 [D loss: 0.498514, acc.: 76.86%] [G loss: 1.454965]\n",
            "372 [D loss: 0.485028, acc.: 78.42%] [G loss: 1.462446]\n",
            "373 [D loss: 0.486250, acc.: 77.15%] [G loss: 1.523298]\n",
            "374 [D loss: 0.465592, acc.: 78.81%] [G loss: 1.522733]\n",
            "375 [D loss: 0.464426, acc.: 79.39%] [G loss: 1.512636]\n",
            "376 [D loss: 0.464026, acc.: 79.49%] [G loss: 1.465275]\n",
            "377 [D loss: 0.463484, acc.: 78.22%] [G loss: 1.421058]\n",
            "378 [D loss: 0.481684, acc.: 74.61%] [G loss: 1.452968]\n",
            "379 [D loss: 0.486002, acc.: 72.07%] [G loss: 1.499076]\n",
            "380 [D loss: 0.465962, acc.: 72.85%] [G loss: 1.547749]\n",
            "381 [D loss: 0.429196, acc.: 80.08%] [G loss: 1.630121]\n",
            "382 [D loss: 0.367869, acc.: 86.23%] [G loss: 1.855494]\n",
            "383 [D loss: 0.327712, acc.: 89.16%] [G loss: 1.982679]\n",
            "384 [D loss: 0.314003, acc.: 91.21%] [G loss: 1.992758]\n",
            "385 [D loss: 0.322601, acc.: 91.21%] [G loss: 1.948185]\n",
            "386 [D loss: 0.349859, acc.: 90.53%] [G loss: 1.870694]\n",
            "387 [D loss: 0.396413, acc.: 86.91%] [G loss: 1.887354]\n",
            "388 [D loss: 0.433370, acc.: 81.84%] [G loss: 1.873874]\n",
            "389 [D loss: 0.478915, acc.: 78.22%] [G loss: 1.858595]\n",
            "390 [D loss: 0.482666, acc.: 80.66%] [G loss: 1.843714]\n",
            "391 [D loss: 0.487279, acc.: 83.40%] [G loss: 1.832433]\n",
            "392 [D loss: 0.472046, acc.: 83.01%] [G loss: 1.746386]\n",
            "393 [D loss: 0.471084, acc.: 82.23%] [G loss: 1.704484]\n",
            "394 [D loss: 0.473929, acc.: 80.96%] [G loss: 1.617455]\n",
            "395 [D loss: 0.495656, acc.: 78.42%] [G loss: 1.561755]\n",
            "396 [D loss: 0.496583, acc.: 77.64%] [G loss: 1.482085]\n",
            "397 [D loss: 0.520358, acc.: 76.46%] [G loss: 1.503427]\n",
            "398 [D loss: 0.506553, acc.: 76.17%] [G loss: 1.545650]\n",
            "399 [D loss: 0.498281, acc.: 76.46%] [G loss: 1.593149]\n",
            "400 [D loss: 0.487356, acc.: 76.27%] [G loss: 1.656515]\n",
            "401 [D loss: 0.458099, acc.: 77.93%] [G loss: 1.619101]\n",
            "402 [D loss: 0.442015, acc.: 78.52%] [G loss: 1.570476]\n",
            "403 [D loss: 0.461184, acc.: 76.27%] [G loss: 1.497800]\n",
            "404 [D loss: 0.456196, acc.: 76.27%] [G loss: 1.508080]\n",
            "405 [D loss: 0.444032, acc.: 77.83%] [G loss: 1.455524]\n",
            "406 [D loss: 0.455864, acc.: 78.12%] [G loss: 1.434354]\n",
            "407 [D loss: 0.450940, acc.: 77.73%] [G loss: 1.389459]\n",
            "408 [D loss: 0.460844, acc.: 76.37%] [G loss: 1.337508]\n",
            "409 [D loss: 0.477018, acc.: 75.59%] [G loss: 1.307005]\n",
            "410 [D loss: 0.581926, acc.: 65.72%] [G loss: 1.188419]\n",
            "411 [D loss: 0.726045, acc.: 59.77%] [G loss: 1.279665]\n",
            "412 [D loss: 0.804787, acc.: 57.42%] [G loss: 1.464644]\n",
            "413 [D loss: 0.729123, acc.: 57.71%] [G loss: 1.901698]\n",
            "414 [D loss: 0.612970, acc.: 66.31%] [G loss: 2.241670]\n",
            "415 [D loss: 0.698302, acc.: 67.77%] [G loss: 1.929038]\n",
            "416 [D loss: 0.708247, acc.: 69.92%] [G loss: 1.679230]\n",
            "417 [D loss: 0.711761, acc.: 70.21%] [G loss: 1.498858]\n",
            "418 [D loss: 0.677201, acc.: 70.80%] [G loss: 1.380837]\n",
            "419 [D loss: 0.666281, acc.: 70.21%] [G loss: 1.290504]\n",
            "420 [D loss: 0.660292, acc.: 70.12%] [G loss: 1.215113]\n",
            "421 [D loss: 0.657199, acc.: 68.36%] [G loss: 1.096517]\n",
            "422 [D loss: 0.658491, acc.: 67.38%] [G loss: 1.067589]\n",
            "423 [D loss: 0.640579, acc.: 67.48%] [G loss: 1.029075]\n",
            "424 [D loss: 0.619433, acc.: 67.87%] [G loss: 1.007830]\n",
            "425 [D loss: 0.616275, acc.: 66.70%] [G loss: 1.034210]\n",
            "426 [D loss: 0.609288, acc.: 68.85%] [G loss: 1.038286]\n",
            "427 [D loss: 0.597808, acc.: 68.65%] [G loss: 1.041542]\n",
            "428 [D loss: 0.587721, acc.: 69.92%] [G loss: 1.049672]\n",
            "429 [D loss: 0.587792, acc.: 69.92%] [G loss: 1.055250]\n",
            "430 [D loss: 0.586352, acc.: 70.31%] [G loss: 1.068399]\n",
            "431 [D loss: 0.575417, acc.: 70.31%] [G loss: 1.057421]\n",
            "432 [D loss: 0.577271, acc.: 71.48%] [G loss: 1.080499]\n",
            "433 [D loss: 0.576096, acc.: 71.09%] [G loss: 1.056416]\n",
            "434 [D loss: 0.571480, acc.: 69.82%] [G loss: 1.069551]\n",
            "435 [D loss: 0.568153, acc.: 69.24%] [G loss: 1.056015]\n",
            "436 [D loss: 0.559963, acc.: 71.39%] [G loss: 1.029279]\n",
            "437 [D loss: 0.565977, acc.: 70.12%] [G loss: 1.044864]\n",
            "438 [D loss: 0.561622, acc.: 69.34%] [G loss: 1.045977]\n",
            "439 [D loss: 0.559679, acc.: 69.24%] [G loss: 1.033199]\n",
            "440 [D loss: 0.556000, acc.: 69.14%] [G loss: 1.044065]\n",
            "441 [D loss: 0.566925, acc.: 68.95%] [G loss: 1.029915]\n",
            "442 [D loss: 0.565542, acc.: 67.19%] [G loss: 1.045999]\n",
            "443 [D loss: 0.557123, acc.: 67.77%] [G loss: 1.036196]\n",
            "444 [D loss: 0.559730, acc.: 66.70%] [G loss: 1.048731]\n",
            "445 [D loss: 0.554495, acc.: 68.65%] [G loss: 1.066096]\n",
            "446 [D loss: 0.552881, acc.: 68.36%] [G loss: 1.041947]\n",
            "447 [D loss: 0.555515, acc.: 69.04%] [G loss: 1.036322]\n",
            "448 [D loss: 0.550740, acc.: 67.97%] [G loss: 1.059983]\n",
            "449 [D loss: 0.550886, acc.: 69.04%] [G loss: 1.052282]\n",
            "450 [D loss: 0.544806, acc.: 69.04%] [G loss: 1.038124]\n",
            "451 [D loss: 0.549311, acc.: 68.75%] [G loss: 1.042888]\n",
            "452 [D loss: 0.545405, acc.: 69.24%] [G loss: 1.057134]\n",
            "453 [D loss: 0.545053, acc.: 69.14%] [G loss: 1.054747]\n",
            "454 [D loss: 0.541114, acc.: 70.31%] [G loss: 1.050055]\n",
            "455 [D loss: 0.539922, acc.: 69.24%] [G loss: 1.049167]\n",
            "456 [D loss: 0.540401, acc.: 69.82%] [G loss: 1.049997]\n",
            "457 [D loss: 0.543672, acc.: 70.02%] [G loss: 1.057829]\n",
            "458 [D loss: 0.541956, acc.: 70.90%] [G loss: 1.063442]\n",
            "459 [D loss: 0.544038, acc.: 69.92%] [G loss: 1.078800]\n",
            "460 [D loss: 0.541470, acc.: 70.21%] [G loss: 1.082079]\n",
            "461 [D loss: 0.538633, acc.: 70.21%] [G loss: 1.093564]\n",
            "462 [D loss: 0.538935, acc.: 70.41%] [G loss: 1.104131]\n",
            "463 [D loss: 0.532789, acc.: 69.92%] [G loss: 1.083753]\n",
            "464 [D loss: 0.532981, acc.: 70.51%] [G loss: 1.099069]\n",
            "465 [D loss: 0.530126, acc.: 71.39%] [G loss: 1.079198]\n",
            "466 [D loss: 0.534067, acc.: 70.02%] [G loss: 1.075703]\n",
            "467 [D loss: 0.535489, acc.: 71.09%] [G loss: 1.080142]\n",
            "468 [D loss: 0.534081, acc.: 71.19%] [G loss: 1.038825]\n",
            "469 [D loss: 0.533739, acc.: 70.41%] [G loss: 1.032368]\n",
            "470 [D loss: 0.530431, acc.: 71.78%] [G loss: 1.005977]\n",
            "471 [D loss: 0.531188, acc.: 71.68%] [G loss: 1.025252]\n",
            "472 [D loss: 0.526404, acc.: 71.48%] [G loss: 1.022685]\n",
            "473 [D loss: 0.522387, acc.: 71.39%] [G loss: 0.994927]\n",
            "474 [D loss: 0.511329, acc.: 73.34%] [G loss: 1.010831]\n",
            "475 [D loss: 0.503704, acc.: 74.61%] [G loss: 1.019738]\n",
            "476 [D loss: 0.496448, acc.: 76.56%] [G loss: 1.022324]\n",
            "477 [D loss: 0.495488, acc.: 77.44%] [G loss: 1.034798]\n",
            "478 [D loss: 0.492234, acc.: 77.64%] [G loss: 1.046071]\n",
            "479 [D loss: 0.513435, acc.: 74.32%] [G loss: 1.046262]\n",
            "480 [D loss: 0.543363, acc.: 67.48%] [G loss: 1.039309]\n",
            "481 [D loss: 0.576890, acc.: 61.52%] [G loss: 1.077329]\n",
            "482 [D loss: 0.574418, acc.: 67.38%] [G loss: 1.172978]\n",
            "483 [D loss: 0.558797, acc.: 69.24%] [G loss: 1.335373]\n",
            "484 [D loss: 0.541111, acc.: 70.51%] [G loss: 1.411973]\n",
            "485 [D loss: 0.526870, acc.: 71.78%] [G loss: 1.408973]\n",
            "486 [D loss: 0.526246, acc.: 71.39%] [G loss: 1.314135]\n",
            "487 [D loss: 0.539821, acc.: 70.12%] [G loss: 1.276634]\n",
            "488 [D loss: 0.522847, acc.: 72.85%] [G loss: 1.210884]\n",
            "489 [D loss: 0.530077, acc.: 71.48%] [G loss: 1.169123]\n",
            "490 [D loss: 0.534880, acc.: 70.12%] [G loss: 1.113389]\n",
            "491 [D loss: 0.542779, acc.: 69.24%] [G loss: 1.067472]\n",
            "492 [D loss: 0.543874, acc.: 69.92%] [G loss: 1.056823]\n",
            "493 [D loss: 0.542209, acc.: 69.24%] [G loss: 1.025342]\n",
            "494 [D loss: 0.535809, acc.: 69.73%] [G loss: 0.993859]\n",
            "495 [D loss: 0.558415, acc.: 67.29%] [G loss: 0.985682]\n",
            "496 [D loss: 0.550206, acc.: 68.95%] [G loss: 1.005210]\n",
            "497 [D loss: 0.546122, acc.: 70.21%] [G loss: 1.006070]\n",
            "498 [D loss: 0.545347, acc.: 69.73%] [G loss: 1.027510]\n",
            "499 [D loss: 0.542373, acc.: 71.29%] [G loss: 1.079610]\n",
            "500 [D loss: 0.498311, acc.: 75.20%] [G loss: 1.195177]\n",
            "501 [D loss: 0.478789, acc.: 77.15%] [G loss: 1.336852]\n",
            "502 [D loss: 0.430744, acc.: 81.84%] [G loss: 1.491736]\n",
            "503 [D loss: 0.422662, acc.: 82.81%] [G loss: 1.541476]\n",
            "504 [D loss: 0.460524, acc.: 80.47%] [G loss: 1.506595]\n",
            "505 [D loss: 0.575493, acc.: 69.53%] [G loss: 1.411817]\n",
            "506 [D loss: 0.653332, acc.: 63.67%] [G loss: 1.408901]\n",
            "507 [D loss: 0.720127, acc.: 62.21%] [G loss: 1.442094]\n",
            "508 [D loss: 0.775477, acc.: 59.57%] [G loss: 1.266458]\n",
            "509 [D loss: 0.754234, acc.: 61.43%] [G loss: 1.151356]\n",
            "510 [D loss: 0.727617, acc.: 56.15%] [G loss: 1.241476]\n",
            "511 [D loss: 0.627391, acc.: 66.41%] [G loss: 1.313143]\n",
            "512 [D loss: 0.596068, acc.: 64.45%] [G loss: 1.318015]\n",
            "513 [D loss: 0.599971, acc.: 64.75%] [G loss: 1.248940]\n",
            "514 [D loss: 0.601533, acc.: 66.41%] [G loss: 1.211976]\n",
            "515 [D loss: 0.597002, acc.: 67.58%] [G loss: 1.189092]\n",
            "516 [D loss: 0.589830, acc.: 68.16%] [G loss: 1.183284]\n",
            "517 [D loss: 0.586294, acc.: 68.26%] [G loss: 1.163789]\n",
            "518 [D loss: 0.593825, acc.: 66.60%] [G loss: 1.124661]\n",
            "519 [D loss: 0.599924, acc.: 63.48%] [G loss: 1.107418]\n",
            "520 [D loss: 0.587811, acc.: 66.21%] [G loss: 1.088140]\n",
            "521 [D loss: 0.589085, acc.: 67.09%] [G loss: 1.063352]\n",
            "522 [D loss: 0.589098, acc.: 66.50%] [G loss: 1.046877]\n",
            "523 [D loss: 0.591106, acc.: 66.31%] [G loss: 1.059788]\n",
            "524 [D loss: 0.583924, acc.: 66.50%] [G loss: 1.031820]\n",
            "525 [D loss: 0.583546, acc.: 66.99%] [G loss: 1.022367]\n",
            "526 [D loss: 0.589958, acc.: 66.41%] [G loss: 1.013940]\n",
            "527 [D loss: 0.582685, acc.: 67.87%] [G loss: 0.988393]\n",
            "528 [D loss: 0.582315, acc.: 67.58%] [G loss: 0.996216]\n",
            "529 [D loss: 0.577310, acc.: 67.87%] [G loss: 0.984019]\n",
            "530 [D loss: 0.577926, acc.: 69.14%] [G loss: 1.000610]\n",
            "531 [D loss: 0.573347, acc.: 68.46%] [G loss: 0.994881]\n",
            "532 [D loss: 0.574606, acc.: 68.65%] [G loss: 0.980864]\n",
            "533 [D loss: 0.577123, acc.: 68.85%] [G loss: 0.992792]\n",
            "534 [D loss: 0.574049, acc.: 69.04%] [G loss: 0.981632]\n",
            "535 [D loss: 0.584183, acc.: 66.41%] [G loss: 0.987264]\n",
            "536 [D loss: 0.575648, acc.: 67.97%] [G loss: 0.995700]\n",
            "537 [D loss: 0.568952, acc.: 70.21%] [G loss: 0.984391]\n",
            "538 [D loss: 0.576120, acc.: 68.46%] [G loss: 1.004346]\n",
            "539 [D loss: 0.571125, acc.: 69.04%] [G loss: 1.006104]\n",
            "540 [D loss: 0.569849, acc.: 67.68%] [G loss: 0.996706]\n",
            "541 [D loss: 0.570016, acc.: 67.68%] [G loss: 1.023331]\n",
            "542 [D loss: 0.570508, acc.: 66.60%] [G loss: 1.015969]\n",
            "543 [D loss: 0.569469, acc.: 66.41%] [G loss: 1.052269]\n",
            "544 [D loss: 0.564389, acc.: 68.16%] [G loss: 1.040890]\n",
            "545 [D loss: 0.561447, acc.: 69.04%] [G loss: 1.070866]\n",
            "546 [D loss: 0.554815, acc.: 69.73%] [G loss: 1.078656]\n",
            "547 [D loss: 0.543400, acc.: 71.00%] [G loss: 1.092996]\n",
            "548 [D loss: 0.537535, acc.: 72.36%] [G loss: 1.110293]\n",
            "549 [D loss: 0.523641, acc.: 72.95%] [G loss: 1.116169]\n",
            "550 [D loss: 0.513219, acc.: 74.02%] [G loss: 1.128786]\n",
            "551 [D loss: 0.509218, acc.: 73.93%] [G loss: 1.121370]\n",
            "552 [D loss: 0.503284, acc.: 74.02%] [G loss: 1.144074]\n",
            "553 [D loss: 0.498714, acc.: 74.22%] [G loss: 1.133737]\n",
            "554 [D loss: 0.490862, acc.: 74.80%] [G loss: 1.100268]\n",
            "555 [D loss: 0.490641, acc.: 73.83%] [G loss: 1.096604]\n",
            "556 [D loss: 0.491076, acc.: 73.73%] [G loss: 1.075699]\n",
            "557 [D loss: 0.492029, acc.: 73.93%] [G loss: 1.072077]\n",
            "558 [D loss: 0.500484, acc.: 72.17%] [G loss: 1.061796]\n",
            "559 [D loss: 0.487159, acc.: 74.02%] [G loss: 1.064962]\n",
            "560 [D loss: 0.493517, acc.: 74.80%] [G loss: 1.059980]\n",
            "561 [D loss: 0.504395, acc.: 73.63%] [G loss: 1.046674]\n",
            "562 [D loss: 0.512266, acc.: 71.58%] [G loss: 1.054351]\n",
            "563 [D loss: 0.514385, acc.: 73.24%] [G loss: 1.042580]\n",
            "564 [D loss: 0.520663, acc.: 73.54%] [G loss: 1.042599]\n",
            "565 [D loss: 0.550482, acc.: 70.51%] [G loss: 1.028646]\n",
            "566 [D loss: 0.557678, acc.: 70.21%] [G loss: 1.031304]\n",
            "567 [D loss: 0.572269, acc.: 66.50%] [G loss: 1.016345]\n",
            "568 [D loss: 0.588751, acc.: 64.84%] [G loss: 1.061947]\n",
            "569 [D loss: 0.582158, acc.: 65.53%] [G loss: 1.119520]\n",
            "570 [D loss: 0.557786, acc.: 67.19%] [G loss: 1.194701]\n",
            "571 [D loss: 0.547279, acc.: 66.70%] [G loss: 1.273089]\n",
            "572 [D loss: 0.514174, acc.: 65.92%] [G loss: 1.352837]\n",
            "573 [D loss: 0.495880, acc.: 64.36%] [G loss: 1.358354]\n",
            "574 [D loss: 0.505310, acc.: 65.92%] [G loss: 1.282189]\n",
            "575 [D loss: 0.547713, acc.: 67.87%] [G loss: 1.138438]\n",
            "576 [D loss: 0.586023, acc.: 64.55%] [G loss: 1.023634]\n",
            "577 [D loss: 0.606645, acc.: 63.18%] [G loss: 0.972630]\n",
            "578 [D loss: 0.618147, acc.: 62.30%] [G loss: 1.000195]\n",
            "579 [D loss: 0.623999, acc.: 63.18%] [G loss: 1.028173]\n",
            "580 [D loss: 0.618263, acc.: 63.48%] [G loss: 1.030288]\n",
            "581 [D loss: 0.618502, acc.: 63.09%] [G loss: 1.047357]\n",
            "582 [D loss: 0.626537, acc.: 61.82%] [G loss: 1.100281]\n",
            "583 [D loss: 0.585440, acc.: 65.43%] [G loss: 1.139273]\n",
            "584 [D loss: 0.569222, acc.: 67.77%] [G loss: 1.186786]\n",
            "585 [D loss: 0.558531, acc.: 69.14%] [G loss: 1.175059]\n",
            "586 [D loss: 0.538101, acc.: 69.92%] [G loss: 1.177631]\n",
            "587 [D loss: 0.546188, acc.: 70.41%] [G loss: 1.155126]\n",
            "588 [D loss: 0.535424, acc.: 71.00%] [G loss: 1.139910]\n",
            "589 [D loss: 0.539945, acc.: 72.27%] [G loss: 1.126594]\n",
            "590 [D loss: 0.536641, acc.: 72.75%] [G loss: 1.122489]\n",
            "591 [D loss: 0.531734, acc.: 73.44%] [G loss: 1.091624]\n",
            "592 [D loss: 0.534930, acc.: 74.41%] [G loss: 1.106064]\n",
            "593 [D loss: 0.526479, acc.: 74.51%] [G loss: 1.157672]\n",
            "594 [D loss: 0.525619, acc.: 74.41%] [G loss: 1.167982]\n",
            "595 [D loss: 0.524565, acc.: 74.02%] [G loss: 1.201452]\n",
            "596 [D loss: 0.519298, acc.: 73.83%] [G loss: 1.237426]\n",
            "597 [D loss: 0.515027, acc.: 73.05%] [G loss: 1.246697]\n",
            "598 [D loss: 0.513591, acc.: 71.68%] [G loss: 1.299965]\n",
            "599 [D loss: 0.503300, acc.: 73.24%] [G loss: 1.276448]\n",
            "600 [D loss: 0.534392, acc.: 71.88%] [G loss: 1.259127]\n",
            "601 [D loss: 0.549257, acc.: 70.41%] [G loss: 1.228251]\n",
            "602 [D loss: 0.577367, acc.: 69.73%] [G loss: 1.199081]\n",
            "603 [D loss: 0.599109, acc.: 69.43%] [G loss: 1.131983]\n",
            "604 [D loss: 0.614377, acc.: 67.68%] [G loss: 1.076674]\n",
            "605 [D loss: 0.613135, acc.: 68.16%] [G loss: 1.064574]\n",
            "606 [D loss: 0.613916, acc.: 67.97%] [G loss: 1.006004]\n",
            "607 [D loss: 0.617490, acc.: 66.70%] [G loss: 1.009585]\n",
            "608 [D loss: 0.601408, acc.: 66.50%] [G loss: 1.013210]\n",
            "609 [D loss: 0.599384, acc.: 66.02%] [G loss: 1.005693]\n",
            "610 [D loss: 0.585610, acc.: 66.41%] [G loss: 1.003508]\n",
            "611 [D loss: 0.575269, acc.: 66.31%] [G loss: 1.006097]\n",
            "612 [D loss: 0.568311, acc.: 67.77%] [G loss: 1.026372]\n",
            "613 [D loss: 0.573480, acc.: 66.02%] [G loss: 1.022208]\n",
            "614 [D loss: 0.554491, acc.: 66.99%] [G loss: 1.038130]\n",
            "615 [D loss: 0.564086, acc.: 65.14%] [G loss: 1.042306]\n",
            "616 [D loss: 0.550732, acc.: 65.43%] [G loss: 1.038418]\n",
            "617 [D loss: 0.549750, acc.: 66.89%] [G loss: 1.054204]\n",
            "618 [D loss: 0.555758, acc.: 64.75%] [G loss: 1.065477]\n",
            "619 [D loss: 0.541832, acc.: 67.58%] [G loss: 1.066203]\n",
            "620 [D loss: 0.544438, acc.: 66.80%] [G loss: 1.055754]\n",
            "621 [D loss: 0.542011, acc.: 67.29%] [G loss: 1.082454]\n",
            "622 [D loss: 0.530990, acc.: 70.70%] [G loss: 1.094606]\n",
            "623 [D loss: 0.532291, acc.: 70.61%] [G loss: 1.129590]\n",
            "624 [D loss: 0.522895, acc.: 72.36%] [G loss: 1.130550]\n",
            "625 [D loss: 0.522398, acc.: 72.36%] [G loss: 1.153097]\n",
            "626 [D loss: 0.517682, acc.: 74.02%] [G loss: 1.179168]\n",
            "627 [D loss: 0.513239, acc.: 74.32%] [G loss: 1.206846]\n",
            "628 [D loss: 0.509238, acc.: 74.90%] [G loss: 1.232668]\n",
            "629 [D loss: 0.521542, acc.: 72.27%] [G loss: 1.225803]\n",
            "630 [D loss: 0.527732, acc.: 70.12%] [G loss: 1.240880]\n",
            "631 [D loss: 0.519097, acc.: 71.58%] [G loss: 1.254818]\n",
            "632 [D loss: 0.532139, acc.: 68.46%] [G loss: 1.212970]\n",
            "633 [D loss: 0.535553, acc.: 67.29%] [G loss: 1.183836]\n",
            "634 [D loss: 0.539276, acc.: 67.97%] [G loss: 1.154127]\n",
            "635 [D loss: 0.546694, acc.: 67.29%] [G loss: 1.137669]\n",
            "636 [D loss: 0.548167, acc.: 66.41%] [G loss: 1.107382]\n",
            "637 [D loss: 0.547596, acc.: 67.68%] [G loss: 1.089042]\n",
            "638 [D loss: 0.552041, acc.: 66.50%] [G loss: 1.057463]\n",
            "639 [D loss: 0.551052, acc.: 66.60%] [G loss: 1.039593]\n",
            "640 [D loss: 0.542761, acc.: 67.87%] [G loss: 1.029776]\n",
            "641 [D loss: 0.547806, acc.: 67.97%] [G loss: 1.015435]\n",
            "642 [D loss: 0.541515, acc.: 70.12%] [G loss: 1.019930]\n",
            "643 [D loss: 0.536776, acc.: 71.68%] [G loss: 1.019066]\n",
            "644 [D loss: 0.524362, acc.: 72.95%] [G loss: 1.030608]\n",
            "645 [D loss: 0.504797, acc.: 74.71%] [G loss: 1.060121]\n",
            "646 [D loss: 0.474708, acc.: 79.10%] [G loss: 1.131008]\n",
            "647 [D loss: 0.449490, acc.: 80.37%] [G loss: 1.262578]\n",
            "648 [D loss: 0.414299, acc.: 82.91%] [G loss: 1.390143]\n",
            "649 [D loss: 0.410326, acc.: 83.50%] [G loss: 1.534148]\n",
            "650 [D loss: 0.453528, acc.: 80.08%] [G loss: 1.853233]\n",
            "651 [D loss: 0.476991, acc.: 79.10%] [G loss: 2.419920]\n",
            "652 [D loss: 0.414097, acc.: 81.15%] [G loss: 3.196248]\n",
            "653 [D loss: 0.537695, acc.: 75.49%] [G loss: 2.899920]\n",
            "654 [D loss: 0.746805, acc.: 64.06%] [G loss: 2.779120]\n",
            "655 [D loss: 0.704354, acc.: 75.68%] [G loss: 2.575355]\n",
            "656 [D loss: 0.705115, acc.: 80.86%] [G loss: 2.496244]\n",
            "657 [D loss: 0.609126, acc.: 82.71%] [G loss: 2.433877]\n",
            "658 [D loss: 0.515512, acc.: 83.20%] [G loss: 2.223469]\n",
            "659 [D loss: 0.450772, acc.: 83.98%] [G loss: 2.134725]\n",
            "660 [D loss: 0.393915, acc.: 84.18%] [G loss: 2.047272]\n",
            "661 [D loss: 0.355524, acc.: 84.28%] [G loss: 2.039970]\n",
            "662 [D loss: 0.327805, acc.: 84.96%] [G loss: 1.994223]\n",
            "663 [D loss: 0.303431, acc.: 86.33%] [G loss: 2.063837]\n",
            "664 [D loss: 0.276981, acc.: 88.96%] [G loss: 2.198565]\n",
            "665 [D loss: 0.250940, acc.: 91.50%] [G loss: 2.297423]\n",
            "666 [D loss: 0.234126, acc.: 92.68%] [G loss: 2.311597]\n",
            "667 [D loss: 0.240281, acc.: 92.68%] [G loss: 2.225976]\n",
            "668 [D loss: 0.281899, acc.: 88.87%] [G loss: 2.064682]\n",
            "669 [D loss: 0.354998, acc.: 84.18%] [G loss: 1.823915]\n",
            "670 [D loss: 0.459135, acc.: 76.27%] [G loss: 1.736698]\n",
            "671 [D loss: 0.488364, acc.: 74.41%] [G loss: 1.631649]\n",
            "672 [D loss: 0.502497, acc.: 77.64%] [G loss: 1.604107]\n",
            "673 [D loss: 0.500504, acc.: 81.84%] [G loss: 1.587068]\n",
            "674 [D loss: 0.499842, acc.: 81.54%] [G loss: 1.564307]\n",
            "675 [D loss: 0.496949, acc.: 79.79%] [G loss: 1.563309]\n",
            "676 [D loss: 0.479389, acc.: 80.57%] [G loss: 1.589717]\n",
            "677 [D loss: 0.446991, acc.: 82.62%] [G loss: 1.604145]\n",
            "678 [D loss: 0.428967, acc.: 83.50%] [G loss: 1.661644]\n",
            "679 [D loss: 0.404800, acc.: 84.18%] [G loss: 1.726070]\n",
            "680 [D loss: 0.383411, acc.: 84.28%] [G loss: 1.793600]\n",
            "681 [D loss: 0.390099, acc.: 84.96%] [G loss: 1.766370]\n",
            "682 [D loss: 0.381399, acc.: 85.06%] [G loss: 1.725680]\n",
            "683 [D loss: 0.417198, acc.: 81.64%] [G loss: 1.654889]\n",
            "684 [D loss: 0.480422, acc.: 79.59%] [G loss: 1.507232]\n",
            "685 [D loss: 0.578840, acc.: 76.46%] [G loss: 1.452186]\n",
            "686 [D loss: 0.573355, acc.: 77.34%] [G loss: 1.386594]\n",
            "687 [D loss: 0.588774, acc.: 76.17%] [G loss: 1.351911]\n",
            "688 [D loss: 0.588350, acc.: 72.07%] [G loss: 1.429224]\n",
            "689 [D loss: 0.554255, acc.: 74.12%] [G loss: 1.446567]\n",
            "690 [D loss: 0.521089, acc.: 78.71%] [G loss: 1.518994]\n",
            "691 [D loss: 0.462606, acc.: 81.25%] [G loss: 1.628820]\n",
            "692 [D loss: 0.435078, acc.: 81.54%] [G loss: 1.727408]\n",
            "693 [D loss: 0.416629, acc.: 83.50%] [G loss: 1.786552]\n",
            "694 [D loss: 0.409552, acc.: 86.04%] [G loss: 1.733585]\n",
            "695 [D loss: 0.414660, acc.: 87.21%] [G loss: 1.670637]\n",
            "696 [D loss: 0.427333, acc.: 87.40%] [G loss: 1.625208]\n",
            "697 [D loss: 0.461594, acc.: 85.64%] [G loss: 1.598276]\n",
            "698 [D loss: 0.483408, acc.: 82.62%] [G loss: 1.608296]\n",
            "699 [D loss: 0.504229, acc.: 79.20%] [G loss: 1.584846]\n",
            "700 [D loss: 0.513510, acc.: 80.57%] [G loss: 1.519743]\n",
            "701 [D loss: 0.509758, acc.: 78.71%] [G loss: 1.489399]\n",
            "702 [D loss: 0.498947, acc.: 79.79%] [G loss: 1.512229]\n",
            "703 [D loss: 0.503182, acc.: 78.03%] [G loss: 1.495615]\n",
            "704 [D loss: 0.488035, acc.: 77.93%] [G loss: 1.501831]\n",
            "705 [D loss: 0.475138, acc.: 77.64%] [G loss: 1.465856]\n",
            "706 [D loss: 0.456907, acc.: 80.76%] [G loss: 1.515856]\n",
            "707 [D loss: 0.465609, acc.: 77.93%] [G loss: 1.503608]\n",
            "708 [D loss: 0.457939, acc.: 78.61%] [G loss: 1.515288]\n",
            "709 [D loss: 0.459258, acc.: 78.42%] [G loss: 1.491611]\n",
            "710 [D loss: 0.433240, acc.: 81.64%] [G loss: 1.513119]\n",
            "711 [D loss: 0.440453, acc.: 81.05%] [G loss: 1.495149]\n",
            "712 [D loss: 0.446868, acc.: 81.64%] [G loss: 1.455771]\n",
            "713 [D loss: 0.459000, acc.: 80.18%] [G loss: 1.450238]\n",
            "714 [D loss: 0.458465, acc.: 81.25%] [G loss: 1.469442]\n",
            "715 [D loss: 0.461970, acc.: 80.08%] [G loss: 1.389082]\n",
            "716 [D loss: 0.447639, acc.: 82.03%] [G loss: 1.369330]\n",
            "717 [D loss: 0.450837, acc.: 81.05%] [G loss: 1.320534]\n",
            "718 [D loss: 0.471824, acc.: 77.64%] [G loss: 1.333874]\n",
            "719 [D loss: 0.466170, acc.: 78.71%] [G loss: 1.270128]\n",
            "720 [D loss: 0.455408, acc.: 80.66%] [G loss: 1.313957]\n",
            "721 [D loss: 0.466868, acc.: 78.91%] [G loss: 1.335283]\n",
            "722 [D loss: 0.468690, acc.: 77.93%] [G loss: 1.386342]\n",
            "723 [D loss: 0.444962, acc.: 80.57%] [G loss: 1.412382]\n",
            "724 [D loss: 0.467670, acc.: 77.83%] [G loss: 1.405117]\n",
            "725 [D loss: 0.451204, acc.: 79.49%] [G loss: 1.460531]\n",
            "726 [D loss: 0.462340, acc.: 77.15%] [G loss: 1.435469]\n",
            "727 [D loss: 0.465054, acc.: 77.73%] [G loss: 1.451805]\n",
            "728 [D loss: 0.474550, acc.: 75.98%] [G loss: 1.451636]\n",
            "729 [D loss: 0.482551, acc.: 75.39%] [G loss: 1.367095]\n",
            "730 [D loss: 0.491932, acc.: 75.98%] [G loss: 1.352458]\n",
            "731 [D loss: 0.504474, acc.: 73.54%] [G loss: 1.350644]\n",
            "732 [D loss: 0.516898, acc.: 71.48%] [G loss: 1.296242]\n",
            "733 [D loss: 0.508435, acc.: 73.14%] [G loss: 1.239791]\n",
            "734 [D loss: 0.510866, acc.: 72.75%] [G loss: 1.218766]\n",
            "735 [D loss: 0.497183, acc.: 72.56%] [G loss: 1.250607]\n",
            "736 [D loss: 0.466335, acc.: 76.66%] [G loss: 1.316339]\n",
            "737 [D loss: 0.442433, acc.: 82.13%] [G loss: 1.390131]\n",
            "738 [D loss: 0.463242, acc.: 75.68%] [G loss: 1.338715]\n",
            "739 [D loss: 0.435284, acc.: 78.32%] [G loss: 1.452756]\n",
            "740 [D loss: 0.405815, acc.: 83.69%] [G loss: 1.446145]\n",
            "741 [D loss: 0.428280, acc.: 78.32%] [G loss: 1.436469]\n",
            "742 [D loss: 0.430444, acc.: 77.93%] [G loss: 1.489070]\n",
            "743 [D loss: 0.456671, acc.: 76.46%] [G loss: 1.423212]\n",
            "744 [D loss: 0.469475, acc.: 72.27%] [G loss: 1.386989]\n",
            "745 [D loss: 0.491207, acc.: 70.51%] [G loss: 1.358717]\n",
            "746 [D loss: 0.506207, acc.: 71.39%] [G loss: 1.363636]\n",
            "747 [D loss: 0.549545, acc.: 66.11%] [G loss: 1.238103]\n",
            "748 [D loss: 0.565916, acc.: 62.99%] [G loss: 1.177782]\n",
            "749 [D loss: 0.598212, acc.: 61.43%] [G loss: 1.158636]\n",
            "750 [D loss: 0.611289, acc.: 55.86%] [G loss: 1.165493]\n",
            "751 [D loss: 0.586518, acc.: 64.65%] [G loss: 1.193813]\n",
            "752 [D loss: 0.557260, acc.: 69.63%] [G loss: 1.254359]\n",
            "753 [D loss: 0.553691, acc.: 66.21%] [G loss: 1.202460]\n",
            "754 [D loss: 0.523812, acc.: 70.21%] [G loss: 1.217312]\n",
            "755 [D loss: 0.496160, acc.: 75.29%] [G loss: 1.298110]\n",
            "756 [D loss: 0.494925, acc.: 76.17%] [G loss: 1.285762]\n",
            "757 [D loss: 0.506266, acc.: 70.21%] [G loss: 1.253394]\n",
            "758 [D loss: 0.476013, acc.: 75.98%] [G loss: 1.304293]\n",
            "759 [D loss: 0.469386, acc.: 77.34%] [G loss: 1.328750]\n",
            "760 [D loss: 0.463451, acc.: 75.88%] [G loss: 1.319391]\n",
            "761 [D loss: 0.468785, acc.: 72.07%] [G loss: 1.322811]\n",
            "762 [D loss: 0.456788, acc.: 75.00%] [G loss: 1.383206]\n",
            "763 [D loss: 0.459625, acc.: 74.71%] [G loss: 1.382434]\n",
            "764 [D loss: 0.456917, acc.: 75.20%] [G loss: 1.331078]\n",
            "765 [D loss: 0.460792, acc.: 75.88%] [G loss: 1.357469]\n",
            "766 [D loss: 0.450345, acc.: 77.25%] [G loss: 1.410526]\n",
            "767 [D loss: 0.464868, acc.: 76.07%] [G loss: 1.424188]\n",
            "768 [D loss: 0.504931, acc.: 67.19%] [G loss: 1.375719]\n",
            "769 [D loss: 0.488411, acc.: 72.66%] [G loss: 1.438043]\n",
            "770 [D loss: 0.499666, acc.: 71.00%] [G loss: 1.546511]\n",
            "771 [D loss: 0.468103, acc.: 76.66%] [G loss: 1.601638]\n",
            "772 [D loss: 0.487264, acc.: 73.83%] [G loss: 1.521603]\n",
            "773 [D loss: 0.485201, acc.: 72.17%] [G loss: 1.664970]\n",
            "774 [D loss: 0.440907, acc.: 77.15%] [G loss: 1.696749]\n",
            "775 [D loss: 0.485036, acc.: 75.98%] [G loss: 1.484844]\n",
            "776 [D loss: 0.408795, acc.: 78.81%] [G loss: 1.720438]\n",
            "777 [D loss: 0.466249, acc.: 81.15%] [G loss: 1.394539]\n",
            "778 [D loss: 0.476576, acc.: 79.30%] [G loss: 1.372115]\n",
            "779 [D loss: 0.402710, acc.: 76.66%] [G loss: 1.613735]\n",
            "780 [D loss: 0.514932, acc.: 67.48%] [G loss: 1.272021]\n",
            "781 [D loss: 0.412040, acc.: 76.56%] [G loss: 1.535227]\n",
            "782 [D loss: 0.448449, acc.: 77.44%] [G loss: 1.382437]\n",
            "783 [D loss: 0.470864, acc.: 76.37%] [G loss: 1.349224]\n",
            "784 [D loss: 0.450173, acc.: 76.37%] [G loss: 1.432607]\n",
            "785 [D loss: 0.486320, acc.: 76.07%] [G loss: 1.401486]\n",
            "786 [D loss: 0.447593, acc.: 75.88%] [G loss: 1.531110]\n",
            "787 [D loss: 0.511824, acc.: 73.24%] [G loss: 1.359547]\n",
            "788 [D loss: 0.430508, acc.: 76.56%] [G loss: 1.627256]\n",
            "789 [D loss: 0.469032, acc.: 76.66%] [G loss: 1.470057]\n",
            "790 [D loss: 0.455293, acc.: 76.46%] [G loss: 1.500506]\n",
            "791 [D loss: 0.449898, acc.: 76.76%] [G loss: 1.507918]\n",
            "792 [D loss: 0.466656, acc.: 77.15%] [G loss: 1.397673]\n",
            "793 [D loss: 0.432863, acc.: 77.05%] [G loss: 1.540886]\n",
            "794 [D loss: 0.475397, acc.: 77.34%] [G loss: 1.358516]\n",
            "795 [D loss: 0.445602, acc.: 76.76%] [G loss: 1.463442]\n",
            "796 [D loss: 0.461610, acc.: 77.05%] [G loss: 1.307767]\n",
            "797 [D loss: 0.451363, acc.: 77.05%] [G loss: 1.338382]\n",
            "798 [D loss: 0.466912, acc.: 77.05%] [G loss: 1.264583]\n",
            "799 [D loss: 0.443429, acc.: 77.34%] [G loss: 1.322827]\n",
            "800 [D loss: 0.459504, acc.: 77.05%] [G loss: 1.226114]\n",
            "801 [D loss: 0.445035, acc.: 77.34%] [G loss: 1.301942]\n",
            "802 [D loss: 0.452112, acc.: 77.34%] [G loss: 1.254691]\n",
            "803 [D loss: 0.432479, acc.: 77.44%] [G loss: 1.328660]\n",
            "804 [D loss: 0.445552, acc.: 77.25%] [G loss: 1.262523]\n",
            "805 [D loss: 0.436024, acc.: 77.54%] [G loss: 1.338854]\n",
            "806 [D loss: 0.455378, acc.: 76.37%] [G loss: 1.255005]\n",
            "807 [D loss: 0.452147, acc.: 76.66%] [G loss: 1.341321]\n",
            "808 [D loss: 0.478407, acc.: 73.73%] [G loss: 1.312717]\n",
            "809 [D loss: 0.468519, acc.: 76.66%] [G loss: 1.355420]\n",
            "810 [D loss: 0.483827, acc.: 77.05%] [G loss: 1.368352]\n",
            "811 [D loss: 0.474827, acc.: 77.54%] [G loss: 1.443269]\n",
            "812 [D loss: 0.466823, acc.: 77.25%] [G loss: 1.448929]\n",
            "813 [D loss: 0.461414, acc.: 77.54%] [G loss: 1.439782]\n",
            "814 [D loss: 0.458502, acc.: 77.44%] [G loss: 1.412799]\n",
            "815 [D loss: 0.454226, acc.: 77.64%] [G loss: 1.399551]\n",
            "816 [D loss: 0.457750, acc.: 77.25%] [G loss: 1.399972]\n",
            "817 [D loss: 0.453802, acc.: 77.44%] [G loss: 1.369672]\n",
            "818 [D loss: 0.455883, acc.: 77.34%] [G loss: 1.332876]\n",
            "819 [D loss: 0.457677, acc.: 77.64%] [G loss: 1.266408]\n",
            "820 [D loss: 0.464637, acc.: 77.64%] [G loss: 1.260759]\n",
            "821 [D loss: 0.464325, acc.: 77.64%] [G loss: 1.228618]\n",
            "822 [D loss: 0.469943, acc.: 77.34%] [G loss: 1.183968]\n",
            "823 [D loss: 0.467509, acc.: 77.44%] [G loss: 1.165051]\n",
            "824 [D loss: 0.460619, acc.: 77.54%] [G loss: 1.168988]\n",
            "825 [D loss: 0.461460, acc.: 77.54%] [G loss: 1.163161]\n",
            "826 [D loss: 0.448005, acc.: 77.34%] [G loss: 1.170381]\n",
            "827 [D loss: 0.442662, acc.: 77.34%] [G loss: 1.182310]\n",
            "828 [D loss: 0.435347, acc.: 77.83%] [G loss: 1.215645]\n",
            "829 [D loss: 0.428478, acc.: 78.81%] [G loss: 1.218932]\n",
            "830 [D loss: 0.428962, acc.: 76.46%] [G loss: 1.225560]\n",
            "831 [D loss: 0.437559, acc.: 75.98%] [G loss: 1.227975]\n",
            "832 [D loss: 0.447051, acc.: 73.34%] [G loss: 1.228554]\n",
            "833 [D loss: 0.458807, acc.: 73.14%] [G loss: 1.227136]\n",
            "834 [D loss: 0.472947, acc.: 73.73%] [G loss: 1.239287]\n",
            "835 [D loss: 0.482118, acc.: 74.51%] [G loss: 1.238726]\n",
            "836 [D loss: 0.480176, acc.: 77.34%] [G loss: 1.326978]\n",
            "837 [D loss: 0.473703, acc.: 77.44%] [G loss: 1.350240]\n",
            "838 [D loss: 0.469127, acc.: 77.64%] [G loss: 1.363729]\n",
            "839 [D loss: 0.461632, acc.: 77.64%] [G loss: 1.400450]\n",
            "840 [D loss: 0.459490, acc.: 77.64%] [G loss: 1.410769]\n",
            "841 [D loss: 0.457898, acc.: 77.64%] [G loss: 1.404046]\n",
            "842 [D loss: 0.455772, acc.: 77.54%] [G loss: 1.384317]\n",
            "843 [D loss: 0.456970, acc.: 77.64%] [G loss: 1.364268]\n",
            "844 [D loss: 0.458428, acc.: 77.64%] [G loss: 1.329727]\n",
            "845 [D loss: 0.459907, acc.: 77.44%] [G loss: 1.308836]\n",
            "846 [D loss: 0.458922, acc.: 77.54%] [G loss: 1.287686]\n",
            "847 [D loss: 0.458717, acc.: 77.64%] [G loss: 1.269237]\n",
            "848 [D loss: 0.459361, acc.: 77.64%] [G loss: 1.265517]\n",
            "849 [D loss: 0.461024, acc.: 77.64%] [G loss: 1.258505]\n",
            "850 [D loss: 0.459403, acc.: 77.64%] [G loss: 1.245986]\n",
            "851 [D loss: 0.461322, acc.: 77.64%] [G loss: 1.226893]\n",
            "852 [D loss: 0.457190, acc.: 77.64%] [G loss: 1.220320]\n",
            "853 [D loss: 0.455405, acc.: 77.64%] [G loss: 1.211101]\n",
            "854 [D loss: 0.458622, acc.: 77.64%] [G loss: 1.205228]\n",
            "855 [D loss: 0.453900, acc.: 77.64%] [G loss: 1.201326]\n",
            "856 [D loss: 0.453703, acc.: 77.64%] [G loss: 1.212651]\n",
            "857 [D loss: 0.454505, acc.: 77.64%] [G loss: 1.198814]\n",
            "858 [D loss: 0.450225, acc.: 77.64%] [G loss: 1.215297]\n",
            "859 [D loss: 0.455251, acc.: 77.64%] [G loss: 1.218627]\n",
            "860 [D loss: 0.455839, acc.: 77.64%] [G loss: 1.218790]\n",
            "861 [D loss: 0.451440, acc.: 77.64%] [G loss: 1.203668]\n",
            "862 [D loss: 0.459786, acc.: 77.64%] [G loss: 1.211313]\n",
            "863 [D loss: 0.454008, acc.: 77.64%] [G loss: 1.212505]\n",
            "864 [D loss: 0.455838, acc.: 77.64%] [G loss: 1.222538]\n",
            "865 [D loss: 0.459860, acc.: 77.64%] [G loss: 1.232157]\n",
            "866 [D loss: 0.457364, acc.: 77.64%] [G loss: 1.223864]\n",
            "867 [D loss: 0.456708, acc.: 77.64%] [G loss: 1.235076]\n",
            "868 [D loss: 0.458916, acc.: 77.64%] [G loss: 1.241015]\n",
            "869 [D loss: 0.453833, acc.: 77.64%] [G loss: 1.259371]\n",
            "870 [D loss: 0.453409, acc.: 77.64%] [G loss: 1.251619]\n",
            "871 [D loss: 0.453894, acc.: 77.64%] [G loss: 1.245864]\n",
            "872 [D loss: 0.456108, acc.: 77.64%] [G loss: 1.238111]\n",
            "873 [D loss: 0.455199, acc.: 77.64%] [G loss: 1.245737]\n",
            "874 [D loss: 0.454433, acc.: 77.64%] [G loss: 1.237386]\n",
            "875 [D loss: 0.455536, acc.: 77.64%] [G loss: 1.224895]\n",
            "876 [D loss: 0.457872, acc.: 77.64%] [G loss: 1.228547]\n",
            "877 [D loss: 0.455166, acc.: 77.54%] [G loss: 1.215135]\n",
            "878 [D loss: 0.457716, acc.: 77.34%] [G loss: 1.199870]\n",
            "879 [D loss: 0.459690, acc.: 77.54%] [G loss: 1.200576]\n",
            "880 [D loss: 0.459217, acc.: 77.64%] [G loss: 1.189945]\n",
            "881 [D loss: 0.459580, acc.: 77.25%] [G loss: 1.189052]\n",
            "882 [D loss: 0.459607, acc.: 77.15%] [G loss: 1.178679]\n",
            "883 [D loss: 0.456773, acc.: 77.25%] [G loss: 1.177420]\n",
            "884 [D loss: 0.460270, acc.: 76.95%] [G loss: 1.183409]\n",
            "885 [D loss: 0.455115, acc.: 76.95%] [G loss: 1.185226]\n",
            "886 [D loss: 0.458041, acc.: 77.05%] [G loss: 1.198945]\n",
            "887 [D loss: 0.456617, acc.: 77.15%] [G loss: 1.181667]\n",
            "888 [D loss: 0.460207, acc.: 76.17%] [G loss: 1.188166]\n",
            "889 [D loss: 0.460673, acc.: 76.37%] [G loss: 1.193931]\n",
            "890 [D loss: 0.455658, acc.: 76.76%] [G loss: 1.177890]\n",
            "891 [D loss: 0.460249, acc.: 75.98%] [G loss: 1.171488]\n",
            "892 [D loss: 0.459866, acc.: 76.56%] [G loss: 1.174573]\n",
            "893 [D loss: 0.461523, acc.: 76.46%] [G loss: 1.186049]\n",
            "894 [D loss: 0.463505, acc.: 76.17%] [G loss: 1.196627]\n",
            "895 [D loss: 0.463901, acc.: 76.66%] [G loss: 1.205534]\n",
            "896 [D loss: 0.464785, acc.: 76.07%] [G loss: 1.202689]\n",
            "897 [D loss: 0.470582, acc.: 75.59%] [G loss: 1.207317]\n",
            "898 [D loss: 0.458509, acc.: 76.46%] [G loss: 1.213014]\n",
            "899 [D loss: 0.466001, acc.: 76.07%] [G loss: 1.218231]\n",
            "900 [D loss: 0.462086, acc.: 76.46%] [G loss: 1.223810]\n",
            "901 [D loss: 0.462076, acc.: 76.46%] [G loss: 1.239543]\n",
            "902 [D loss: 0.463848, acc.: 76.17%] [G loss: 1.226190]\n",
            "903 [D loss: 0.465238, acc.: 76.46%] [G loss: 1.206598]\n",
            "904 [D loss: 0.465856, acc.: 76.56%] [G loss: 1.213367]\n",
            "905 [D loss: 0.476314, acc.: 75.88%] [G loss: 1.218748]\n",
            "906 [D loss: 0.468849, acc.: 76.66%] [G loss: 1.213802]\n",
            "907 [D loss: 0.475150, acc.: 76.56%] [G loss: 1.212680]\n",
            "908 [D loss: 0.469908, acc.: 76.86%] [G loss: 1.236799]\n",
            "909 [D loss: 0.469345, acc.: 76.56%] [G loss: 1.218223]\n",
            "910 [D loss: 0.473969, acc.: 76.56%] [G loss: 1.214452]\n",
            "911 [D loss: 0.481976, acc.: 75.98%] [G loss: 1.214216]\n",
            "912 [D loss: 0.490839, acc.: 75.68%] [G loss: 1.205632]\n",
            "913 [D loss: 0.477337, acc.: 76.56%] [G loss: 1.196109]\n",
            "914 [D loss: 0.474883, acc.: 76.66%] [G loss: 1.196451]\n",
            "915 [D loss: 0.474116, acc.: 76.66%] [G loss: 1.196887]\n",
            "916 [D loss: 0.472768, acc.: 76.76%] [G loss: 1.213550]\n",
            "917 [D loss: 0.494787, acc.: 75.98%] [G loss: 1.202464]\n",
            "918 [D loss: 0.484031, acc.: 76.17%] [G loss: 1.205743]\n",
            "919 [D loss: 0.480832, acc.: 76.37%] [G loss: 1.202272]\n",
            "920 [D loss: 0.475109, acc.: 76.46%] [G loss: 1.196505]\n",
            "921 [D loss: 0.468069, acc.: 76.76%] [G loss: 1.196629]\n",
            "922 [D loss: 0.481846, acc.: 76.37%] [G loss: 1.195203]\n",
            "923 [D loss: 0.471198, acc.: 76.66%] [G loss: 1.180779]\n",
            "924 [D loss: 0.462644, acc.: 77.05%] [G loss: 1.208489]\n",
            "925 [D loss: 0.472849, acc.: 76.37%] [G loss: 1.199582]\n",
            "926 [D loss: 0.464823, acc.: 76.86%] [G loss: 1.200903]\n",
            "927 [D loss: 0.466939, acc.: 76.56%] [G loss: 1.203383]\n",
            "928 [D loss: 0.467731, acc.: 76.76%] [G loss: 1.197340]\n",
            "929 [D loss: 0.489885, acc.: 76.07%] [G loss: 1.193391]\n",
            "930 [D loss: 0.469309, acc.: 76.66%] [G loss: 1.190708]\n",
            "931 [D loss: 0.479496, acc.: 76.27%] [G loss: 1.190431]\n",
            "932 [D loss: 0.477326, acc.: 76.17%] [G loss: 1.187952]\n",
            "933 [D loss: 0.482793, acc.: 75.98%] [G loss: 1.188465]\n",
            "934 [D loss: 0.468015, acc.: 76.46%] [G loss: 1.196242]\n",
            "935 [D loss: 0.474881, acc.: 76.17%] [G loss: 1.178953]\n",
            "936 [D loss: 0.474986, acc.: 76.56%] [G loss: 1.174181]\n",
            "937 [D loss: 0.474123, acc.: 76.37%] [G loss: 1.176789]\n",
            "938 [D loss: 0.477785, acc.: 76.27%] [G loss: 1.165378]\n",
            "939 [D loss: 0.472570, acc.: 76.37%] [G loss: 1.175086]\n",
            "940 [D loss: 0.474285, acc.: 76.07%] [G loss: 1.174891]\n",
            "941 [D loss: 0.483399, acc.: 75.98%] [G loss: 1.165710]\n",
            "942 [D loss: 0.494431, acc.: 75.68%] [G loss: 1.165819]\n",
            "943 [D loss: 0.489160, acc.: 75.68%] [G loss: 1.172676]\n",
            "944 [D loss: 0.483882, acc.: 75.68%] [G loss: 1.172508]\n",
            "945 [D loss: 0.486476, acc.: 75.78%] [G loss: 1.171474]\n",
            "946 [D loss: 0.506277, acc.: 74.90%] [G loss: 1.161809]\n",
            "947 [D loss: 0.522618, acc.: 74.12%] [G loss: 1.163418]\n",
            "948 [D loss: 0.495179, acc.: 75.20%] [G loss: 1.154130]\n",
            "949 [D loss: 0.508147, acc.: 74.51%] [G loss: 1.187121]\n",
            "950 [D loss: 0.512824, acc.: 74.32%] [G loss: 1.165655]\n",
            "951 [D loss: 0.508611, acc.: 73.83%] [G loss: 1.139907]\n",
            "952 [D loss: 0.505162, acc.: 74.41%] [G loss: 1.141475]\n",
            "953 [D loss: 0.497206, acc.: 74.12%] [G loss: 1.139232]\n",
            "954 [D loss: 0.504849, acc.: 74.22%] [G loss: 1.135673]\n",
            "955 [D loss: 0.502410, acc.: 74.41%] [G loss: 1.136282]\n",
            "956 [D loss: 0.533029, acc.: 72.56%] [G loss: 1.124657]\n",
            "957 [D loss: 0.539146, acc.: 71.88%] [G loss: 1.124273]\n",
            "958 [D loss: 0.544478, acc.: 71.68%] [G loss: 1.125712]\n",
            "959 [D loss: 0.544709, acc.: 71.09%] [G loss: 1.136122]\n",
            "960 [D loss: 0.561626, acc.: 69.34%] [G loss: 1.131388]\n",
            "961 [D loss: 0.561278, acc.: 68.55%] [G loss: 1.089444]\n",
            "962 [D loss: 0.563882, acc.: 67.68%] [G loss: 1.114372]\n",
            "963 [D loss: 0.589031, acc.: 66.41%] [G loss: 1.137196]\n",
            "964 [D loss: 0.570910, acc.: 65.23%] [G loss: 1.140863]\n",
            "965 [D loss: 0.568309, acc.: 66.02%] [G loss: 1.158380]\n",
            "966 [D loss: 0.562463, acc.: 67.09%] [G loss: 1.095347]\n",
            "967 [D loss: 0.558134, acc.: 67.09%] [G loss: 1.104848]\n",
            "968 [D loss: 0.536396, acc.: 70.31%] [G loss: 1.089259]\n",
            "969 [D loss: 0.537470, acc.: 72.36%] [G loss: 1.061756]\n",
            "970 [D loss: 0.527294, acc.: 72.75%] [G loss: 1.041380]\n",
            "971 [D loss: 0.528321, acc.: 72.66%] [G loss: 1.062199]\n",
            "972 [D loss: 0.518716, acc.: 73.73%] [G loss: 1.052374]\n",
            "973 [D loss: 0.516342, acc.: 73.73%] [G loss: 1.064310]\n",
            "974 [D loss: 0.521544, acc.: 72.75%] [G loss: 1.047476]\n",
            "975 [D loss: 0.516244, acc.: 74.12%] [G loss: 1.072527]\n",
            "976 [D loss: 0.512997, acc.: 73.83%] [G loss: 1.075091]\n",
            "977 [D loss: 0.520824, acc.: 72.95%] [G loss: 1.090222]\n",
            "978 [D loss: 0.511933, acc.: 73.73%] [G loss: 1.103487]\n",
            "979 [D loss: 0.520366, acc.: 71.58%] [G loss: 1.112464]\n",
            "980 [D loss: 0.519812, acc.: 72.07%] [G loss: 1.135415]\n",
            "981 [D loss: 0.516749, acc.: 72.46%] [G loss: 1.137247]\n",
            "982 [D loss: 0.523922, acc.: 71.97%] [G loss: 1.120250]\n",
            "983 [D loss: 0.515403, acc.: 73.05%] [G loss: 1.118216]\n",
            "984 [D loss: 0.528333, acc.: 72.27%] [G loss: 1.110924]\n",
            "985 [D loss: 0.553354, acc.: 70.31%] [G loss: 1.133703]\n",
            "986 [D loss: 0.551034, acc.: 68.65%] [G loss: 1.117273]\n",
            "987 [D loss: 0.554285, acc.: 66.02%] [G loss: 1.141289]\n",
            "988 [D loss: 0.551940, acc.: 66.70%] [G loss: 1.117074]\n",
            "989 [D loss: 0.561631, acc.: 65.72%] [G loss: 1.096070]\n",
            "990 [D loss: 0.567687, acc.: 66.31%] [G loss: 1.115235]\n",
            "991 [D loss: 0.548504, acc.: 68.26%] [G loss: 1.101053]\n",
            "992 [D loss: 0.554571, acc.: 67.68%] [G loss: 1.104844]\n",
            "993 [D loss: 0.547590, acc.: 68.95%] [G loss: 1.119294]\n",
            "994 [D loss: 0.535240, acc.: 71.29%] [G loss: 1.113386]\n",
            "995 [D loss: 0.536450, acc.: 71.68%] [G loss: 1.120034]\n",
            "996 [D loss: 0.523423, acc.: 74.02%] [G loss: 1.124544]\n",
            "997 [D loss: 0.519793, acc.: 73.54%] [G loss: 1.117289]\n",
            "998 [D loss: 0.514811, acc.: 73.05%] [G loss: 1.109566]\n",
            "999 [D loss: 0.508100, acc.: 73.93%] [G loss: 1.136012]\n",
            "1000 [D loss: 0.514523, acc.: 73.14%] [G loss: 1.127569]\n",
            "1001 [D loss: 0.501150, acc.: 75.00%] [G loss: 1.145439]\n",
            "1002 [D loss: 0.503459, acc.: 73.63%] [G loss: 1.128908]\n",
            "1003 [D loss: 0.504647, acc.: 74.02%] [G loss: 1.125474]\n",
            "1004 [D loss: 0.497965, acc.: 74.02%] [G loss: 1.116066]\n",
            "1005 [D loss: 0.503814, acc.: 73.14%] [G loss: 1.118005]\n",
            "1006 [D loss: 0.506736, acc.: 72.07%] [G loss: 1.102566]\n",
            "1007 [D loss: 0.503938, acc.: 72.46%] [G loss: 1.109283]\n",
            "1008 [D loss: 0.505361, acc.: 72.85%] [G loss: 1.092210]\n",
            "1009 [D loss: 0.513764, acc.: 71.97%] [G loss: 1.093710]\n",
            "1010 [D loss: 0.516515, acc.: 71.78%] [G loss: 1.090739]\n",
            "1011 [D loss: 0.515160, acc.: 71.78%] [G loss: 1.073349]\n",
            "1012 [D loss: 0.514783, acc.: 72.46%] [G loss: 1.067153]\n",
            "1013 [D loss: 0.515642, acc.: 72.85%] [G loss: 1.062083]\n",
            "1014 [D loss: 0.534817, acc.: 71.78%] [G loss: 1.036693]\n",
            "1015 [D loss: 0.539496, acc.: 72.07%] [G loss: 1.044391]\n",
            "1016 [D loss: 0.544061, acc.: 71.00%] [G loss: 1.057909]\n",
            "1017 [D loss: 0.545224, acc.: 70.70%] [G loss: 1.047215]\n",
            "1018 [D loss: 0.539783, acc.: 71.29%] [G loss: 1.045866]\n",
            "1019 [D loss: 0.545185, acc.: 71.58%] [G loss: 1.038345]\n",
            "1020 [D loss: 0.546567, acc.: 71.97%] [G loss: 1.028347]\n",
            "1021 [D loss: 0.567875, acc.: 70.31%] [G loss: 1.043620]\n",
            "1022 [D loss: 0.548414, acc.: 71.39%] [G loss: 1.050768]\n",
            "1023 [D loss: 0.559266, acc.: 70.31%] [G loss: 1.041012]\n",
            "1024 [D loss: 0.557212, acc.: 69.92%] [G loss: 1.072794]\n",
            "1025 [D loss: 0.551952, acc.: 70.02%] [G loss: 1.115605]\n",
            "1026 [D loss: 0.554616, acc.: 67.68%] [G loss: 1.104545]\n",
            "1027 [D loss: 0.550668, acc.: 69.34%] [G loss: 1.125475]\n",
            "1028 [D loss: 0.542442, acc.: 70.31%] [G loss: 1.129436]\n",
            "1029 [D loss: 0.529613, acc.: 71.88%] [G loss: 1.121157]\n",
            "1030 [D loss: 0.540494, acc.: 70.31%] [G loss: 1.113359]\n",
            "1031 [D loss: 0.519351, acc.: 72.17%] [G loss: 1.164235]\n",
            "1032 [D loss: 0.504500, acc.: 73.44%] [G loss: 1.220745]\n",
            "1033 [D loss: 0.482194, acc.: 75.49%] [G loss: 1.225966]\n",
            "1034 [D loss: 0.472635, acc.: 75.68%] [G loss: 1.245039]\n",
            "1035 [D loss: 0.470837, acc.: 75.88%] [G loss: 1.291666]\n",
            "1036 [D loss: 0.466991, acc.: 75.98%] [G loss: 1.216430]\n",
            "1037 [D loss: 0.478670, acc.: 75.88%] [G loss: 1.229720]\n",
            "1038 [D loss: 0.478329, acc.: 75.59%] [G loss: 1.249093]\n",
            "1039 [D loss: 0.481708, acc.: 75.59%] [G loss: 1.270138]\n",
            "1040 [D loss: 0.488367, acc.: 74.61%] [G loss: 1.234233]\n",
            "1041 [D loss: 0.527379, acc.: 71.58%] [G loss: 1.185030]\n",
            "1042 [D loss: 0.523166, acc.: 72.75%] [G loss: 1.169948]\n",
            "1043 [D loss: 0.549525, acc.: 70.51%] [G loss: 1.169653]\n",
            "1044 [D loss: 0.558076, acc.: 68.65%] [G loss: 1.221137]\n",
            "1045 [D loss: 0.544397, acc.: 72.17%] [G loss: 1.167811]\n",
            "1046 [D loss: 0.545717, acc.: 71.58%] [G loss: 1.137413]\n",
            "1047 [D loss: 0.557359, acc.: 69.82%] [G loss: 1.107123]\n",
            "1048 [D loss: 0.542451, acc.: 70.61%] [G loss: 1.079978]\n",
            "1049 [D loss: 0.531497, acc.: 71.00%] [G loss: 1.089853]\n",
            "1050 [D loss: 0.535728, acc.: 71.19%] [G loss: 1.063156]\n",
            "1051 [D loss: 0.542629, acc.: 70.02%] [G loss: 1.051935]\n",
            "1052 [D loss: 0.542777, acc.: 70.02%] [G loss: 1.065616]\n",
            "1053 [D loss: 0.546557, acc.: 69.34%] [G loss: 1.059298]\n",
            "1054 [D loss: 0.533473, acc.: 70.61%] [G loss: 1.065889]\n",
            "1055 [D loss: 0.533957, acc.: 69.92%] [G loss: 1.079875]\n",
            "1056 [D loss: 0.534676, acc.: 69.92%] [G loss: 1.083249]\n",
            "1057 [D loss: 0.523553, acc.: 71.19%] [G loss: 1.086653]\n",
            "1058 [D loss: 0.540874, acc.: 69.73%] [G loss: 1.074268]\n",
            "1059 [D loss: 0.526933, acc.: 71.58%] [G loss: 1.076767]\n",
            "1060 [D loss: 0.534387, acc.: 70.31%] [G loss: 1.080512]\n",
            "1061 [D loss: 0.525164, acc.: 71.58%] [G loss: 1.078772]\n",
            "1062 [D loss: 0.528322, acc.: 70.12%] [G loss: 1.080094]\n",
            "1063 [D loss: 0.515073, acc.: 72.46%] [G loss: 1.096772]\n",
            "1064 [D loss: 0.526401, acc.: 70.51%] [G loss: 1.080017]\n",
            "1065 [D loss: 0.525919, acc.: 71.09%] [G loss: 1.049241]\n",
            "1066 [D loss: 0.519718, acc.: 71.29%] [G loss: 1.104197]\n",
            "1067 [D loss: 0.522325, acc.: 71.09%] [G loss: 1.078408]\n",
            "1068 [D loss: 0.515750, acc.: 72.46%] [G loss: 1.078130]\n",
            "1069 [D loss: 0.516282, acc.: 71.88%] [G loss: 1.089533]\n",
            "1070 [D loss: 0.526447, acc.: 71.09%] [G loss: 1.075071]\n",
            "1071 [D loss: 0.522732, acc.: 71.78%] [G loss: 1.079560]\n",
            "1072 [D loss: 0.524089, acc.: 71.29%] [G loss: 1.121809]\n",
            "1073 [D loss: 0.517518, acc.: 72.17%] [G loss: 1.117567]\n",
            "1074 [D loss: 0.525926, acc.: 71.29%] [G loss: 1.144728]\n",
            "1075 [D loss: 0.526187, acc.: 71.48%] [G loss: 1.152146]\n",
            "1076 [D loss: 0.523280, acc.: 72.56%] [G loss: 1.084589]\n",
            "1077 [D loss: 0.533648, acc.: 71.09%] [G loss: 1.082410]\n",
            "1078 [D loss: 0.537499, acc.: 71.68%] [G loss: 1.082745]\n",
            "1079 [D loss: 0.540710, acc.: 70.80%] [G loss: 1.067189]\n",
            "1080 [D loss: 0.540664, acc.: 71.19%] [G loss: 1.063267]\n",
            "1081 [D loss: 0.545033, acc.: 69.14%] [G loss: 1.063412]\n",
            "1082 [D loss: 0.542330, acc.: 70.21%] [G loss: 1.058249]\n",
            "1083 [D loss: 0.546454, acc.: 68.46%] [G loss: 1.034206]\n",
            "1084 [D loss: 0.548169, acc.: 68.55%] [G loss: 1.022609]\n",
            "1085 [D loss: 0.546275, acc.: 68.95%] [G loss: 1.027452]\n",
            "1086 [D loss: 0.551475, acc.: 69.92%] [G loss: 1.037207]\n",
            "1087 [D loss: 0.556876, acc.: 69.14%] [G loss: 1.018370]\n",
            "1088 [D loss: 0.570556, acc.: 67.77%] [G loss: 1.047927]\n",
            "1089 [D loss: 0.568350, acc.: 66.11%] [G loss: 1.039153]\n",
            "1090 [D loss: 0.558460, acc.: 67.29%] [G loss: 1.082782]\n",
            "1091 [D loss: 0.555418, acc.: 67.19%] [G loss: 1.064602]\n",
            "1092 [D loss: 0.554179, acc.: 67.77%] [G loss: 1.059032]\n",
            "1093 [D loss: 0.555173, acc.: 68.26%] [G loss: 1.048281]\n",
            "1094 [D loss: 0.559316, acc.: 69.14%] [G loss: 1.055934]\n",
            "1095 [D loss: 0.564065, acc.: 67.68%] [G loss: 1.058705]\n",
            "1096 [D loss: 0.540422, acc.: 69.63%] [G loss: 1.065931]\n",
            "1097 [D loss: 0.554940, acc.: 69.34%] [G loss: 1.058545]\n",
            "1098 [D loss: 0.545961, acc.: 70.21%] [G loss: 1.040737]\n",
            "1099 [D loss: 0.542963, acc.: 69.63%] [G loss: 1.023356]\n",
            "1100 [D loss: 0.532817, acc.: 70.61%] [G loss: 1.049693]\n",
            "1101 [D loss: 0.538856, acc.: 69.63%] [G loss: 1.050631]\n",
            "1102 [D loss: 0.542596, acc.: 69.43%] [G loss: 1.076018]\n",
            "1103 [D loss: 0.534419, acc.: 70.02%] [G loss: 1.038815]\n",
            "1104 [D loss: 0.536373, acc.: 69.34%] [G loss: 1.042431]\n",
            "1105 [D loss: 0.535200, acc.: 68.65%] [G loss: 1.064358]\n",
            "1106 [D loss: 0.528369, acc.: 70.02%] [G loss: 1.068359]\n",
            "1107 [D loss: 0.523251, acc.: 69.92%] [G loss: 1.091021]\n",
            "1108 [D loss: 0.527175, acc.: 69.24%] [G loss: 1.057392]\n",
            "1109 [D loss: 0.527395, acc.: 69.73%] [G loss: 1.061018]\n",
            "1110 [D loss: 0.526183, acc.: 70.31%] [G loss: 1.047910]\n",
            "1111 [D loss: 0.532868, acc.: 68.85%] [G loss: 1.025585]\n",
            "1112 [D loss: 0.536932, acc.: 67.97%] [G loss: 1.039393]\n",
            "1113 [D loss: 0.543984, acc.: 67.97%] [G loss: 1.060010]\n",
            "1114 [D loss: 0.535453, acc.: 69.04%] [G loss: 1.091049]\n",
            "1115 [D loss: 0.541405, acc.: 68.75%] [G loss: 1.033366]\n",
            "1116 [D loss: 0.548124, acc.: 67.29%] [G loss: 1.056294]\n",
            "1117 [D loss: 0.542122, acc.: 68.26%] [G loss: 1.061645]\n",
            "1118 [D loss: 0.546263, acc.: 67.19%] [G loss: 1.049256]\n",
            "1119 [D loss: 0.538424, acc.: 68.26%] [G loss: 1.034604]\n",
            "1120 [D loss: 0.550551, acc.: 67.29%] [G loss: 1.031409]\n",
            "1121 [D loss: 0.546017, acc.: 68.85%] [G loss: 1.031465]\n",
            "1122 [D loss: 0.550600, acc.: 68.85%] [G loss: 1.013866]\n",
            "1123 [D loss: 0.546327, acc.: 70.31%] [G loss: 0.965189]\n",
            "1124 [D loss: 0.549531, acc.: 69.34%] [G loss: 1.030576]\n",
            "1125 [D loss: 0.552292, acc.: 68.55%] [G loss: 1.061752]\n",
            "1126 [D loss: 0.536564, acc.: 70.51%] [G loss: 1.050297]\n",
            "1127 [D loss: 0.540048, acc.: 69.34%] [G loss: 1.051244]\n",
            "1128 [D loss: 0.546397, acc.: 68.75%] [G loss: 1.059700]\n",
            "1129 [D loss: 0.537782, acc.: 70.21%] [G loss: 1.032112]\n",
            "1130 [D loss: 0.541726, acc.: 69.82%] [G loss: 1.040910]\n",
            "1131 [D loss: 0.546531, acc.: 68.55%] [G loss: 1.041247]\n",
            "1132 [D loss: 0.543397, acc.: 69.34%] [G loss: 1.069143]\n",
            "1133 [D loss: 0.535525, acc.: 70.02%] [G loss: 1.061848]\n",
            "1134 [D loss: 0.538633, acc.: 69.63%] [G loss: 1.050636]\n",
            "1135 [D loss: 0.531008, acc.: 71.29%] [G loss: 1.042529]\n",
            "1136 [D loss: 0.532660, acc.: 70.90%] [G loss: 1.007912]\n",
            "1137 [D loss: 0.534811, acc.: 71.29%] [G loss: 1.033171]\n",
            "1138 [D loss: 0.535139, acc.: 70.31%] [G loss: 1.027402]\n",
            "1139 [D loss: 0.539895, acc.: 69.43%] [G loss: 1.054060]\n",
            "1140 [D loss: 0.536419, acc.: 70.21%] [G loss: 1.061494]\n",
            "1141 [D loss: 0.536703, acc.: 69.73%] [G loss: 1.057523]\n",
            "1142 [D loss: 0.538728, acc.: 70.02%] [G loss: 1.073216]\n",
            "1143 [D loss: 0.537232, acc.: 70.70%] [G loss: 1.066909]\n",
            "1144 [D loss: 0.534967, acc.: 71.29%] [G loss: 1.054547]\n",
            "1145 [D loss: 0.534438, acc.: 70.31%] [G loss: 1.045367]\n",
            "1146 [D loss: 0.527818, acc.: 71.58%] [G loss: 1.047324]\n",
            "1147 [D loss: 0.529541, acc.: 71.00%] [G loss: 1.034428]\n",
            "1148 [D loss: 0.526820, acc.: 71.97%] [G loss: 1.058561]\n",
            "1149 [D loss: 0.532178, acc.: 70.70%] [G loss: 1.046163]\n",
            "1150 [D loss: 0.524636, acc.: 71.39%] [G loss: 1.040641]\n",
            "1151 [D loss: 0.523736, acc.: 72.27%] [G loss: 1.059337]\n",
            "1152 [D loss: 0.528419, acc.: 70.80%] [G loss: 1.058942]\n",
            "1153 [D loss: 0.528830, acc.: 72.36%] [G loss: 1.049353]\n",
            "1154 [D loss: 0.529376, acc.: 71.68%] [G loss: 1.086112]\n",
            "1155 [D loss: 0.528491, acc.: 72.27%] [G loss: 1.042208]\n",
            "1156 [D loss: 0.536131, acc.: 70.21%] [G loss: 1.044641]\n",
            "1157 [D loss: 0.535857, acc.: 71.39%] [G loss: 1.045176]\n",
            "1158 [D loss: 0.540387, acc.: 69.53%] [G loss: 1.046329]\n",
            "1159 [D loss: 0.537594, acc.: 69.82%] [G loss: 1.033698]\n",
            "1160 [D loss: 0.546657, acc.: 68.75%] [G loss: 1.035969]\n",
            "1161 [D loss: 0.547282, acc.: 67.68%] [G loss: 1.052982]\n",
            "1162 [D loss: 0.524678, acc.: 71.97%] [G loss: 1.037886]\n",
            "1163 [D loss: 0.535083, acc.: 69.63%] [G loss: 1.055818]\n",
            "1164 [D loss: 0.517979, acc.: 73.73%] [G loss: 1.086507]\n",
            "1165 [D loss: 0.507801, acc.: 74.90%] [G loss: 1.088583]\n",
            "1166 [D loss: 0.504432, acc.: 75.00%] [G loss: 1.098591]\n",
            "1167 [D loss: 0.498004, acc.: 75.10%] [G loss: 1.087184]\n",
            "1168 [D loss: 0.493792, acc.: 75.10%] [G loss: 1.098783]\n",
            "1169 [D loss: 0.498106, acc.: 73.63%] [G loss: 1.097619]\n",
            "1170 [D loss: 0.504643, acc.: 72.56%] [G loss: 1.122279]\n",
            "1171 [D loss: 0.506165, acc.: 71.78%] [G loss: 1.093080]\n",
            "1172 [D loss: 0.528093, acc.: 71.29%] [G loss: 1.118810]\n",
            "1173 [D loss: 0.562623, acc.: 68.26%] [G loss: 1.143847]\n",
            "1174 [D loss: 0.553660, acc.: 71.68%] [G loss: 1.083741]\n",
            "1175 [D loss: 0.578446, acc.: 71.48%] [G loss: 1.073542]\n",
            "1176 [D loss: 0.575499, acc.: 70.90%] [G loss: 1.059631]\n",
            "1177 [D loss: 0.575434, acc.: 68.85%] [G loss: 1.051255]\n",
            "1178 [D loss: 0.545970, acc.: 68.95%] [G loss: 1.087036]\n",
            "1179 [D loss: 0.525971, acc.: 71.19%] [G loss: 1.098166]\n",
            "1180 [D loss: 0.537786, acc.: 68.07%] [G loss: 1.109372]\n",
            "1181 [D loss: 0.534460, acc.: 69.82%] [G loss: 1.103627]\n",
            "1182 [D loss: 0.536208, acc.: 69.24%] [G loss: 1.087214]\n",
            "1183 [D loss: 0.534881, acc.: 70.31%] [G loss: 1.117239]\n",
            "1184 [D loss: 0.538429, acc.: 69.73%] [G loss: 1.113714]\n",
            "1185 [D loss: 0.541056, acc.: 69.04%] [G loss: 1.090749]\n",
            "1186 [D loss: 0.548680, acc.: 67.58%] [G loss: 1.107193]\n",
            "1187 [D loss: 0.550715, acc.: 69.43%] [G loss: 1.076886]\n",
            "1188 [D loss: 0.570631, acc.: 68.36%] [G loss: 1.081292]\n",
            "1189 [D loss: 0.595880, acc.: 67.87%] [G loss: 1.055470]\n",
            "1190 [D loss: 0.571699, acc.: 69.43%] [G loss: 1.091920]\n",
            "1191 [D loss: 0.579065, acc.: 69.24%] [G loss: 1.096819]\n",
            "1192 [D loss: 0.563173, acc.: 70.21%] [G loss: 1.146982]\n",
            "1193 [D loss: 0.566161, acc.: 70.02%] [G loss: 1.141770]\n",
            "1194 [D loss: 0.559294, acc.: 70.21%] [G loss: 1.149625]\n",
            "1195 [D loss: 0.539505, acc.: 71.09%] [G loss: 1.149224]\n",
            "1196 [D loss: 0.536920, acc.: 70.70%] [G loss: 1.085692]\n",
            "1197 [D loss: 0.543771, acc.: 69.53%] [G loss: 1.079793]\n",
            "1198 [D loss: 0.531783, acc.: 71.58%] [G loss: 1.062728]\n",
            "1199 [D loss: 0.530575, acc.: 71.97%] [G loss: 1.048978]\n",
            "1200 [D loss: 0.532361, acc.: 71.48%] [G loss: 1.054575]\n",
            "1201 [D loss: 0.530662, acc.: 71.19%] [G loss: 1.043180]\n",
            "1202 [D loss: 0.534174, acc.: 70.31%] [G loss: 1.059453]\n",
            "1203 [D loss: 0.529515, acc.: 70.80%] [G loss: 1.062083]\n",
            "1204 [D loss: 0.532989, acc.: 70.41%] [G loss: 1.055956]\n",
            "1205 [D loss: 0.532309, acc.: 70.70%] [G loss: 1.082970]\n",
            "1206 [D loss: 0.534440, acc.: 70.41%] [G loss: 1.076052]\n",
            "1207 [D loss: 0.534452, acc.: 70.70%] [G loss: 1.058184]\n",
            "1208 [D loss: 0.544147, acc.: 69.34%] [G loss: 1.058599]\n",
            "1209 [D loss: 0.550298, acc.: 68.36%] [G loss: 1.051456]\n",
            "1210 [D loss: 0.552738, acc.: 67.68%] [G loss: 1.061213]\n",
            "1211 [D loss: 0.548843, acc.: 68.55%] [G loss: 1.061792]\n",
            "1212 [D loss: 0.548191, acc.: 67.77%] [G loss: 1.044169]\n",
            "1213 [D loss: 0.557097, acc.: 67.97%] [G loss: 1.069592]\n",
            "1214 [D loss: 0.535963, acc.: 71.09%] [G loss: 1.079371]\n",
            "1215 [D loss: 0.524360, acc.: 72.66%] [G loss: 1.058689]\n",
            "1216 [D loss: 0.531055, acc.: 71.88%] [G loss: 1.032990]\n",
            "1217 [D loss: 0.520882, acc.: 72.56%] [G loss: 1.043144]\n",
            "1218 [D loss: 0.521806, acc.: 71.19%] [G loss: 1.053189]\n",
            "1219 [D loss: 0.534804, acc.: 69.24%] [G loss: 1.056908]\n",
            "1220 [D loss: 0.542925, acc.: 69.92%] [G loss: 1.059913]\n",
            "1221 [D loss: 0.541792, acc.: 69.14%] [G loss: 1.068019]\n",
            "1222 [D loss: 0.564193, acc.: 67.19%] [G loss: 1.056856]\n",
            "1223 [D loss: 0.552819, acc.: 68.65%] [G loss: 1.090488]\n",
            "1224 [D loss: 0.583573, acc.: 65.14%] [G loss: 1.051944]\n",
            "1225 [D loss: 0.576400, acc.: 66.60%] [G loss: 1.047422]\n",
            "1226 [D loss: 0.578340, acc.: 66.50%] [G loss: 1.023443]\n",
            "1227 [D loss: 0.596534, acc.: 65.14%] [G loss: 1.038777]\n",
            "1228 [D loss: 0.587244, acc.: 66.89%] [G loss: 1.057812]\n",
            "1229 [D loss: 0.568298, acc.: 68.55%] [G loss: 1.057198]\n",
            "1230 [D loss: 0.551934, acc.: 70.70%] [G loss: 1.077751]\n",
            "1231 [D loss: 0.551460, acc.: 70.41%] [G loss: 1.073927]\n",
            "1232 [D loss: 0.540080, acc.: 70.12%] [G loss: 1.067908]\n",
            "1233 [D loss: 0.540983, acc.: 69.63%] [G loss: 1.042900]\n",
            "1234 [D loss: 0.543648, acc.: 69.53%] [G loss: 1.037476]\n",
            "1235 [D loss: 0.552669, acc.: 68.26%] [G loss: 1.071777]\n",
            "1236 [D loss: 0.553078, acc.: 68.95%] [G loss: 1.056682]\n",
            "1237 [D loss: 0.550149, acc.: 69.34%] [G loss: 1.039572]\n",
            "1238 [D loss: 0.560401, acc.: 68.46%] [G loss: 1.018271]\n",
            "1239 [D loss: 0.561114, acc.: 67.58%] [G loss: 1.054606]\n",
            "1240 [D loss: 0.564076, acc.: 67.97%] [G loss: 0.992048]\n",
            "1241 [D loss: 0.565914, acc.: 69.04%] [G loss: 1.027739]\n",
            "1242 [D loss: 0.570569, acc.: 66.31%] [G loss: 1.030656]\n",
            "1243 [D loss: 0.565121, acc.: 66.41%] [G loss: 1.046681]\n",
            "1244 [D loss: 0.560288, acc.: 67.87%] [G loss: 1.046581]\n",
            "1245 [D loss: 0.554134, acc.: 66.80%] [G loss: 1.065093]\n",
            "1246 [D loss: 0.556342, acc.: 67.29%] [G loss: 1.054156]\n",
            "1247 [D loss: 0.543353, acc.: 70.02%] [G loss: 1.063924]\n",
            "1248 [D loss: 0.543936, acc.: 68.07%] [G loss: 1.054660]\n",
            "1249 [D loss: 0.542010, acc.: 69.34%] [G loss: 1.069324]\n",
            "1250 [D loss: 0.538633, acc.: 70.02%] [G loss: 1.076492]\n",
            "1251 [D loss: 0.538477, acc.: 69.73%] [G loss: 1.042184]\n",
            "1252 [D loss: 0.544808, acc.: 69.34%] [G loss: 1.011660]\n",
            "1253 [D loss: 0.553231, acc.: 68.75%] [G loss: 1.030292]\n",
            "1254 [D loss: 0.555759, acc.: 70.12%] [G loss: 1.027790]\n",
            "1255 [D loss: 0.556373, acc.: 68.55%] [G loss: 0.987002]\n",
            "1256 [D loss: 0.550700, acc.: 68.75%] [G loss: 1.003111]\n",
            "1257 [D loss: 0.555537, acc.: 70.41%] [G loss: 0.976197]\n",
            "1258 [D loss: 0.561868, acc.: 68.16%] [G loss: 0.975313]\n",
            "1259 [D loss: 0.556153, acc.: 68.07%] [G loss: 0.973423]\n",
            "1260 [D loss: 0.546532, acc.: 69.14%] [G loss: 1.000062]\n",
            "1261 [D loss: 0.538258, acc.: 71.78%] [G loss: 1.005963]\n",
            "1262 [D loss: 0.547073, acc.: 68.55%] [G loss: 1.003774]\n",
            "1263 [D loss: 0.536956, acc.: 70.80%] [G loss: 1.024101]\n",
            "1264 [D loss: 0.539207, acc.: 70.41%] [G loss: 1.013755]\n",
            "1265 [D loss: 0.539511, acc.: 70.12%] [G loss: 0.994869]\n",
            "1266 [D loss: 0.540339, acc.: 71.19%] [G loss: 1.013394]\n",
            "1267 [D loss: 0.544178, acc.: 71.19%] [G loss: 1.011934]\n",
            "1268 [D loss: 0.546106, acc.: 71.29%] [G loss: 1.032654]\n",
            "1269 [D loss: 0.546036, acc.: 70.90%] [G loss: 1.028088]\n",
            "1270 [D loss: 0.538274, acc.: 71.29%] [G loss: 1.012794]\n",
            "1271 [D loss: 0.548419, acc.: 69.63%] [G loss: 1.011575]\n",
            "1272 [D loss: 0.543675, acc.: 69.34%] [G loss: 1.013526]\n",
            "1273 [D loss: 0.541979, acc.: 70.31%] [G loss: 1.025620]\n",
            "1274 [D loss: 0.550866, acc.: 68.26%] [G loss: 1.034623]\n",
            "1275 [D loss: 0.545558, acc.: 70.80%] [G loss: 1.051894]\n",
            "1276 [D loss: 0.544772, acc.: 70.12%] [G loss: 1.044208]\n",
            "1277 [D loss: 0.544513, acc.: 69.82%] [G loss: 1.035101]\n",
            "1278 [D loss: 0.551838, acc.: 69.34%] [G loss: 1.049185]\n",
            "1279 [D loss: 0.546781, acc.: 68.65%] [G loss: 1.045684]\n",
            "1280 [D loss: 0.544500, acc.: 69.92%] [G loss: 1.038992]\n",
            "1281 [D loss: 0.547709, acc.: 68.85%] [G loss: 1.040851]\n",
            "1282 [D loss: 0.542339, acc.: 69.63%] [G loss: 1.016570]\n",
            "1283 [D loss: 0.542451, acc.: 69.82%] [G loss: 1.010586]\n",
            "1284 [D loss: 0.541363, acc.: 70.31%] [G loss: 1.009631]\n",
            "1285 [D loss: 0.546442, acc.: 69.14%] [G loss: 1.009397]\n",
            "1286 [D loss: 0.536181, acc.: 71.68%] [G loss: 1.005963]\n",
            "1287 [D loss: 0.532289, acc.: 72.27%] [G loss: 1.024955]\n",
            "1288 [D loss: 0.528022, acc.: 72.95%] [G loss: 1.006496]\n",
            "1289 [D loss: 0.527599, acc.: 72.56%] [G loss: 1.020907]\n",
            "1290 [D loss: 0.522437, acc.: 72.56%] [G loss: 1.016196]\n",
            "1291 [D loss: 0.518618, acc.: 72.07%] [G loss: 1.024758]\n",
            "1292 [D loss: 0.517030, acc.: 72.75%] [G loss: 1.037063]\n",
            "1293 [D loss: 0.510778, acc.: 72.85%] [G loss: 1.035789]\n",
            "1294 [D loss: 0.511237, acc.: 71.68%] [G loss: 1.033103]\n",
            "1295 [D loss: 0.511044, acc.: 71.48%] [G loss: 1.036508]\n",
            "1296 [D loss: 0.520773, acc.: 69.63%] [G loss: 1.044304]\n",
            "1297 [D loss: 0.525510, acc.: 70.02%] [G loss: 1.062574]\n",
            "1298 [D loss: 0.534060, acc.: 68.95%] [G loss: 1.054646]\n",
            "1299 [D loss: 0.527552, acc.: 69.53%] [G loss: 1.063198]\n",
            "1300 [D loss: 0.534208, acc.: 71.48%] [G loss: 1.095600]\n",
            "1301 [D loss: 0.502842, acc.: 74.61%] [G loss: 1.105228]\n",
            "1302 [D loss: 0.495013, acc.: 74.90%] [G loss: 1.137494]\n",
            "1303 [D loss: 0.482538, acc.: 76.37%] [G loss: 1.176603]\n",
            "1304 [D loss: 0.486083, acc.: 75.98%] [G loss: 1.168896]\n",
            "1305 [D loss: 0.497199, acc.: 73.83%] [G loss: 1.135193]\n",
            "1306 [D loss: 0.523896, acc.: 71.09%] [G loss: 1.121943]\n",
            "1307 [D loss: 0.581020, acc.: 66.60%] [G loss: 1.116063]\n",
            "1308 [D loss: 0.568879, acc.: 66.60%] [G loss: 1.130307]\n",
            "1309 [D loss: 0.636511, acc.: 62.11%] [G loss: 1.117947]\n",
            "1310 [D loss: 0.612205, acc.: 60.94%] [G loss: 1.113543]\n",
            "1311 [D loss: 0.590352, acc.: 63.77%] [G loss: 1.126203]\n",
            "1312 [D loss: 0.566229, acc.: 66.11%] [G loss: 1.118057]\n",
            "1313 [D loss: 0.560575, acc.: 66.99%] [G loss: 1.100494]\n",
            "1314 [D loss: 0.553181, acc.: 69.63%] [G loss: 1.080631]\n",
            "1315 [D loss: 0.546724, acc.: 70.90%] [G loss: 1.049794]\n",
            "1316 [D loss: 0.548839, acc.: 70.51%] [G loss: 1.092740]\n",
            "1317 [D loss: 0.535158, acc.: 71.68%] [G loss: 1.092312]\n",
            "1318 [D loss: 0.548668, acc.: 69.24%] [G loss: 1.072384]\n",
            "1319 [D loss: 0.546783, acc.: 69.63%] [G loss: 1.071443]\n",
            "1320 [D loss: 0.550495, acc.: 69.63%] [G loss: 1.046963]\n",
            "1321 [D loss: 0.544138, acc.: 70.41%] [G loss: 1.024391]\n",
            "1322 [D loss: 0.550956, acc.: 68.95%] [G loss: 1.043368]\n",
            "1323 [D loss: 0.549348, acc.: 69.63%] [G loss: 1.037649]\n",
            "1324 [D loss: 0.559243, acc.: 66.70%] [G loss: 1.043416]\n",
            "1325 [D loss: 0.551910, acc.: 68.07%] [G loss: 1.033082]\n",
            "1326 [D loss: 0.553908, acc.: 67.97%] [G loss: 1.019696]\n",
            "1327 [D loss: 0.559584, acc.: 67.77%] [G loss: 1.005977]\n",
            "1328 [D loss: 0.554771, acc.: 67.97%] [G loss: 1.012521]\n",
            "1329 [D loss: 0.560922, acc.: 67.58%] [G loss: 0.977915]\n",
            "1330 [D loss: 0.576662, acc.: 68.95%] [G loss: 0.993278]\n",
            "1331 [D loss: 0.579133, acc.: 68.55%] [G loss: 0.971675]\n",
            "1332 [D loss: 0.581254, acc.: 68.95%] [G loss: 0.978489]\n",
            "1333 [D loss: 0.569659, acc.: 70.70%] [G loss: 1.006494]\n",
            "1334 [D loss: 0.545357, acc.: 70.70%] [G loss: 1.019746]\n",
            "1335 [D loss: 0.529056, acc.: 72.17%] [G loss: 1.039166]\n",
            "1336 [D loss: 0.522440, acc.: 72.07%] [G loss: 1.035364]\n",
            "1337 [D loss: 0.531411, acc.: 71.48%] [G loss: 1.040126]\n",
            "1338 [D loss: 0.519944, acc.: 71.88%] [G loss: 1.039075]\n",
            "1339 [D loss: 0.518492, acc.: 71.58%] [G loss: 1.039770]\n",
            "1340 [D loss: 0.531137, acc.: 69.92%] [G loss: 1.058708]\n",
            "1341 [D loss: 0.518820, acc.: 71.88%] [G loss: 1.042474]\n",
            "1342 [D loss: 0.522744, acc.: 70.31%] [G loss: 1.051471]\n",
            "1343 [D loss: 0.529530, acc.: 69.34%] [G loss: 1.082763]\n",
            "1344 [D loss: 0.520166, acc.: 70.12%] [G loss: 1.070159]\n",
            "1345 [D loss: 0.524127, acc.: 70.70%] [G loss: 1.097138]\n",
            "1346 [D loss: 0.533182, acc.: 70.41%] [G loss: 1.077882]\n",
            "1347 [D loss: 0.544881, acc.: 69.53%] [G loss: 1.075092]\n",
            "1348 [D loss: 0.542366, acc.: 70.21%] [G loss: 1.059752]\n",
            "1349 [D loss: 0.555952, acc.: 68.75%] [G loss: 1.029330]\n",
            "1350 [D loss: 0.574152, acc.: 67.38%] [G loss: 1.029881]\n",
            "1351 [D loss: 0.586730, acc.: 65.23%] [G loss: 1.024003]\n",
            "1352 [D loss: 0.609528, acc.: 62.50%] [G loss: 1.017499]\n",
            "1353 [D loss: 0.616970, acc.: 58.59%] [G loss: 1.022930]\n",
            "1354 [D loss: 0.629594, acc.: 59.28%] [G loss: 1.028502]\n",
            "1355 [D loss: 0.606226, acc.: 61.62%] [G loss: 1.033008]\n",
            "1356 [D loss: 0.577710, acc.: 66.60%] [G loss: 1.055880]\n",
            "1357 [D loss: 0.567956, acc.: 67.38%] [G loss: 1.047142]\n",
            "1358 [D loss: 0.549289, acc.: 70.02%] [G loss: 1.058964]\n",
            "1359 [D loss: 0.546981, acc.: 70.21%] [G loss: 1.057412]\n",
            "1360 [D loss: 0.538660, acc.: 70.70%] [G loss: 1.080679]\n",
            "1361 [D loss: 0.543128, acc.: 70.41%] [G loss: 1.049419]\n",
            "1362 [D loss: 0.539321, acc.: 70.90%] [G loss: 1.046108]\n",
            "1363 [D loss: 0.554695, acc.: 68.36%] [G loss: 1.006560]\n",
            "1364 [D loss: 0.556659, acc.: 67.38%] [G loss: 0.995281]\n",
            "1365 [D loss: 0.562104, acc.: 67.38%] [G loss: 1.016491]\n",
            "1366 [D loss: 0.556305, acc.: 67.77%] [G loss: 0.994954]\n",
            "1367 [D loss: 0.552150, acc.: 67.38%] [G loss: 0.998970]\n",
            "1368 [D loss: 0.548837, acc.: 66.21%] [G loss: 1.016214]\n",
            "1369 [D loss: 0.546723, acc.: 67.68%] [G loss: 0.969862]\n",
            "1370 [D loss: 0.541445, acc.: 69.53%] [G loss: 0.979914]\n",
            "1371 [D loss: 0.555038, acc.: 67.48%] [G loss: 0.972578]\n",
            "1372 [D loss: 0.559450, acc.: 66.99%] [G loss: 0.976813]\n",
            "1373 [D loss: 0.562750, acc.: 69.92%] [G loss: 0.976104]\n",
            "1374 [D loss: 0.557137, acc.: 69.92%] [G loss: 0.965399]\n",
            "1375 [D loss: 0.563891, acc.: 68.65%] [G loss: 0.970353]\n",
            "1376 [D loss: 0.549268, acc.: 71.29%] [G loss: 0.978152]\n",
            "1377 [D loss: 0.552035, acc.: 70.02%] [G loss: 0.975847]\n",
            "1378 [D loss: 0.540450, acc.: 70.80%] [G loss: 0.997097]\n",
            "1379 [D loss: 0.540885, acc.: 69.82%] [G loss: 0.990443]\n",
            "1380 [D loss: 0.543273, acc.: 70.31%] [G loss: 0.997598]\n",
            "1381 [D loss: 0.535466, acc.: 70.80%] [G loss: 1.013850]\n",
            "1382 [D loss: 0.538337, acc.: 70.41%] [G loss: 1.006708]\n",
            "1383 [D loss: 0.528057, acc.: 71.09%] [G loss: 1.033467]\n",
            "1384 [D loss: 0.518965, acc.: 72.95%] [G loss: 1.038789]\n",
            "1385 [D loss: 0.522474, acc.: 72.17%] [G loss: 1.048639]\n",
            "1386 [D loss: 0.519861, acc.: 72.27%] [G loss: 1.031371]\n",
            "1387 [D loss: 0.528429, acc.: 70.21%] [G loss: 1.016017]\n",
            "1388 [D loss: 0.540937, acc.: 67.68%] [G loss: 1.010135]\n",
            "1389 [D loss: 0.571330, acc.: 65.82%] [G loss: 0.977626]\n",
            "1390 [D loss: 0.610585, acc.: 65.04%] [G loss: 1.007342]\n",
            "1391 [D loss: 0.609249, acc.: 66.80%] [G loss: 1.048379]\n",
            "1392 [D loss: 0.603322, acc.: 68.36%] [G loss: 1.126002]\n",
            "1393 [D loss: 0.588651, acc.: 69.24%] [G loss: 1.174961]\n",
            "1394 [D loss: 0.539600, acc.: 71.78%] [G loss: 1.272858]\n",
            "1395 [D loss: 0.525246, acc.: 73.24%] [G loss: 1.206758]\n",
            "1396 [D loss: 0.524935, acc.: 72.27%] [G loss: 1.175181]\n",
            "1397 [D loss: 0.514902, acc.: 72.66%] [G loss: 1.195041]\n",
            "1398 [D loss: 0.505613, acc.: 73.24%] [G loss: 1.124326]\n",
            "1399 [D loss: 0.502628, acc.: 72.56%] [G loss: 1.129092]\n",
            "1400 [D loss: 0.501599, acc.: 72.17%] [G loss: 1.166381]\n",
            "1401 [D loss: 0.503461, acc.: 72.66%] [G loss: 1.146004]\n",
            "1402 [D loss: 0.542515, acc.: 70.12%] [G loss: 1.175380]\n",
            "1403 [D loss: 0.568546, acc.: 68.85%] [G loss: 1.128626]\n",
            "1404 [D loss: 0.603671, acc.: 66.80%] [G loss: 1.088808]\n",
            "1405 [D loss: 0.601998, acc.: 66.02%] [G loss: 1.051623]\n",
            "1406 [D loss: 0.599246, acc.: 64.36%] [G loss: 1.093522]\n",
            "1407 [D loss: 0.581071, acc.: 66.80%] [G loss: 1.067405]\n",
            "1408 [D loss: 0.574573, acc.: 67.19%] [G loss: 1.081256]\n",
            "1409 [D loss: 0.566031, acc.: 66.31%] [G loss: 1.049319]\n",
            "1410 [D loss: 0.548052, acc.: 69.34%] [G loss: 1.085903]\n",
            "1411 [D loss: 0.522624, acc.: 72.66%] [G loss: 1.086045]\n",
            "1412 [D loss: 0.512727, acc.: 73.73%] [G loss: 1.073204]\n",
            "1413 [D loss: 0.512329, acc.: 73.34%] [G loss: 1.073917]\n",
            "1414 [D loss: 0.512881, acc.: 72.56%] [G loss: 1.059546]\n",
            "1415 [D loss: 0.517760, acc.: 71.09%] [G loss: 1.051994]\n",
            "1416 [D loss: 0.517181, acc.: 70.80%] [G loss: 1.059948]\n",
            "1417 [D loss: 0.530493, acc.: 68.85%] [G loss: 1.072537]\n",
            "1418 [D loss: 0.531459, acc.: 68.95%] [G loss: 1.050515]\n",
            "1419 [D loss: 0.550234, acc.: 67.87%] [G loss: 1.084344]\n",
            "1420 [D loss: 0.556397, acc.: 66.60%] [G loss: 1.081526]\n",
            "1421 [D loss: 0.581669, acc.: 65.23%] [G loss: 1.049755]\n",
            "1422 [D loss: 0.575665, acc.: 66.70%] [G loss: 1.030499]\n",
            "1423 [D loss: 0.584549, acc.: 66.41%] [G loss: 1.001260]\n",
            "1424 [D loss: 0.576141, acc.: 66.50%] [G loss: 1.025852]\n",
            "1425 [D loss: 0.560043, acc.: 70.02%] [G loss: 1.032704]\n",
            "1426 [D loss: 0.574274, acc.: 67.68%] [G loss: 1.008463]\n",
            "1427 [D loss: 0.565609, acc.: 68.16%] [G loss: 1.039022]\n",
            "1428 [D loss: 0.556664, acc.: 70.02%] [G loss: 1.015816]\n",
            "1429 [D loss: 0.549055, acc.: 70.31%] [G loss: 1.026554]\n",
            "1430 [D loss: 0.545151, acc.: 71.29%] [G loss: 1.034908]\n",
            "1431 [D loss: 0.540059, acc.: 71.09%] [G loss: 1.039230]\n",
            "1432 [D loss: 0.537326, acc.: 70.80%] [G loss: 1.033297]\n",
            "1433 [D loss: 0.533009, acc.: 71.48%] [G loss: 1.022276]\n",
            "1434 [D loss: 0.532108, acc.: 71.78%] [G loss: 1.022564]\n",
            "1435 [D loss: 0.543766, acc.: 68.36%] [G loss: 1.029408]\n",
            "1436 [D loss: 0.537069, acc.: 71.00%] [G loss: 1.028053]\n",
            "1437 [D loss: 0.547353, acc.: 68.16%] [G loss: 0.993692]\n",
            "1438 [D loss: 0.564019, acc.: 66.11%] [G loss: 0.968663]\n",
            "1439 [D loss: 0.571418, acc.: 65.53%] [G loss: 0.975827]\n",
            "1440 [D loss: 0.570571, acc.: 65.33%] [G loss: 0.989737]\n",
            "1441 [D loss: 0.566182, acc.: 64.45%] [G loss: 1.009011]\n",
            "1442 [D loss: 0.558943, acc.: 66.70%] [G loss: 1.004117]\n",
            "1443 [D loss: 0.553236, acc.: 66.50%] [G loss: 1.015167]\n",
            "1444 [D loss: 0.548437, acc.: 68.16%] [G loss: 1.027927]\n",
            "1445 [D loss: 0.534129, acc.: 71.29%] [G loss: 1.029942]\n",
            "1446 [D loss: 0.528225, acc.: 71.58%] [G loss: 1.050910]\n",
            "1447 [D loss: 0.528400, acc.: 72.17%] [G loss: 1.061756]\n",
            "1448 [D loss: 0.530122, acc.: 71.58%] [G loss: 1.038019]\n",
            "1449 [D loss: 0.526935, acc.: 71.29%] [G loss: 1.053994]\n",
            "1450 [D loss: 0.536935, acc.: 70.61%] [G loss: 1.057825]\n",
            "1451 [D loss: 0.543865, acc.: 70.31%] [G loss: 1.068034]\n",
            "1452 [D loss: 0.545463, acc.: 69.63%] [G loss: 1.072423]\n",
            "1453 [D loss: 0.546666, acc.: 69.24%] [G loss: 1.092787]\n",
            "1454 [D loss: 0.542392, acc.: 69.63%] [G loss: 1.067299]\n",
            "1455 [D loss: 0.540852, acc.: 67.97%] [G loss: 1.059639]\n",
            "1456 [D loss: 0.532277, acc.: 70.61%] [G loss: 1.062677]\n",
            "1457 [D loss: 0.533519, acc.: 69.53%] [G loss: 1.078923]\n",
            "1458 [D loss: 0.531999, acc.: 71.58%] [G loss: 1.034268]\n",
            "1459 [D loss: 0.531031, acc.: 71.09%] [G loss: 1.056079]\n",
            "1460 [D loss: 0.539016, acc.: 71.00%] [G loss: 1.026484]\n",
            "1461 [D loss: 0.549547, acc.: 69.04%] [G loss: 1.016923]\n",
            "1462 [D loss: 0.554217, acc.: 69.24%] [G loss: 1.032146]\n",
            "1463 [D loss: 0.543159, acc.: 70.31%] [G loss: 1.039920]\n",
            "1464 [D loss: 0.535721, acc.: 70.41%] [G loss: 1.058591]\n",
            "1465 [D loss: 0.531324, acc.: 70.31%] [G loss: 1.069282]\n",
            "1466 [D loss: 0.519680, acc.: 71.97%] [G loss: 1.071981]\n",
            "1467 [D loss: 0.518584, acc.: 72.07%] [G loss: 1.078857]\n",
            "1468 [D loss: 0.515057, acc.: 73.83%] [G loss: 1.084395]\n",
            "1469 [D loss: 0.519046, acc.: 74.22%] [G loss: 1.074672]\n",
            "1470 [D loss: 0.517538, acc.: 74.61%] [G loss: 1.076092]\n",
            "1471 [D loss: 0.514855, acc.: 75.20%] [G loss: 1.079195]\n",
            "1472 [D loss: 0.518423, acc.: 74.90%] [G loss: 1.058980]\n",
            "1473 [D loss: 0.543239, acc.: 69.04%] [G loss: 1.007849]\n",
            "1474 [D loss: 0.557059, acc.: 67.68%] [G loss: 1.009415]\n",
            "1475 [D loss: 0.547047, acc.: 68.95%] [G loss: 0.996920]\n",
            "1476 [D loss: 0.548712, acc.: 69.14%] [G loss: 0.999910]\n",
            "1477 [D loss: 0.539327, acc.: 70.31%] [G loss: 1.012888]\n",
            "1478 [D loss: 0.536705, acc.: 69.82%] [G loss: 1.031253]\n",
            "1479 [D loss: 0.537954, acc.: 69.73%] [G loss: 1.037844]\n",
            "1480 [D loss: 0.522462, acc.: 71.68%] [G loss: 1.025880]\n",
            "1481 [D loss: 0.530492, acc.: 71.48%] [G loss: 0.999603]\n",
            "1482 [D loss: 0.540821, acc.: 69.24%] [G loss: 1.026425]\n",
            "1483 [D loss: 0.544674, acc.: 68.46%] [G loss: 1.007555]\n",
            "1484 [D loss: 0.547727, acc.: 69.04%] [G loss: 0.987064]\n",
            "1485 [D loss: 0.545949, acc.: 69.04%] [G loss: 1.012594]\n",
            "1486 [D loss: 0.565293, acc.: 67.38%] [G loss: 1.023279]\n",
            "1487 [D loss: 0.554281, acc.: 68.65%] [G loss: 1.047728]\n",
            "1488 [D loss: 0.552016, acc.: 69.34%] [G loss: 1.015105]\n",
            "1489 [D loss: 0.555860, acc.: 68.65%] [G loss: 1.036126]\n",
            "1490 [D loss: 0.550318, acc.: 69.63%] [G loss: 1.036752]\n",
            "1491 [D loss: 0.550200, acc.: 68.85%] [G loss: 1.018488]\n",
            "1492 [D loss: 0.543143, acc.: 70.61%] [G loss: 1.040500]\n",
            "1493 [D loss: 0.543482, acc.: 70.31%] [G loss: 1.021275]\n",
            "1494 [D loss: 0.544141, acc.: 69.34%] [G loss: 1.046824]\n",
            "1495 [D loss: 0.541379, acc.: 69.53%] [G loss: 1.026036]\n",
            "1496 [D loss: 0.545003, acc.: 69.82%] [G loss: 1.031067]\n",
            "1497 [D loss: 0.547399, acc.: 68.95%] [G loss: 1.027156]\n",
            "1498 [D loss: 0.545625, acc.: 69.63%] [G loss: 1.023235]\n",
            "1499 [D loss: 0.534120, acc.: 70.02%] [G loss: 1.037130]\n",
            "1500 [D loss: 0.543846, acc.: 69.63%] [G loss: 1.040920]\n",
            "1501 [D loss: 0.539487, acc.: 70.31%] [G loss: 1.040808]\n",
            "1502 [D loss: 0.543453, acc.: 69.73%] [G loss: 1.031407]\n",
            "1503 [D loss: 0.540540, acc.: 69.63%] [G loss: 1.023008]\n",
            "1504 [D loss: 0.535284, acc.: 69.34%] [G loss: 1.022240]\n",
            "1505 [D loss: 0.534617, acc.: 69.82%] [G loss: 1.029445]\n",
            "1506 [D loss: 0.535014, acc.: 71.09%] [G loss: 1.045950]\n",
            "1507 [D loss: 0.530073, acc.: 71.78%] [G loss: 1.024589]\n",
            "1508 [D loss: 0.528337, acc.: 70.51%] [G loss: 1.017274]\n",
            "1509 [D loss: 0.535344, acc.: 71.68%] [G loss: 1.037023]\n",
            "1510 [D loss: 0.524239, acc.: 72.07%] [G loss: 1.031811]\n",
            "1511 [D loss: 0.523177, acc.: 71.88%] [G loss: 1.048593]\n",
            "1512 [D loss: 0.525674, acc.: 72.85%] [G loss: 1.030137]\n",
            "1513 [D loss: 0.522533, acc.: 72.46%] [G loss: 1.046181]\n",
            "1514 [D loss: 0.522996, acc.: 72.46%] [G loss: 1.030120]\n",
            "1515 [D loss: 0.525989, acc.: 71.48%] [G loss: 1.003848]\n",
            "1516 [D loss: 0.526190, acc.: 70.41%] [G loss: 1.024009]\n",
            "1517 [D loss: 0.530551, acc.: 70.51%] [G loss: 1.043062]\n",
            "1518 [D loss: 0.527522, acc.: 70.12%] [G loss: 1.035149]\n",
            "1519 [D loss: 0.531972, acc.: 70.31%] [G loss: 1.035599]\n",
            "1520 [D loss: 0.520894, acc.: 72.36%] [G loss: 1.063428]\n",
            "1521 [D loss: 0.529201, acc.: 71.00%] [G loss: 1.065645]\n",
            "1522 [D loss: 0.526894, acc.: 71.09%] [G loss: 1.049348]\n",
            "1523 [D loss: 0.537263, acc.: 69.92%] [G loss: 1.054110]\n",
            "1524 [D loss: 0.548578, acc.: 69.73%] [G loss: 1.024405]\n",
            "1525 [D loss: 0.559961, acc.: 66.02%] [G loss: 1.018245]\n",
            "1526 [D loss: 0.564288, acc.: 65.33%] [G loss: 1.046047]\n",
            "1527 [D loss: 0.545973, acc.: 67.68%] [G loss: 1.077693]\n",
            "1528 [D loss: 0.540042, acc.: 68.36%] [G loss: 1.082195]\n",
            "1529 [D loss: 0.532138, acc.: 69.43%] [G loss: 1.091835]\n",
            "1530 [D loss: 0.524231, acc.: 71.09%] [G loss: 1.083396]\n",
            "1531 [D loss: 0.528831, acc.: 70.90%] [G loss: 1.069506]\n",
            "1532 [D loss: 0.528760, acc.: 71.78%] [G loss: 1.030393]\n",
            "1533 [D loss: 0.534192, acc.: 71.39%] [G loss: 1.016080]\n",
            "1534 [D loss: 0.533739, acc.: 71.58%] [G loss: 1.016261]\n",
            "1535 [D loss: 0.521264, acc.: 72.17%] [G loss: 1.037638]\n",
            "1536 [D loss: 0.533536, acc.: 70.80%] [G loss: 1.039798]\n",
            "1537 [D loss: 0.529019, acc.: 70.70%] [G loss: 1.080275]\n",
            "1538 [D loss: 0.528360, acc.: 70.21%] [G loss: 1.056237]\n",
            "1539 [D loss: 0.534392, acc.: 70.70%] [G loss: 1.042430]\n",
            "1540 [D loss: 0.528178, acc.: 71.39%] [G loss: 1.090557]\n",
            "1541 [D loss: 0.528622, acc.: 70.41%] [G loss: 1.092532]\n",
            "1542 [D loss: 0.525589, acc.: 71.88%] [G loss: 1.052903]\n",
            "1543 [D loss: 0.521622, acc.: 73.14%] [G loss: 1.052957]\n",
            "1544 [D loss: 0.533214, acc.: 71.19%] [G loss: 1.026875]\n",
            "1545 [D loss: 0.534201, acc.: 71.09%] [G loss: 1.029774]\n",
            "1546 [D loss: 0.533378, acc.: 71.09%] [G loss: 1.033903]\n",
            "1547 [D loss: 0.527975, acc.: 71.88%] [G loss: 1.019159]\n",
            "1548 [D loss: 0.533272, acc.: 71.09%] [G loss: 1.037867]\n",
            "1549 [D loss: 0.527301, acc.: 71.39%] [G loss: 1.044114]\n",
            "1550 [D loss: 0.528569, acc.: 73.14%] [G loss: 1.058712]\n",
            "1551 [D loss: 0.528622, acc.: 72.75%] [G loss: 1.057568]\n",
            "1552 [D loss: 0.529300, acc.: 71.68%] [G loss: 1.053794]\n",
            "1553 [D loss: 0.524936, acc.: 73.63%] [G loss: 1.077489]\n",
            "1554 [D loss: 0.531446, acc.: 72.66%] [G loss: 1.045526]\n",
            "1555 [D loss: 0.535808, acc.: 71.78%] [G loss: 1.034297]\n",
            "1556 [D loss: 0.537919, acc.: 70.21%] [G loss: 1.088384]\n",
            "1557 [D loss: 0.529178, acc.: 72.17%] [G loss: 1.084801]\n",
            "1558 [D loss: 0.538486, acc.: 70.02%] [G loss: 1.066936]\n",
            "1559 [D loss: 0.519620, acc.: 71.48%] [G loss: 1.115696]\n",
            "1560 [D loss: 0.532978, acc.: 71.00%] [G loss: 1.091857]\n",
            "1561 [D loss: 0.508346, acc.: 72.85%] [G loss: 1.158116]\n",
            "1562 [D loss: 0.524400, acc.: 71.19%] [G loss: 1.093131]\n",
            "1563 [D loss: 0.528855, acc.: 71.39%] [G loss: 1.067291]\n",
            "1564 [D loss: 0.534676, acc.: 69.92%] [G loss: 1.060374]\n",
            "1565 [D loss: 0.542243, acc.: 70.31%] [G loss: 1.042094]\n",
            "1566 [D loss: 0.552315, acc.: 68.07%] [G loss: 1.030122]\n",
            "1567 [D loss: 0.543030, acc.: 68.75%] [G loss: 1.054627]\n",
            "1568 [D loss: 0.534959, acc.: 71.48%] [G loss: 1.047593]\n",
            "1569 [D loss: 0.535847, acc.: 71.19%] [G loss: 1.046744]\n",
            "1570 [D loss: 0.526747, acc.: 72.95%] [G loss: 1.052079]\n",
            "1571 [D loss: 0.516982, acc.: 74.12%] [G loss: 1.071852]\n",
            "1572 [D loss: 0.512196, acc.: 76.07%] [G loss: 1.053403]\n",
            "1573 [D loss: 0.506562, acc.: 75.68%] [G loss: 1.058242]\n",
            "1574 [D loss: 0.504664, acc.: 75.98%] [G loss: 1.048944]\n",
            "1575 [D loss: 0.508181, acc.: 74.71%] [G loss: 1.078383]\n",
            "1576 [D loss: 0.508573, acc.: 75.10%] [G loss: 1.070207]\n",
            "1577 [D loss: 0.506068, acc.: 75.88%] [G loss: 1.073045]\n",
            "1578 [D loss: 0.521921, acc.: 72.95%] [G loss: 1.106207]\n",
            "1579 [D loss: 0.531755, acc.: 71.00%] [G loss: 1.104189]\n",
            "1580 [D loss: 0.545090, acc.: 70.02%] [G loss: 1.078094]\n",
            "1581 [D loss: 0.576651, acc.: 68.36%] [G loss: 1.094515]\n",
            "1582 [D loss: 0.587427, acc.: 68.65%] [G loss: 1.113452]\n",
            "1583 [D loss: 0.572632, acc.: 68.16%] [G loss: 1.135839]\n",
            "1584 [D loss: 0.549335, acc.: 69.53%] [G loss: 1.139992]\n",
            "1585 [D loss: 0.550317, acc.: 70.21%] [G loss: 1.179271]\n",
            "1586 [D loss: 0.529258, acc.: 70.31%] [G loss: 1.215572]\n",
            "1587 [D loss: 0.512364, acc.: 73.14%] [G loss: 1.237000]\n",
            "1588 [D loss: 0.495006, acc.: 73.14%] [G loss: 1.260880]\n",
            "1589 [D loss: 0.496292, acc.: 73.44%] [G loss: 1.265903]\n",
            "1590 [D loss: 0.491094, acc.: 74.12%] [G loss: 1.201203]\n",
            "1591 [D loss: 0.503723, acc.: 73.83%] [G loss: 1.153507]\n",
            "1592 [D loss: 0.494201, acc.: 75.20%] [G loss: 1.162760]\n",
            "1593 [D loss: 0.508483, acc.: 74.12%] [G loss: 1.162284]\n",
            "1594 [D loss: 0.494940, acc.: 74.71%] [G loss: 1.158345]\n",
            "1595 [D loss: 0.519299, acc.: 73.44%] [G loss: 1.100748]\n",
            "1596 [D loss: 0.539495, acc.: 71.58%] [G loss: 1.099319]\n",
            "1597 [D loss: 0.552562, acc.: 70.51%] [G loss: 1.048551]\n",
            "1598 [D loss: 0.549058, acc.: 71.19%] [G loss: 1.052888]\n",
            "1599 [D loss: 0.548584, acc.: 71.88%] [G loss: 1.076224]\n",
            "1600 [D loss: 0.537486, acc.: 73.05%] [G loss: 1.072672]\n",
            "1601 [D loss: 0.538327, acc.: 72.66%] [G loss: 1.089629]\n",
            "1602 [D loss: 0.523100, acc.: 73.63%] [G loss: 1.042819]\n",
            "1603 [D loss: 0.547171, acc.: 70.90%] [G loss: 1.020236]\n",
            "1604 [D loss: 0.543144, acc.: 71.00%] [G loss: 1.038682]\n",
            "1605 [D loss: 0.546694, acc.: 69.82%] [G loss: 1.077232]\n",
            "1606 [D loss: 0.532625, acc.: 71.00%] [G loss: 1.069516]\n",
            "1607 [D loss: 0.529324, acc.: 71.39%] [G loss: 1.087327]\n",
            "1608 [D loss: 0.524328, acc.: 71.68%] [G loss: 1.052417]\n",
            "1609 [D loss: 0.524621, acc.: 71.68%] [G loss: 1.081957]\n",
            "1610 [D loss: 0.524723, acc.: 70.41%] [G loss: 1.094368]\n",
            "1611 [D loss: 0.525098, acc.: 71.39%] [G loss: 1.089025]\n",
            "1612 [D loss: 0.534455, acc.: 70.51%] [G loss: 1.084719]\n",
            "1613 [D loss: 0.534899, acc.: 71.00%] [G loss: 1.050751]\n",
            "1614 [D loss: 0.517026, acc.: 71.78%] [G loss: 1.079304]\n",
            "1615 [D loss: 0.527048, acc.: 71.78%] [G loss: 1.068051]\n",
            "1616 [D loss: 0.529965, acc.: 71.29%] [G loss: 1.066275]\n",
            "1617 [D loss: 0.520396, acc.: 71.78%] [G loss: 1.056630]\n",
            "1618 [D loss: 0.526179, acc.: 70.90%] [G loss: 1.041992]\n",
            "1619 [D loss: 0.521330, acc.: 72.95%] [G loss: 1.032007]\n",
            "1620 [D loss: 0.515579, acc.: 73.83%] [G loss: 1.044003]\n",
            "1621 [D loss: 0.514209, acc.: 74.41%] [G loss: 1.038505]\n",
            "1622 [D loss: 0.523673, acc.: 73.63%] [G loss: 1.067248]\n",
            "1623 [D loss: 0.525368, acc.: 73.73%] [G loss: 1.053356]\n",
            "1624 [D loss: 0.529460, acc.: 73.83%] [G loss: 1.080803]\n",
            "1625 [D loss: 0.520266, acc.: 74.41%] [G loss: 1.049302]\n",
            "1626 [D loss: 0.526340, acc.: 73.24%] [G loss: 1.084267]\n",
            "1627 [D loss: 0.533071, acc.: 71.97%] [G loss: 1.073810]\n",
            "1628 [D loss: 0.523306, acc.: 73.14%] [G loss: 1.073486]\n",
            "1629 [D loss: 0.532768, acc.: 72.17%] [G loss: 1.068341]\n",
            "1630 [D loss: 0.536426, acc.: 72.17%] [G loss: 1.052197]\n",
            "1631 [D loss: 0.531642, acc.: 72.17%] [G loss: 1.088882]\n",
            "1632 [D loss: 0.530280, acc.: 72.17%] [G loss: 1.051516]\n",
            "1633 [D loss: 0.541127, acc.: 71.19%] [G loss: 1.049923]\n",
            "1634 [D loss: 0.524084, acc.: 71.00%] [G loss: 1.132610]\n",
            "1635 [D loss: 0.536254, acc.: 68.75%] [G loss: 1.116663]\n",
            "1636 [D loss: 0.524115, acc.: 72.46%] [G loss: 1.073240]\n",
            "1637 [D loss: 0.525143, acc.: 71.29%] [G loss: 1.125796]\n",
            "1638 [D loss: 0.537723, acc.: 70.80%] [G loss: 1.060541]\n",
            "1639 [D loss: 0.543919, acc.: 69.82%] [G loss: 1.025226]\n",
            "1640 [D loss: 0.531078, acc.: 71.48%] [G loss: 1.062886]\n",
            "1641 [D loss: 0.535426, acc.: 70.61%] [G loss: 1.043783]\n",
            "1642 [D loss: 0.529293, acc.: 71.29%] [G loss: 1.048649]\n",
            "1643 [D loss: 0.532258, acc.: 71.58%] [G loss: 1.032222]\n",
            "1644 [D loss: 0.529782, acc.: 72.17%] [G loss: 1.036012]\n",
            "1645 [D loss: 0.528215, acc.: 72.66%] [G loss: 1.049234]\n",
            "1646 [D loss: 0.531162, acc.: 72.27%] [G loss: 1.043466]\n",
            "1647 [D loss: 0.529897, acc.: 73.14%] [G loss: 1.025614]\n",
            "1648 [D loss: 0.524305, acc.: 73.44%] [G loss: 1.046771]\n",
            "1649 [D loss: 0.536040, acc.: 71.97%] [G loss: 1.052625]\n",
            "1650 [D loss: 0.530966, acc.: 71.68%] [G loss: 1.091769]\n",
            "1651 [D loss: 0.536994, acc.: 71.97%] [G loss: 1.077758]\n",
            "1652 [D loss: 0.524531, acc.: 73.05%] [G loss: 1.088090]\n",
            "1653 [D loss: 0.525561, acc.: 73.83%] [G loss: 1.057927]\n",
            "1654 [D loss: 0.534483, acc.: 72.27%] [G loss: 1.053914]\n",
            "1655 [D loss: 0.530828, acc.: 71.19%] [G loss: 1.059233]\n",
            "1656 [D loss: 0.536854, acc.: 70.90%] [G loss: 1.069213]\n",
            "1657 [D loss: 0.532904, acc.: 71.00%] [G loss: 1.073164]\n",
            "1658 [D loss: 0.526423, acc.: 72.07%] [G loss: 1.051638]\n",
            "1659 [D loss: 0.523505, acc.: 72.95%] [G loss: 1.033514]\n",
            "1660 [D loss: 0.526419, acc.: 72.07%] [G loss: 1.062307]\n",
            "1661 [D loss: 0.518854, acc.: 73.24%] [G loss: 1.053961]\n",
            "1662 [D loss: 0.525070, acc.: 72.46%] [G loss: 1.031732]\n",
            "1663 [D loss: 0.525326, acc.: 72.17%] [G loss: 1.022935]\n",
            "1664 [D loss: 0.533309, acc.: 71.29%] [G loss: 1.067123]\n",
            "1665 [D loss: 0.522715, acc.: 72.85%] [G loss: 1.059336]\n",
            "1666 [D loss: 0.534861, acc.: 70.61%] [G loss: 1.058463]\n",
            "1667 [D loss: 0.522838, acc.: 72.85%] [G loss: 1.046713]\n",
            "1668 [D loss: 0.530057, acc.: 71.97%] [G loss: 1.085116]\n",
            "1669 [D loss: 0.524250, acc.: 71.88%] [G loss: 1.062631]\n",
            "1670 [D loss: 0.524552, acc.: 71.97%] [G loss: 1.089568]\n",
            "1671 [D loss: 0.518134, acc.: 73.54%] [G loss: 1.091961]\n",
            "1672 [D loss: 0.504543, acc.: 74.80%] [G loss: 1.109368]\n",
            "1673 [D loss: 0.498958, acc.: 74.32%] [G loss: 1.089343]\n",
            "1674 [D loss: 0.495772, acc.: 75.49%] [G loss: 1.145085]\n",
            "1675 [D loss: 0.488180, acc.: 76.95%] [G loss: 1.124711]\n",
            "1676 [D loss: 0.479414, acc.: 77.34%] [G loss: 1.107440]\n",
            "1677 [D loss: 0.492502, acc.: 77.25%] [G loss: 1.123378]\n",
            "1678 [D loss: 0.497429, acc.: 74.61%] [G loss: 1.125930]\n",
            "1679 [D loss: 0.506085, acc.: 74.02%] [G loss: 1.119106]\n",
            "1680 [D loss: 0.522602, acc.: 72.56%] [G loss: 1.094016]\n",
            "1681 [D loss: 0.528780, acc.: 73.05%] [G loss: 1.082629]\n",
            "1682 [D loss: 0.556772, acc.: 69.92%] [G loss: 1.047001]\n",
            "1683 [D loss: 0.567759, acc.: 68.95%] [G loss: 1.051189]\n",
            "1684 [D loss: 0.576154, acc.: 67.19%] [G loss: 1.064530]\n",
            "1685 [D loss: 0.551119, acc.: 69.63%] [G loss: 1.089624]\n",
            "1686 [D loss: 0.536103, acc.: 71.09%] [G loss: 1.100026]\n",
            "1687 [D loss: 0.528901, acc.: 73.14%] [G loss: 1.120091]\n",
            "1688 [D loss: 0.515009, acc.: 74.61%] [G loss: 1.165272]\n",
            "1689 [D loss: 0.519895, acc.: 73.83%] [G loss: 1.160944]\n",
            "1690 [D loss: 0.514408, acc.: 72.95%] [G loss: 1.124733]\n",
            "1691 [D loss: 0.513847, acc.: 75.20%] [G loss: 1.109165]\n",
            "1692 [D loss: 0.520818, acc.: 73.63%] [G loss: 1.070111]\n",
            "1693 [D loss: 0.526134, acc.: 73.44%] [G loss: 1.043261]\n",
            "1694 [D loss: 0.544156, acc.: 71.00%] [G loss: 1.050240]\n",
            "1695 [D loss: 0.547828, acc.: 70.51%] [G loss: 1.057944]\n",
            "1696 [D loss: 0.540878, acc.: 71.19%] [G loss: 1.027601]\n",
            "1697 [D loss: 0.539467, acc.: 71.97%] [G loss: 1.042942]\n",
            "1698 [D loss: 0.534223, acc.: 72.27%] [G loss: 1.065037]\n",
            "1699 [D loss: 0.525408, acc.: 73.44%] [G loss: 1.035270]\n",
            "1700 [D loss: 0.521156, acc.: 73.44%] [G loss: 1.041065]\n",
            "1701 [D loss: 0.527760, acc.: 71.29%] [G loss: 1.049842]\n",
            "1702 [D loss: 0.520710, acc.: 73.44%] [G loss: 1.046502]\n",
            "1703 [D loss: 0.512641, acc.: 74.61%] [G loss: 1.047808]\n",
            "1704 [D loss: 0.520387, acc.: 73.34%] [G loss: 1.039553]\n",
            "1705 [D loss: 0.517588, acc.: 73.83%] [G loss: 1.037863]\n",
            "1706 [D loss: 0.516084, acc.: 73.63%] [G loss: 1.084334]\n",
            "1707 [D loss: 0.514947, acc.: 73.93%] [G loss: 1.068396]\n",
            "1708 [D loss: 0.526315, acc.: 73.63%] [G loss: 1.057024]\n",
            "1709 [D loss: 0.517676, acc.: 73.83%] [G loss: 1.034299]\n",
            "1710 [D loss: 0.521622, acc.: 73.83%] [G loss: 1.076275]\n",
            "1711 [D loss: 0.527421, acc.: 73.44%] [G loss: 1.052512]\n",
            "1712 [D loss: 0.533451, acc.: 71.97%] [G loss: 1.072527]\n",
            "1713 [D loss: 0.518339, acc.: 73.05%] [G loss: 1.079833]\n",
            "1714 [D loss: 0.526786, acc.: 73.54%] [G loss: 1.072412]\n",
            "1715 [D loss: 0.522716, acc.: 74.02%] [G loss: 1.107940]\n",
            "1716 [D loss: 0.517792, acc.: 74.12%] [G loss: 1.100518]\n",
            "1717 [D loss: 0.517146, acc.: 74.41%] [G loss: 1.085217]\n",
            "1718 [D loss: 0.529259, acc.: 73.14%] [G loss: 1.041106]\n",
            "1719 [D loss: 0.514275, acc.: 74.12%] [G loss: 1.064758]\n",
            "1720 [D loss: 0.526567, acc.: 72.46%] [G loss: 1.042053]\n",
            "1721 [D loss: 0.532309, acc.: 72.66%] [G loss: 1.035413]\n",
            "1722 [D loss: 0.522750, acc.: 72.75%] [G loss: 1.118654]\n",
            "1723 [D loss: 0.527707, acc.: 72.27%] [G loss: 1.110199]\n",
            "1724 [D loss: 0.528460, acc.: 72.07%] [G loss: 1.078066]\n",
            "1725 [D loss: 0.529568, acc.: 71.97%] [G loss: 1.118182]\n",
            "1726 [D loss: 0.522553, acc.: 72.85%] [G loss: 1.092810]\n",
            "1727 [D loss: 0.554839, acc.: 70.70%] [G loss: 1.081333]\n",
            "1728 [D loss: 0.563086, acc.: 71.29%] [G loss: 1.131280]\n",
            "1729 [D loss: 0.577108, acc.: 70.90%] [G loss: 1.157048]\n",
            "1730 [D loss: 0.564312, acc.: 70.90%] [G loss: 1.182424]\n",
            "1731 [D loss: 0.543561, acc.: 71.78%] [G loss: 1.261253]\n",
            "1732 [D loss: 0.525935, acc.: 73.73%] [G loss: 1.251067]\n",
            "1733 [D loss: 0.518011, acc.: 73.73%] [G loss: 1.273410]\n",
            "1734 [D loss: 0.502337, acc.: 74.90%] [G loss: 1.299059]\n",
            "1735 [D loss: 0.487921, acc.: 76.07%] [G loss: 1.279364]\n",
            "1736 [D loss: 0.486127, acc.: 75.88%] [G loss: 1.239170]\n",
            "1737 [D loss: 0.476369, acc.: 76.66%] [G loss: 1.225134]\n",
            "1738 [D loss: 0.471803, acc.: 77.34%] [G loss: 1.200172]\n",
            "1739 [D loss: 0.479220, acc.: 76.86%] [G loss: 1.193430]\n",
            "1740 [D loss: 0.475115, acc.: 77.15%] [G loss: 1.156442]\n",
            "1741 [D loss: 0.473289, acc.: 77.44%] [G loss: 1.221752]\n",
            "1742 [D loss: 0.475053, acc.: 77.34%] [G loss: 1.175746]\n",
            "1743 [D loss: 0.476286, acc.: 77.05%] [G loss: 1.181945]\n",
            "1744 [D loss: 0.478752, acc.: 76.86%] [G loss: 1.178442]\n",
            "1745 [D loss: 0.488670, acc.: 75.49%] [G loss: 1.197178]\n",
            "1746 [D loss: 0.484655, acc.: 76.37%] [G loss: 1.181433]\n",
            "1747 [D loss: 0.497857, acc.: 75.98%] [G loss: 1.113093]\n",
            "1748 [D loss: 0.517862, acc.: 73.93%] [G loss: 1.084346]\n",
            "1749 [D loss: 0.530519, acc.: 72.75%] [G loss: 1.048893]\n",
            "1750 [D loss: 0.547232, acc.: 70.80%] [G loss: 1.091914]\n",
            "1751 [D loss: 0.541277, acc.: 71.68%] [G loss: 1.055922]\n",
            "1752 [D loss: 0.549207, acc.: 68.95%] [G loss: 1.063076]\n",
            "1753 [D loss: 0.531227, acc.: 70.80%] [G loss: 1.091740]\n",
            "1754 [D loss: 0.528650, acc.: 71.68%] [G loss: 1.084135]\n",
            "1755 [D loss: 0.517046, acc.: 73.73%] [G loss: 1.074648]\n",
            "1756 [D loss: 0.519059, acc.: 73.83%] [G loss: 1.090652]\n",
            "1757 [D loss: 0.519834, acc.: 72.85%] [G loss: 1.075730]\n",
            "1758 [D loss: 0.522516, acc.: 74.32%] [G loss: 1.092782]\n",
            "1759 [D loss: 0.517535, acc.: 74.41%] [G loss: 1.129631]\n",
            "1760 [D loss: 0.528153, acc.: 73.24%] [G loss: 1.105272]\n",
            "1761 [D loss: 0.526689, acc.: 73.44%] [G loss: 1.101109]\n",
            "1762 [D loss: 0.519567, acc.: 73.24%] [G loss: 1.123412]\n",
            "1763 [D loss: 0.523393, acc.: 73.24%] [G loss: 1.104278]\n",
            "1764 [D loss: 0.524291, acc.: 73.14%] [G loss: 1.103256]\n",
            "1765 [D loss: 0.523503, acc.: 72.56%] [G loss: 1.164081]\n",
            "1766 [D loss: 0.530707, acc.: 72.56%] [G loss: 1.078722]\n",
            "1767 [D loss: 0.529548, acc.: 72.36%] [G loss: 1.083014]\n",
            "1768 [D loss: 0.525107, acc.: 72.07%] [G loss: 1.100189]\n",
            "1769 [D loss: 0.525578, acc.: 73.14%] [G loss: 1.086835]\n",
            "1770 [D loss: 0.522383, acc.: 72.75%] [G loss: 1.081637]\n",
            "1771 [D loss: 0.521976, acc.: 72.95%] [G loss: 1.084255]\n",
            "1772 [D loss: 0.519571, acc.: 73.24%] [G loss: 1.093611]\n",
            "1773 [D loss: 0.523265, acc.: 74.02%] [G loss: 1.071824]\n",
            "1774 [D loss: 0.529253, acc.: 72.27%] [G loss: 1.087239]\n",
            "1775 [D loss: 0.519276, acc.: 74.12%] [G loss: 1.095898]\n",
            "1776 [D loss: 0.527240, acc.: 72.46%] [G loss: 1.099042]\n",
            "1777 [D loss: 0.525051, acc.: 73.05%] [G loss: 1.094839]\n",
            "1778 [D loss: 0.513758, acc.: 73.73%] [G loss: 1.111580]\n",
            "1779 [D loss: 0.519630, acc.: 73.05%] [G loss: 1.078774]\n",
            "1780 [D loss: 0.522304, acc.: 73.34%] [G loss: 1.104200]\n",
            "1781 [D loss: 0.515834, acc.: 75.10%] [G loss: 1.097735]\n",
            "1782 [D loss: 0.523534, acc.: 72.17%] [G loss: 1.082574]\n",
            "1783 [D loss: 0.523284, acc.: 73.05%] [G loss: 1.083239]\n",
            "1784 [D loss: 0.522635, acc.: 72.75%] [G loss: 1.100512]\n",
            "1785 [D loss: 0.511269, acc.: 74.61%] [G loss: 1.044976]\n",
            "1786 [D loss: 0.530045, acc.: 72.46%] [G loss: 1.123710]\n",
            "1787 [D loss: 0.525717, acc.: 73.63%] [G loss: 1.115408]\n",
            "1788 [D loss: 0.528418, acc.: 71.78%] [G loss: 1.069264]\n",
            "1789 [D loss: 0.524863, acc.: 72.66%] [G loss: 1.116055]\n",
            "1790 [D loss: 0.535495, acc.: 71.19%] [G loss: 1.095424]\n",
            "1791 [D loss: 0.525738, acc.: 72.85%] [G loss: 1.071836]\n",
            "1792 [D loss: 0.533494, acc.: 72.36%] [G loss: 1.086911]\n",
            "1793 [D loss: 0.521735, acc.: 74.80%] [G loss: 1.076078]\n",
            "1794 [D loss: 0.523940, acc.: 73.44%] [G loss: 1.077089]\n",
            "1795 [D loss: 0.528572, acc.: 72.46%] [G loss: 1.082861]\n",
            "1796 [D loss: 0.520497, acc.: 74.12%] [G loss: 1.076464]\n",
            "1797 [D loss: 0.521400, acc.: 74.51%] [G loss: 1.066816]\n",
            "1798 [D loss: 0.523800, acc.: 73.34%] [G loss: 1.079563]\n",
            "1799 [D loss: 0.517410, acc.: 73.14%] [G loss: 1.079146]\n",
            "1800 [D loss: 0.521729, acc.: 73.54%] [G loss: 1.064849]\n",
            "1801 [D loss: 0.524047, acc.: 73.14%] [G loss: 1.087025]\n",
            "1802 [D loss: 0.512670, acc.: 73.54%] [G loss: 1.110170]\n",
            "1803 [D loss: 0.516991, acc.: 73.34%] [G loss: 1.078253]\n",
            "1804 [D loss: 0.507241, acc.: 73.83%] [G loss: 1.159190]\n",
            "1805 [D loss: 0.520550, acc.: 73.93%] [G loss: 1.092258]\n",
            "1806 [D loss: 0.510232, acc.: 75.00%] [G loss: 1.133111]\n",
            "1807 [D loss: 0.506456, acc.: 73.93%] [G loss: 1.151010]\n",
            "1808 [D loss: 0.507180, acc.: 73.54%] [G loss: 1.145684]\n",
            "1809 [D loss: 0.501043, acc.: 75.00%] [G loss: 1.144944]\n",
            "1810 [D loss: 0.491787, acc.: 75.29%] [G loss: 1.144724]\n",
            "1811 [D loss: 0.515133, acc.: 73.63%] [G loss: 1.123522]\n",
            "1812 [D loss: 0.500679, acc.: 74.71%] [G loss: 1.145587]\n",
            "1813 [D loss: 0.510509, acc.: 74.80%] [G loss: 1.134666]\n",
            "1814 [D loss: 0.526179, acc.: 74.41%] [G loss: 1.093942]\n",
            "1815 [D loss: 0.521010, acc.: 74.71%] [G loss: 1.092961]\n",
            "1816 [D loss: 0.539268, acc.: 71.48%] [G loss: 1.092324]\n",
            "1817 [D loss: 0.548661, acc.: 70.61%] [G loss: 1.082802]\n",
            "1818 [D loss: 0.545640, acc.: 69.53%] [G loss: 1.129187]\n",
            "1819 [D loss: 0.531855, acc.: 72.85%] [G loss: 1.083456]\n",
            "1820 [D loss: 0.535424, acc.: 72.36%] [G loss: 1.063021]\n",
            "1821 [D loss: 0.512797, acc.: 72.56%] [G loss: 1.143030]\n",
            "1822 [D loss: 0.518125, acc.: 73.73%] [G loss: 1.117034]\n",
            "1823 [D loss: 0.513763, acc.: 74.51%] [G loss: 1.105716]\n",
            "1824 [D loss: 0.514547, acc.: 74.80%] [G loss: 1.114295]\n",
            "1825 [D loss: 0.517230, acc.: 73.93%] [G loss: 1.079584]\n",
            "1826 [D loss: 0.526666, acc.: 73.24%] [G loss: 1.045526]\n",
            "1827 [D loss: 0.531056, acc.: 72.07%] [G loss: 1.119233]\n",
            "1828 [D loss: 0.514549, acc.: 74.12%] [G loss: 1.115803]\n",
            "1829 [D loss: 0.517839, acc.: 73.73%] [G loss: 1.066188]\n",
            "1830 [D loss: 0.525549, acc.: 70.90%] [G loss: 1.146495]\n",
            "1831 [D loss: 0.515147, acc.: 72.75%] [G loss: 1.135770]\n",
            "1832 [D loss: 0.534215, acc.: 71.48%] [G loss: 1.096735]\n",
            "1833 [D loss: 0.536296, acc.: 71.29%] [G loss: 1.124185]\n",
            "1834 [D loss: 0.530348, acc.: 72.07%] [G loss: 1.089789]\n",
            "1835 [D loss: 0.533640, acc.: 72.07%] [G loss: 1.062385]\n",
            "1836 [D loss: 0.542119, acc.: 71.00%] [G loss: 1.074845]\n",
            "1837 [D loss: 0.527159, acc.: 73.34%] [G loss: 1.060836]\n",
            "1838 [D loss: 0.522661, acc.: 73.63%] [G loss: 1.081154]\n",
            "1839 [D loss: 0.535699, acc.: 72.66%] [G loss: 1.053993]\n",
            "1840 [D loss: 0.532736, acc.: 72.85%] [G loss: 1.062451]\n",
            "1841 [D loss: 0.521312, acc.: 73.54%] [G loss: 1.082528]\n",
            "1842 [D loss: 0.524062, acc.: 73.24%] [G loss: 1.080066]\n",
            "1843 [D loss: 0.525665, acc.: 73.54%] [G loss: 1.074836]\n",
            "1844 [D loss: 0.527774, acc.: 72.75%] [G loss: 1.084186]\n",
            "1845 [D loss: 0.521147, acc.: 73.14%] [G loss: 1.103347]\n",
            "1846 [D loss: 0.533379, acc.: 72.95%] [G loss: 1.068145]\n",
            "1847 [D loss: 0.537987, acc.: 71.88%] [G loss: 1.093619]\n",
            "1848 [D loss: 0.522442, acc.: 73.54%] [G loss: 1.124929]\n",
            "1849 [D loss: 0.538881, acc.: 70.80%] [G loss: 1.073657]\n",
            "1850 [D loss: 0.522560, acc.: 73.63%] [G loss: 1.101045]\n",
            "1851 [D loss: 0.525517, acc.: 71.68%] [G loss: 1.098015]\n",
            "1852 [D loss: 0.528304, acc.: 73.05%] [G loss: 1.093909]\n",
            "1853 [D loss: 0.528739, acc.: 72.66%] [G loss: 1.109883]\n",
            "1854 [D loss: 0.526631, acc.: 71.78%] [G loss: 1.105160]\n",
            "1855 [D loss: 0.529869, acc.: 72.46%] [G loss: 1.057676]\n",
            "1856 [D loss: 0.515335, acc.: 72.75%] [G loss: 1.118788]\n",
            "1857 [D loss: 0.528805, acc.: 72.36%] [G loss: 1.069893]\n",
            "1858 [D loss: 0.527173, acc.: 72.46%] [G loss: 1.109036]\n",
            "1859 [D loss: 0.524546, acc.: 72.56%] [G loss: 1.042703]\n",
            "1860 [D loss: 0.520289, acc.: 73.44%] [G loss: 1.067884]\n",
            "1861 [D loss: 0.533171, acc.: 72.27%] [G loss: 1.049937]\n",
            "1862 [D loss: 0.527570, acc.: 71.97%] [G loss: 1.088356]\n",
            "1863 [D loss: 0.518076, acc.: 74.51%] [G loss: 1.101516]\n",
            "1864 [D loss: 0.518670, acc.: 74.22%] [G loss: 1.066153]\n",
            "1865 [D loss: 0.523079, acc.: 73.05%] [G loss: 1.077563]\n",
            "1866 [D loss: 0.529586, acc.: 73.05%] [G loss: 1.094512]\n",
            "1867 [D loss: 0.526345, acc.: 72.36%] [G loss: 1.074749]\n",
            "1868 [D loss: 0.530025, acc.: 72.27%] [G loss: 1.103062]\n",
            "1869 [D loss: 0.525943, acc.: 72.46%] [G loss: 1.067572]\n",
            "1870 [D loss: 0.534356, acc.: 71.58%] [G loss: 1.089757]\n",
            "1871 [D loss: 0.531613, acc.: 73.14%] [G loss: 1.066239]\n",
            "1872 [D loss: 0.514009, acc.: 75.49%] [G loss: 1.115491]\n",
            "1873 [D loss: 0.527352, acc.: 72.07%] [G loss: 1.098610]\n",
            "1874 [D loss: 0.512961, acc.: 75.00%] [G loss: 1.143519]\n",
            "1875 [D loss: 0.524316, acc.: 73.05%] [G loss: 1.115485]\n",
            "1876 [D loss: 0.518276, acc.: 73.14%] [G loss: 1.113233]\n",
            "1877 [D loss: 0.520036, acc.: 74.12%] [G loss: 1.081871]\n",
            "1878 [D loss: 0.522316, acc.: 72.46%] [G loss: 1.092485]\n",
            "1879 [D loss: 0.533406, acc.: 72.46%] [G loss: 1.065857]\n",
            "1880 [D loss: 0.524471, acc.: 73.83%] [G loss: 1.147692]\n",
            "1881 [D loss: 0.517283, acc.: 73.05%] [G loss: 1.102888]\n",
            "1882 [D loss: 0.527271, acc.: 71.48%] [G loss: 1.101398]\n",
            "1883 [D loss: 0.522695, acc.: 72.56%] [G loss: 1.146236]\n",
            "1884 [D loss: 0.521894, acc.: 72.85%] [G loss: 1.089178]\n",
            "1885 [D loss: 0.527132, acc.: 72.75%] [G loss: 1.110213]\n",
            "1886 [D loss: 0.510640, acc.: 74.90%] [G loss: 1.081260]\n",
            "1887 [D loss: 0.530249, acc.: 72.36%] [G loss: 1.081178]\n",
            "1888 [D loss: 0.519112, acc.: 73.93%] [G loss: 1.078824]\n",
            "1889 [D loss: 0.520197, acc.: 74.22%] [G loss: 1.075262]\n",
            "1890 [D loss: 0.533098, acc.: 72.27%] [G loss: 1.101056]\n",
            "1891 [D loss: 0.519539, acc.: 72.75%] [G loss: 1.078954]\n",
            "1892 [D loss: 0.511760, acc.: 75.10%] [G loss: 1.077951]\n",
            "1893 [D loss: 0.511503, acc.: 75.59%] [G loss: 1.079571]\n",
            "1894 [D loss: 0.516291, acc.: 73.34%] [G loss: 1.121201]\n",
            "1895 [D loss: 0.523308, acc.: 73.44%] [G loss: 1.104133]\n",
            "1896 [D loss: 0.515568, acc.: 74.90%] [G loss: 1.057307]\n",
            "1897 [D loss: 0.527412, acc.: 72.36%] [G loss: 1.125384]\n",
            "1898 [D loss: 0.520359, acc.: 72.66%] [G loss: 1.117942]\n",
            "1899 [D loss: 0.518764, acc.: 73.44%] [G loss: 1.153648]\n",
            "1900 [D loss: 0.519026, acc.: 72.07%] [G loss: 1.152360]\n",
            "1901 [D loss: 0.523283, acc.: 72.85%] [G loss: 1.136163]\n",
            "1902 [D loss: 0.531013, acc.: 71.48%] [G loss: 1.122903]\n",
            "1903 [D loss: 0.520180, acc.: 71.88%] [G loss: 1.165632]\n",
            "1904 [D loss: 0.524573, acc.: 73.05%] [G loss: 1.145925]\n",
            "1905 [D loss: 0.523692, acc.: 72.56%] [G loss: 1.072169]\n",
            "1906 [D loss: 0.511004, acc.: 73.63%] [G loss: 1.152438]\n",
            "1907 [D loss: 0.531631, acc.: 73.24%] [G loss: 1.038982]\n",
            "1908 [D loss: 0.524363, acc.: 73.05%] [G loss: 1.072738]\n",
            "1909 [D loss: 0.525184, acc.: 72.75%] [G loss: 1.075622]\n",
            "1910 [D loss: 0.513611, acc.: 74.12%] [G loss: 1.083439]\n",
            "1911 [D loss: 0.537321, acc.: 71.39%] [G loss: 1.070609]\n",
            "1912 [D loss: 0.523789, acc.: 73.14%] [G loss: 1.131358]\n",
            "1913 [D loss: 0.526446, acc.: 72.75%] [G loss: 1.093045]\n",
            "1914 [D loss: 0.518552, acc.: 74.51%] [G loss: 1.097968]\n",
            "1915 [D loss: 0.530315, acc.: 72.36%] [G loss: 1.082647]\n",
            "1916 [D loss: 0.517049, acc.: 73.93%] [G loss: 1.112402]\n",
            "1917 [D loss: 0.526647, acc.: 72.85%] [G loss: 1.052229]\n",
            "1918 [D loss: 0.514196, acc.: 75.10%] [G loss: 1.103448]\n",
            "1919 [D loss: 0.525566, acc.: 72.85%] [G loss: 1.067879]\n",
            "1920 [D loss: 0.521802, acc.: 73.93%] [G loss: 1.113294]\n",
            "1921 [D loss: 0.527190, acc.: 72.17%] [G loss: 1.092735]\n",
            "1922 [D loss: 0.528132, acc.: 73.73%] [G loss: 1.100922]\n",
            "1923 [D loss: 0.528907, acc.: 72.66%] [G loss: 1.056239]\n",
            "1924 [D loss: 0.533431, acc.: 71.68%] [G loss: 1.075305]\n",
            "1925 [D loss: 0.521961, acc.: 73.54%] [G loss: 1.068544]\n",
            "1926 [D loss: 0.523016, acc.: 72.95%] [G loss: 1.086263]\n",
            "1927 [D loss: 0.523218, acc.: 71.88%] [G loss: 1.060257]\n",
            "1928 [D loss: 0.528484, acc.: 72.46%] [G loss: 1.091321]\n",
            "1929 [D loss: 0.529220, acc.: 73.44%] [G loss: 1.073060]\n",
            "1930 [D loss: 0.522719, acc.: 73.93%] [G loss: 1.099680]\n",
            "1931 [D loss: 0.527573, acc.: 73.34%] [G loss: 1.110771]\n",
            "1932 [D loss: 0.530758, acc.: 73.54%] [G loss: 1.049414]\n",
            "1933 [D loss: 0.514778, acc.: 73.63%] [G loss: 1.093457]\n",
            "1934 [D loss: 0.526693, acc.: 72.56%] [G loss: 1.064321]\n",
            "1935 [D loss: 0.545104, acc.: 72.36%] [G loss: 1.120929]\n",
            "1936 [D loss: 0.536459, acc.: 72.36%] [G loss: 1.124638]\n",
            "1937 [D loss: 0.540905, acc.: 72.07%] [G loss: 1.101099]\n",
            "1938 [D loss: 0.541968, acc.: 71.39%] [G loss: 1.128724]\n",
            "1939 [D loss: 0.525279, acc.: 73.14%] [G loss: 1.118750]\n",
            "1940 [D loss: 0.532033, acc.: 71.88%] [G loss: 1.130123]\n",
            "1941 [D loss: 0.538573, acc.: 72.07%] [G loss: 1.122506]\n",
            "1942 [D loss: 0.542706, acc.: 70.90%] [G loss: 1.085209]\n",
            "1943 [D loss: 0.516033, acc.: 74.41%] [G loss: 1.143114]\n",
            "1944 [D loss: 0.531057, acc.: 72.27%] [G loss: 1.073753]\n",
            "1945 [D loss: 0.526012, acc.: 73.63%] [G loss: 1.078361]\n",
            "1946 [D loss: 0.523674, acc.: 73.54%] [G loss: 1.124752]\n",
            "1947 [D loss: 0.512479, acc.: 75.10%] [G loss: 1.128656]\n",
            "1948 [D loss: 0.529676, acc.: 73.54%] [G loss: 1.058774]\n",
            "1949 [D loss: 0.525175, acc.: 74.80%] [G loss: 1.117839]\n",
            "1950 [D loss: 0.520286, acc.: 72.36%] [G loss: 1.095444]\n",
            "1951 [D loss: 0.520960, acc.: 73.44%] [G loss: 1.097496]\n",
            "1952 [D loss: 0.517171, acc.: 73.93%] [G loss: 1.115556]\n",
            "1953 [D loss: 0.517192, acc.: 73.73%] [G loss: 1.118840]\n",
            "1954 [D loss: 0.514594, acc.: 74.12%] [G loss: 1.098057]\n",
            "1955 [D loss: 0.510775, acc.: 73.73%] [G loss: 1.110093]\n",
            "1956 [D loss: 0.517546, acc.: 73.54%] [G loss: 1.114542]\n",
            "1957 [D loss: 0.507482, acc.: 74.12%] [G loss: 1.155342]\n",
            "1958 [D loss: 0.522271, acc.: 73.14%] [G loss: 1.092033]\n",
            "1959 [D loss: 0.525433, acc.: 71.58%] [G loss: 1.159928]\n",
            "1960 [D loss: 0.521891, acc.: 72.56%] [G loss: 1.149494]\n",
            "1961 [D loss: 0.528607, acc.: 71.39%] [G loss: 1.166250]\n",
            "1962 [D loss: 0.525918, acc.: 73.54%] [G loss: 1.163038]\n",
            "1963 [D loss: 0.521109, acc.: 74.22%] [G loss: 1.141800]\n",
            "1964 [D loss: 0.514987, acc.: 73.24%] [G loss: 1.099874]\n",
            "1965 [D loss: 0.516234, acc.: 72.56%] [G loss: 1.198627]\n",
            "1966 [D loss: 0.522013, acc.: 73.05%] [G loss: 1.194149]\n",
            "1967 [D loss: 0.530658, acc.: 71.68%] [G loss: 1.088016]\n",
            "1968 [D loss: 0.512340, acc.: 74.80%] [G loss: 1.118616]\n",
            "1969 [D loss: 0.520743, acc.: 72.95%] [G loss: 1.089951]\n",
            "1970 [D loss: 0.527501, acc.: 72.95%] [G loss: 1.101457]\n",
            "1971 [D loss: 0.518656, acc.: 72.66%] [G loss: 1.121504]\n",
            "1972 [D loss: 0.526784, acc.: 73.44%] [G loss: 1.075859]\n",
            "1973 [D loss: 0.510873, acc.: 75.20%] [G loss: 1.070875]\n",
            "1974 [D loss: 0.517514, acc.: 73.63%] [G loss: 1.108622]\n",
            "1975 [D loss: 0.518257, acc.: 73.54%] [G loss: 1.131255]\n",
            "1976 [D loss: 0.516886, acc.: 73.63%] [G loss: 1.139989]\n",
            "1977 [D loss: 0.518093, acc.: 72.46%] [G loss: 1.153587]\n",
            "1978 [D loss: 0.515788, acc.: 73.24%] [G loss: 1.117380]\n",
            "1979 [D loss: 0.513935, acc.: 74.12%] [G loss: 1.122358]\n",
            "1980 [D loss: 0.527019, acc.: 72.17%] [G loss: 1.062891]\n",
            "1981 [D loss: 0.511877, acc.: 73.83%] [G loss: 1.103351]\n",
            "1982 [D loss: 0.517033, acc.: 73.63%] [G loss: 1.130862]\n",
            "1983 [D loss: 0.514823, acc.: 73.24%] [G loss: 1.108299]\n",
            "1984 [D loss: 0.523492, acc.: 72.07%] [G loss: 1.095009]\n",
            "1985 [D loss: 0.516753, acc.: 73.05%] [G loss: 1.058250]\n",
            "1986 [D loss: 0.508292, acc.: 75.20%] [G loss: 1.130364]\n",
            "1987 [D loss: 0.524719, acc.: 72.07%] [G loss: 1.078810]\n",
            "1988 [D loss: 0.515432, acc.: 74.32%] [G loss: 1.081946]\n",
            "1989 [D loss: 0.507111, acc.: 75.00%] [G loss: 1.147029]\n",
            "1990 [D loss: 0.525843, acc.: 72.56%] [G loss: 1.089712]\n",
            "1991 [D loss: 0.522098, acc.: 74.41%] [G loss: 1.131596]\n",
            "1992 [D loss: 0.526262, acc.: 74.22%] [G loss: 1.108346]\n",
            "1993 [D loss: 0.521865, acc.: 74.51%] [G loss: 1.109012]\n",
            "1994 [D loss: 0.517326, acc.: 73.34%] [G loss: 1.132845]\n",
            "1995 [D loss: 0.529200, acc.: 72.36%] [G loss: 1.106437]\n",
            "1996 [D loss: 0.528657, acc.: 73.14%] [G loss: 1.114099]\n",
            "1997 [D loss: 0.528219, acc.: 72.95%] [G loss: 1.168771]\n",
            "1998 [D loss: 0.545777, acc.: 73.44%] [G loss: 1.074441]\n",
            "1999 [D loss: 0.531447, acc.: 73.44%] [G loss: 1.106978]\n",
            "2000 [D loss: 0.518955, acc.: 74.22%] [G loss: 1.119855]\n",
            "2001 [D loss: 0.525939, acc.: 73.83%] [G loss: 1.102757]\n",
            "2002 [D loss: 0.538785, acc.: 71.19%] [G loss: 1.101392]\n",
            "2003 [D loss: 0.527679, acc.: 71.97%] [G loss: 1.126953]\n",
            "2004 [D loss: 0.530565, acc.: 72.36%] [G loss: 1.104679]\n",
            "2005 [D loss: 0.525924, acc.: 72.66%] [G loss: 1.114909]\n",
            "2006 [D loss: 0.529273, acc.: 72.46%] [G loss: 1.092124]\n",
            "2007 [D loss: 0.528531, acc.: 72.75%] [G loss: 1.050745]\n",
            "2008 [D loss: 0.515733, acc.: 73.44%] [G loss: 1.145958]\n",
            "2009 [D loss: 0.526375, acc.: 72.46%] [G loss: 1.092675]\n",
            "2010 [D loss: 0.513577, acc.: 74.41%] [G loss: 1.098636]\n",
            "2011 [D loss: 0.515457, acc.: 73.83%] [G loss: 1.088777]\n",
            "2012 [D loss: 0.512411, acc.: 74.12%] [G loss: 1.162077]\n",
            "2013 [D loss: 0.521664, acc.: 73.83%] [G loss: 1.069575]\n",
            "2014 [D loss: 0.506180, acc.: 75.00%] [G loss: 1.133165]\n",
            "2015 [D loss: 0.522688, acc.: 72.56%] [G loss: 1.067117]\n",
            "2016 [D loss: 0.512743, acc.: 72.95%] [G loss: 1.111648]\n",
            "2017 [D loss: 0.512137, acc.: 73.63%] [G loss: 1.111686]\n",
            "2018 [D loss: 0.520263, acc.: 72.85%] [G loss: 1.106027]\n",
            "2019 [D loss: 0.523500, acc.: 71.58%] [G loss: 1.127358]\n",
            "2020 [D loss: 0.517213, acc.: 74.32%] [G loss: 1.076887]\n",
            "2021 [D loss: 0.523664, acc.: 73.44%] [G loss: 1.099678]\n",
            "2022 [D loss: 0.517387, acc.: 72.17%] [G loss: 1.116589]\n",
            "2023 [D loss: 0.523142, acc.: 72.95%] [G loss: 1.120537]\n",
            "2024 [D loss: 0.527411, acc.: 72.17%] [G loss: 1.094022]\n",
            "2025 [D loss: 0.519035, acc.: 72.95%] [G loss: 1.109153]\n",
            "2026 [D loss: 0.514237, acc.: 73.24%] [G loss: 1.082307]\n",
            "2027 [D loss: 0.510478, acc.: 73.63%] [G loss: 1.138481]\n",
            "2028 [D loss: 0.524025, acc.: 73.05%] [G loss: 1.113549]\n",
            "2029 [D loss: 0.513181, acc.: 73.24%] [G loss: 1.083463]\n",
            "2030 [D loss: 0.526750, acc.: 71.68%] [G loss: 1.143870]\n",
            "2031 [D loss: 0.521227, acc.: 72.85%] [G loss: 1.141558]\n",
            "2032 [D loss: 0.522892, acc.: 72.66%] [G loss: 1.107808]\n",
            "2033 [D loss: 0.516847, acc.: 72.56%] [G loss: 1.118621]\n",
            "2034 [D loss: 0.533757, acc.: 70.51%] [G loss: 1.089130]\n",
            "2035 [D loss: 0.521999, acc.: 72.66%] [G loss: 1.106185]\n",
            "2036 [D loss: 0.519783, acc.: 72.07%] [G loss: 1.135770]\n",
            "2037 [D loss: 0.519133, acc.: 72.85%] [G loss: 1.086690]\n",
            "2038 [D loss: 0.519273, acc.: 73.24%] [G loss: 1.082854]\n",
            "2039 [D loss: 0.522665, acc.: 73.34%] [G loss: 1.127308]\n",
            "2040 [D loss: 0.529828, acc.: 71.88%] [G loss: 1.104083]\n",
            "2041 [D loss: 0.533721, acc.: 70.41%] [G loss: 1.097965]\n",
            "2042 [D loss: 0.511968, acc.: 74.02%] [G loss: 1.228625]\n",
            "2043 [D loss: 0.520146, acc.: 73.83%] [G loss: 1.169179]\n",
            "2044 [D loss: 0.523179, acc.: 73.34%] [G loss: 1.063861]\n",
            "2045 [D loss: 0.501486, acc.: 73.93%] [G loss: 1.205404]\n",
            "2046 [D loss: 0.509507, acc.: 74.32%] [G loss: 1.112335]\n",
            "2047 [D loss: 0.515876, acc.: 73.83%] [G loss: 1.123461]\n",
            "2048 [D loss: 0.512885, acc.: 74.22%] [G loss: 1.112433]\n",
            "2049 [D loss: 0.514653, acc.: 73.83%] [G loss: 1.124492]\n",
            "2050 [D loss: 0.526423, acc.: 72.07%] [G loss: 1.104296]\n",
            "2051 [D loss: 0.510802, acc.: 73.44%] [G loss: 1.114539]\n",
            "2052 [D loss: 0.528399, acc.: 72.85%] [G loss: 1.093168]\n",
            "2053 [D loss: 0.518677, acc.: 72.85%] [G loss: 1.087227]\n",
            "2054 [D loss: 0.515609, acc.: 73.34%] [G loss: 1.151963]\n",
            "2055 [D loss: 0.521888, acc.: 72.36%] [G loss: 1.109579]\n",
            "2056 [D loss: 0.511500, acc.: 75.20%] [G loss: 1.106864]\n",
            "2057 [D loss: 0.510484, acc.: 73.73%] [G loss: 1.147511]\n",
            "2058 [D loss: 0.509557, acc.: 74.41%] [G loss: 1.108420]\n",
            "2059 [D loss: 0.525368, acc.: 73.83%] [G loss: 1.082785]\n",
            "2060 [D loss: 0.524826, acc.: 72.75%] [G loss: 1.178083]\n",
            "2061 [D loss: 0.521816, acc.: 72.36%] [G loss: 1.164005]\n",
            "2062 [D loss: 0.518601, acc.: 73.83%] [G loss: 1.088929]\n",
            "2063 [D loss: 0.517284, acc.: 72.27%] [G loss: 1.190399]\n",
            "2064 [D loss: 0.517308, acc.: 73.93%] [G loss: 1.098343]\n",
            "2065 [D loss: 0.518294, acc.: 73.63%] [G loss: 1.071360]\n",
            "2066 [D loss: 0.515985, acc.: 73.73%] [G loss: 1.131341]\n",
            "2067 [D loss: 0.507889, acc.: 74.90%] [G loss: 1.099758]\n",
            "2068 [D loss: 0.528030, acc.: 71.39%] [G loss: 1.087837]\n",
            "2069 [D loss: 0.510927, acc.: 73.14%] [G loss: 1.231916]\n",
            "2070 [D loss: 0.510249, acc.: 75.00%] [G loss: 1.120252]\n",
            "2071 [D loss: 0.506946, acc.: 74.32%] [G loss: 1.097038]\n",
            "2072 [D loss: 0.508850, acc.: 72.36%] [G loss: 1.205344]\n",
            "2073 [D loss: 0.518646, acc.: 73.44%] [G loss: 1.094102]\n",
            "2074 [D loss: 0.508783, acc.: 74.90%] [G loss: 1.105294]\n",
            "2075 [D loss: 0.521439, acc.: 72.07%] [G loss: 1.111724]\n",
            "2076 [D loss: 0.521245, acc.: 73.63%] [G loss: 1.069244]\n",
            "2077 [D loss: 0.519030, acc.: 73.24%] [G loss: 1.093604]\n",
            "2078 [D loss: 0.512209, acc.: 73.73%] [G loss: 1.100600]\n",
            "2079 [D loss: 0.514148, acc.: 73.44%] [G loss: 1.111457]\n",
            "2080 [D loss: 0.517703, acc.: 73.54%] [G loss: 1.071240]\n",
            "2081 [D loss: 0.519413, acc.: 72.75%] [G loss: 1.117808]\n",
            "2082 [D loss: 0.514074, acc.: 72.56%] [G loss: 1.112773]\n",
            "2083 [D loss: 0.525306, acc.: 71.78%] [G loss: 1.113258]\n",
            "2084 [D loss: 0.508322, acc.: 74.12%] [G loss: 1.158552]\n",
            "2085 [D loss: 0.506395, acc.: 74.32%] [G loss: 1.103064]\n",
            "2086 [D loss: 0.514950, acc.: 73.44%] [G loss: 1.079987]\n",
            "2087 [D loss: 0.502689, acc.: 74.90%] [G loss: 1.167245]\n",
            "2088 [D loss: 0.509719, acc.: 75.49%] [G loss: 1.081426]\n",
            "2089 [D loss: 0.510104, acc.: 73.83%] [G loss: 1.082365]\n",
            "2090 [D loss: 0.514984, acc.: 73.34%] [G loss: 1.123107]\n",
            "2091 [D loss: 0.507304, acc.: 74.02%] [G loss: 1.131401]\n",
            "2092 [D loss: 0.520380, acc.: 73.63%] [G loss: 1.102185]\n",
            "2093 [D loss: 0.515232, acc.: 73.05%] [G loss: 1.208054]\n",
            "2094 [D loss: 0.507045, acc.: 74.71%] [G loss: 1.115379]\n",
            "2095 [D loss: 0.504556, acc.: 75.20%] [G loss: 1.102570]\n",
            "2096 [D loss: 0.518770, acc.: 72.95%] [G loss: 1.135082]\n",
            "2097 [D loss: 0.505765, acc.: 75.29%] [G loss: 1.057576]\n",
            "2098 [D loss: 0.520187, acc.: 74.02%] [G loss: 1.093487]\n",
            "2099 [D loss: 0.506498, acc.: 73.54%] [G loss: 1.174983]\n",
            "2100 [D loss: 0.505808, acc.: 74.90%] [G loss: 1.150925]\n",
            "2101 [D loss: 0.508151, acc.: 73.93%] [G loss: 1.123237]\n",
            "2102 [D loss: 0.501080, acc.: 74.41%] [G loss: 1.192773]\n",
            "2103 [D loss: 0.505075, acc.: 74.80%] [G loss: 1.145727]\n",
            "2104 [D loss: 0.517983, acc.: 72.66%] [G loss: 1.073891]\n",
            "2105 [D loss: 0.502514, acc.: 73.73%] [G loss: 1.196731]\n",
            "2106 [D loss: 0.517578, acc.: 71.68%] [G loss: 1.131319]\n",
            "2107 [D loss: 0.511544, acc.: 73.83%] [G loss: 1.134267]\n",
            "2108 [D loss: 0.516221, acc.: 71.68%] [G loss: 1.134517]\n",
            "2109 [D loss: 0.516510, acc.: 72.75%] [G loss: 1.109123]\n",
            "2110 [D loss: 0.517007, acc.: 73.63%] [G loss: 1.079156]\n",
            "2111 [D loss: 0.508794, acc.: 74.22%] [G loss: 1.138722]\n",
            "2112 [D loss: 0.522934, acc.: 72.46%] [G loss: 1.122855]\n",
            "2113 [D loss: 0.513597, acc.: 74.12%] [G loss: 1.126912]\n",
            "2114 [D loss: 0.519018, acc.: 73.24%] [G loss: 1.130223]\n",
            "2115 [D loss: 0.508457, acc.: 74.22%] [G loss: 1.149622]\n",
            "2116 [D loss: 0.517430, acc.: 73.24%] [G loss: 1.111708]\n",
            "2117 [D loss: 0.509834, acc.: 73.34%] [G loss: 1.176591]\n",
            "2118 [D loss: 0.514502, acc.: 73.14%] [G loss: 1.115510]\n",
            "2119 [D loss: 0.523781, acc.: 72.27%] [G loss: 1.086085]\n",
            "2120 [D loss: 0.516171, acc.: 71.88%] [G loss: 1.144279]\n",
            "2121 [D loss: 0.513868, acc.: 73.83%] [G loss: 1.125591]\n",
            "2122 [D loss: 0.505859, acc.: 74.90%] [G loss: 1.098094]\n",
            "2123 [D loss: 0.501949, acc.: 72.46%] [G loss: 1.196942]\n",
            "2124 [D loss: 0.518744, acc.: 72.85%] [G loss: 1.111320]\n",
            "2125 [D loss: 0.516419, acc.: 73.14%] [G loss: 1.103561]\n",
            "2126 [D loss: 0.497903, acc.: 74.02%] [G loss: 1.193783]\n",
            "2127 [D loss: 0.519212, acc.: 72.07%] [G loss: 1.136000]\n",
            "2128 [D loss: 0.517172, acc.: 73.14%] [G loss: 1.116054]\n",
            "2129 [D loss: 0.505670, acc.: 73.05%] [G loss: 1.191180]\n",
            "2130 [D loss: 0.516176, acc.: 73.73%] [G loss: 1.104803]\n",
            "2131 [D loss: 0.515843, acc.: 73.05%] [G loss: 1.115070]\n",
            "2132 [D loss: 0.513368, acc.: 73.34%] [G loss: 1.146199]\n",
            "2133 [D loss: 0.513805, acc.: 72.56%] [G loss: 1.131757]\n",
            "2134 [D loss: 0.527011, acc.: 72.17%] [G loss: 1.107690]\n",
            "2135 [D loss: 0.517921, acc.: 72.95%] [G loss: 1.177793]\n",
            "2136 [D loss: 0.513029, acc.: 72.36%] [G loss: 1.166259]\n",
            "2137 [D loss: 0.521932, acc.: 73.34%] [G loss: 1.046337]\n",
            "2138 [D loss: 0.519075, acc.: 71.97%] [G loss: 1.177407]\n",
            "2139 [D loss: 0.527910, acc.: 72.56%] [G loss: 1.088830]\n",
            "2140 [D loss: 0.528583, acc.: 72.36%] [G loss: 1.077284]\n",
            "2141 [D loss: 0.539934, acc.: 71.39%] [G loss: 1.125914]\n",
            "2142 [D loss: 0.525906, acc.: 71.97%] [G loss: 1.119851]\n",
            "2143 [D loss: 0.524965, acc.: 72.27%] [G loss: 1.095157]\n",
            "2144 [D loss: 0.530700, acc.: 73.05%] [G loss: 1.161260]\n",
            "2145 [D loss: 0.518685, acc.: 72.75%] [G loss: 1.116409]\n",
            "2146 [D loss: 0.529947, acc.: 72.85%] [G loss: 1.059279]\n",
            "2147 [D loss: 0.510634, acc.: 74.02%] [G loss: 1.185381]\n",
            "2148 [D loss: 0.525429, acc.: 74.22%] [G loss: 1.096661]\n",
            "2149 [D loss: 0.529847, acc.: 73.24%] [G loss: 1.152969]\n",
            "2150 [D loss: 0.540430, acc.: 72.07%] [G loss: 1.133231]\n",
            "2151 [D loss: 0.528727, acc.: 72.46%] [G loss: 1.162301]\n",
            "2152 [D loss: 0.526136, acc.: 73.34%] [G loss: 1.116936]\n",
            "2153 [D loss: 0.516486, acc.: 72.46%] [G loss: 1.214035]\n",
            "2154 [D loss: 0.525333, acc.: 74.51%] [G loss: 1.142264]\n",
            "2155 [D loss: 0.528067, acc.: 72.17%] [G loss: 1.159121]\n",
            "2156 [D loss: 0.537476, acc.: 72.17%] [G loss: 1.199207]\n",
            "2157 [D loss: 0.515499, acc.: 72.07%] [G loss: 1.183092]\n",
            "2158 [D loss: 0.515394, acc.: 73.05%] [G loss: 1.140636]\n",
            "2159 [D loss: 0.505546, acc.: 73.14%] [G loss: 1.185209]\n",
            "2160 [D loss: 0.524191, acc.: 71.88%] [G loss: 1.154656]\n",
            "2161 [D loss: 0.528373, acc.: 71.29%] [G loss: 1.140653]\n",
            "2162 [D loss: 0.534257, acc.: 71.00%] [G loss: 1.141258]\n",
            "2163 [D loss: 0.537230, acc.: 69.43%] [G loss: 1.120795]\n",
            "2164 [D loss: 0.535401, acc.: 69.53%] [G loss: 1.097900]\n",
            "2165 [D loss: 0.512575, acc.: 73.44%] [G loss: 1.211321]\n",
            "2166 [D loss: 0.513767, acc.: 73.14%] [G loss: 1.143481]\n",
            "2167 [D loss: 0.510673, acc.: 74.41%] [G loss: 1.103650]\n",
            "2168 [D loss: 0.508511, acc.: 73.34%] [G loss: 1.207565]\n",
            "2169 [D loss: 0.508089, acc.: 72.75%] [G loss: 1.130341]\n",
            "2170 [D loss: 0.514854, acc.: 72.56%] [G loss: 1.143473]\n",
            "2171 [D loss: 0.514436, acc.: 72.75%] [G loss: 1.122563]\n",
            "2172 [D loss: 0.523954, acc.: 70.90%] [G loss: 1.137349]\n",
            "2173 [D loss: 0.531552, acc.: 70.51%] [G loss: 1.101212]\n",
            "2174 [D loss: 0.503300, acc.: 73.34%] [G loss: 1.226309]\n",
            "2175 [D loss: 0.524460, acc.: 71.39%] [G loss: 1.127366]\n",
            "2176 [D loss: 0.519337, acc.: 71.39%] [G loss: 1.106790]\n",
            "2177 [D loss: 0.512775, acc.: 71.78%] [G loss: 1.179823]\n",
            "2178 [D loss: 0.523668, acc.: 73.05%] [G loss: 1.092137]\n",
            "2179 [D loss: 0.522562, acc.: 71.19%] [G loss: 1.084906]\n",
            "2180 [D loss: 0.522851, acc.: 71.19%] [G loss: 1.139800]\n",
            "2181 [D loss: 0.511684, acc.: 73.44%] [G loss: 1.105169]\n",
            "2182 [D loss: 0.520586, acc.: 71.88%] [G loss: 1.094244]\n",
            "2183 [D loss: 0.507678, acc.: 72.95%] [G loss: 1.133024]\n",
            "2184 [D loss: 0.509617, acc.: 73.54%] [G loss: 1.115014]\n",
            "2185 [D loss: 0.518062, acc.: 72.36%] [G loss: 1.112014]\n",
            "2186 [D loss: 0.508218, acc.: 73.14%] [G loss: 1.198156]\n",
            "2187 [D loss: 0.509926, acc.: 73.54%] [G loss: 1.132784]\n",
            "2188 [D loss: 0.527917, acc.: 71.09%] [G loss: 1.091836]\n",
            "2189 [D loss: 0.500583, acc.: 74.80%] [G loss: 1.247687]\n",
            "2190 [D loss: 0.513735, acc.: 73.24%] [G loss: 1.112914]\n",
            "2191 [D loss: 0.520772, acc.: 72.27%] [G loss: 1.127690]\n",
            "2192 [D loss: 0.520393, acc.: 71.19%] [G loss: 1.153862]\n",
            "2193 [D loss: 0.506217, acc.: 73.93%] [G loss: 1.169702]\n",
            "2194 [D loss: 0.506156, acc.: 73.83%] [G loss: 1.108379]\n",
            "2195 [D loss: 0.504905, acc.: 73.54%] [G loss: 1.184899]\n",
            "2196 [D loss: 0.520557, acc.: 71.48%] [G loss: 1.113370]\n",
            "2197 [D loss: 0.514111, acc.: 72.75%] [G loss: 1.074726]\n",
            "2198 [D loss: 0.502265, acc.: 74.12%] [G loss: 1.193209]\n",
            "2199 [D loss: 0.510799, acc.: 72.66%] [G loss: 1.148766]\n",
            "2200 [D loss: 0.519419, acc.: 73.14%] [G loss: 1.122512]\n",
            "2201 [D loss: 0.521775, acc.: 72.46%] [G loss: 1.135860]\n",
            "2202 [D loss: 0.511602, acc.: 72.75%] [G loss: 1.099907]\n",
            "2203 [D loss: 0.520346, acc.: 71.39%] [G loss: 1.087650]\n",
            "2204 [D loss: 0.508519, acc.: 72.27%] [G loss: 1.218527]\n",
            "2205 [D loss: 0.510502, acc.: 72.36%] [G loss: 1.129710]\n",
            "2206 [D loss: 0.495875, acc.: 75.39%] [G loss: 1.144768]\n",
            "2207 [D loss: 0.525795, acc.: 70.61%] [G loss: 1.106799]\n",
            "2208 [D loss: 0.531846, acc.: 71.00%] [G loss: 1.111444]\n",
            "2209 [D loss: 0.525893, acc.: 70.90%] [G loss: 1.106178]\n",
            "2210 [D loss: 0.521280, acc.: 72.36%] [G loss: 1.096015]\n",
            "2211 [D loss: 0.516360, acc.: 72.66%] [G loss: 1.133699]\n",
            "2212 [D loss: 0.515771, acc.: 71.48%] [G loss: 1.133515]\n",
            "2213 [D loss: 0.519062, acc.: 72.07%] [G loss: 1.095832]\n",
            "2214 [D loss: 0.510381, acc.: 72.75%] [G loss: 1.221339]\n",
            "2215 [D loss: 0.516922, acc.: 71.68%] [G loss: 1.085938]\n",
            "2216 [D loss: 0.517005, acc.: 72.75%] [G loss: 1.136940]\n",
            "2217 [D loss: 0.514260, acc.: 73.24%] [G loss: 1.218088]\n",
            "2218 [D loss: 0.514714, acc.: 72.17%] [G loss: 1.120308]\n",
            "2219 [D loss: 0.513131, acc.: 74.02%] [G loss: 1.095398]\n",
            "2220 [D loss: 0.505852, acc.: 73.73%] [G loss: 1.216170]\n",
            "2221 [D loss: 0.517578, acc.: 72.66%] [G loss: 1.112724]\n",
            "2222 [D loss: 0.522938, acc.: 71.97%] [G loss: 1.143963]\n",
            "2223 [D loss: 0.508036, acc.: 71.97%] [G loss: 1.173763]\n",
            "2224 [D loss: 0.517482, acc.: 72.66%] [G loss: 1.103085]\n",
            "2225 [D loss: 0.531521, acc.: 72.07%] [G loss: 1.115015]\n",
            "2226 [D loss: 0.526585, acc.: 71.68%] [G loss: 1.189203]\n",
            "2227 [D loss: 0.541707, acc.: 72.07%] [G loss: 1.112206]\n",
            "2228 [D loss: 0.523425, acc.: 73.54%] [G loss: 1.101072]\n",
            "2229 [D loss: 0.513923, acc.: 72.95%] [G loss: 1.219308]\n",
            "2230 [D loss: 0.518404, acc.: 71.29%] [G loss: 1.133309]\n",
            "2231 [D loss: 0.507279, acc.: 73.34%] [G loss: 1.130469]\n",
            "2232 [D loss: 0.525659, acc.: 71.78%] [G loss: 1.203759]\n",
            "2233 [D loss: 0.527536, acc.: 71.39%] [G loss: 1.148232]\n",
            "2234 [D loss: 0.526568, acc.: 71.78%] [G loss: 1.134904]\n",
            "2235 [D loss: 0.512702, acc.: 71.68%] [G loss: 1.229465]\n",
            "2236 [D loss: 0.516144, acc.: 73.54%] [G loss: 1.119563]\n",
            "2237 [D loss: 0.528680, acc.: 71.00%] [G loss: 1.096042]\n",
            "2238 [D loss: 0.508314, acc.: 71.39%] [G loss: 1.233472]\n",
            "2239 [D loss: 0.522777, acc.: 72.27%] [G loss: 1.130753]\n",
            "2240 [D loss: 0.517418, acc.: 72.75%] [G loss: 1.148787]\n",
            "2241 [D loss: 0.513722, acc.: 72.95%] [G loss: 1.196612]\n",
            "2242 [D loss: 0.517550, acc.: 71.97%] [G loss: 1.141673]\n",
            "2243 [D loss: 0.522738, acc.: 72.56%] [G loss: 1.126057]\n",
            "2244 [D loss: 0.518578, acc.: 72.36%] [G loss: 1.251164]\n",
            "2245 [D loss: 0.527468, acc.: 71.19%] [G loss: 1.143372]\n",
            "2246 [D loss: 0.518445, acc.: 72.85%] [G loss: 1.122696]\n",
            "2247 [D loss: 0.518051, acc.: 71.97%] [G loss: 1.235191]\n",
            "2248 [D loss: 0.510586, acc.: 72.85%] [G loss: 1.116592]\n",
            "2249 [D loss: 0.513712, acc.: 73.73%] [G loss: 1.091683]\n",
            "2250 [D loss: 0.519199, acc.: 71.88%] [G loss: 1.184883]\n",
            "2251 [D loss: 0.516448, acc.: 71.78%] [G loss: 1.146248]\n",
            "2252 [D loss: 0.514028, acc.: 73.54%] [G loss: 1.133359]\n",
            "2253 [D loss: 0.527400, acc.: 71.58%] [G loss: 1.181734]\n",
            "2254 [D loss: 0.519070, acc.: 73.54%] [G loss: 1.118926]\n",
            "2255 [D loss: 0.520492, acc.: 72.66%] [G loss: 1.101958]\n",
            "2256 [D loss: 0.513122, acc.: 72.46%] [G loss: 1.174273]\n",
            "2257 [D loss: 0.515829, acc.: 72.85%] [G loss: 1.145103]\n",
            "2258 [D loss: 0.524543, acc.: 72.27%] [G loss: 1.120190]\n",
            "2259 [D loss: 0.506329, acc.: 74.02%] [G loss: 1.211198]\n",
            "2260 [D loss: 0.523748, acc.: 70.90%] [G loss: 1.152263]\n",
            "2261 [D loss: 0.525282, acc.: 71.88%] [G loss: 1.140602]\n",
            "2262 [D loss: 0.517043, acc.: 72.46%] [G loss: 1.200267]\n",
            "2263 [D loss: 0.512730, acc.: 72.36%] [G loss: 1.120075]\n",
            "2264 [D loss: 0.509286, acc.: 73.93%] [G loss: 1.119409]\n",
            "2265 [D loss: 0.503838, acc.: 73.05%] [G loss: 1.212622]\n",
            "2266 [D loss: 0.523414, acc.: 70.80%] [G loss: 1.090763]\n",
            "2267 [D loss: 0.515209, acc.: 72.75%] [G loss: 1.092200]\n",
            "2268 [D loss: 0.504237, acc.: 73.73%] [G loss: 1.186146]\n",
            "2269 [D loss: 0.510079, acc.: 72.95%] [G loss: 1.121799]\n",
            "2270 [D loss: 0.518708, acc.: 72.85%] [G loss: 1.096005]\n",
            "2271 [D loss: 0.517022, acc.: 72.27%] [G loss: 1.193791]\n",
            "2272 [D loss: 0.505759, acc.: 74.51%] [G loss: 1.096563]\n",
            "2273 [D loss: 0.511352, acc.: 72.85%] [G loss: 1.117332]\n",
            "2274 [D loss: 0.518442, acc.: 72.46%] [G loss: 1.142556]\n",
            "2275 [D loss: 0.505539, acc.: 74.32%] [G loss: 1.190853]\n",
            "2276 [D loss: 0.511404, acc.: 73.24%] [G loss: 1.119201]\n",
            "2277 [D loss: 0.511985, acc.: 72.56%] [G loss: 1.143146]\n",
            "2278 [D loss: 0.525769, acc.: 71.09%] [G loss: 1.090737]\n",
            "2279 [D loss: 0.526541, acc.: 71.39%] [G loss: 1.156012]\n",
            "2280 [D loss: 0.528512, acc.: 72.17%] [G loss: 1.128184]\n",
            "2281 [D loss: 0.521687, acc.: 71.19%] [G loss: 1.148547]\n",
            "2282 [D loss: 0.525420, acc.: 70.90%] [G loss: 1.142957]\n",
            "2283 [D loss: 0.525478, acc.: 71.09%] [G loss: 1.105739]\n",
            "2284 [D loss: 0.535228, acc.: 71.29%] [G loss: 1.086365]\n",
            "2285 [D loss: 0.508128, acc.: 72.36%] [G loss: 1.106097]\n",
            "2286 [D loss: 0.528232, acc.: 71.00%] [G loss: 1.114197]\n",
            "2287 [D loss: 0.516815, acc.: 72.85%] [G loss: 1.161319]\n",
            "2288 [D loss: 0.520275, acc.: 72.46%] [G loss: 1.105938]\n",
            "2289 [D loss: 0.528185, acc.: 71.97%] [G loss: 1.116371]\n",
            "2290 [D loss: 0.524670, acc.: 72.56%] [G loss: 1.180186]\n",
            "2291 [D loss: 0.526791, acc.: 70.51%] [G loss: 1.113548]\n",
            "2292 [D loss: 0.530011, acc.: 71.78%] [G loss: 1.134876]\n",
            "2293 [D loss: 0.529827, acc.: 71.97%] [G loss: 1.157770]\n",
            "2294 [D loss: 0.521237, acc.: 71.29%] [G loss: 1.153939]\n",
            "2295 [D loss: 0.526942, acc.: 71.48%] [G loss: 1.087793]\n",
            "2296 [D loss: 0.512279, acc.: 72.75%] [G loss: 1.195070]\n",
            "2297 [D loss: 0.524592, acc.: 72.17%] [G loss: 1.095121]\n",
            "2298 [D loss: 0.510706, acc.: 73.24%] [G loss: 1.103743]\n",
            "2299 [D loss: 0.514422, acc.: 72.95%] [G loss: 1.134270]\n",
            "2300 [D loss: 0.532266, acc.: 70.80%] [G loss: 1.131283]\n",
            "2301 [D loss: 0.522137, acc.: 72.46%] [G loss: 1.110086]\n",
            "2302 [D loss: 0.513443, acc.: 72.75%] [G loss: 1.143728]\n",
            "2303 [D loss: 0.518069, acc.: 71.88%] [G loss: 1.139949]\n",
            "2304 [D loss: 0.512642, acc.: 72.66%] [G loss: 1.099813]\n",
            "2305 [D loss: 0.510926, acc.: 72.85%] [G loss: 1.147942]\n",
            "2306 [D loss: 0.523383, acc.: 70.90%] [G loss: 1.098779]\n",
            "2307 [D loss: 0.525386, acc.: 72.36%] [G loss: 1.133988]\n",
            "2308 [D loss: 0.527501, acc.: 71.78%] [G loss: 1.129442]\n",
            "2309 [D loss: 0.519957, acc.: 71.78%] [G loss: 1.152322]\n",
            "2310 [D loss: 0.519236, acc.: 71.97%] [G loss: 1.090934]\n",
            "2311 [D loss: 0.525746, acc.: 72.66%] [G loss: 1.183471]\n",
            "2312 [D loss: 0.518218, acc.: 72.17%] [G loss: 1.121801]\n",
            "2313 [D loss: 0.508568, acc.: 73.05%] [G loss: 1.150846]\n",
            "2314 [D loss: 0.525910, acc.: 71.48%] [G loss: 1.211947]\n",
            "2315 [D loss: 0.516625, acc.: 72.75%] [G loss: 1.157021]\n",
            "2316 [D loss: 0.528137, acc.: 71.97%] [G loss: 1.121606]\n",
            "2317 [D loss: 0.527447, acc.: 71.97%] [G loss: 1.154254]\n",
            "2318 [D loss: 0.516395, acc.: 72.56%] [G loss: 1.114788]\n",
            "2319 [D loss: 0.530694, acc.: 72.95%] [G loss: 1.143426]\n",
            "2320 [D loss: 0.519586, acc.: 71.97%] [G loss: 1.103949]\n",
            "2321 [D loss: 0.514518, acc.: 73.05%] [G loss: 1.082061]\n",
            "2322 [D loss: 0.524046, acc.: 72.75%] [G loss: 1.179140]\n",
            "2323 [D loss: 0.514249, acc.: 73.44%] [G loss: 1.146657]\n",
            "2324 [D loss: 0.510532, acc.: 73.73%] [G loss: 1.108813]\n",
            "2325 [D loss: 0.523164, acc.: 71.68%] [G loss: 1.143249]\n",
            "2326 [D loss: 0.520858, acc.: 71.58%] [G loss: 1.133230]\n",
            "2327 [D loss: 0.522782, acc.: 71.39%] [G loss: 1.163363]\n",
            "2328 [D loss: 0.517960, acc.: 71.97%] [G loss: 1.141321]\n",
            "2329 [D loss: 0.525398, acc.: 71.78%] [G loss: 1.128614]\n",
            "2330 [D loss: 0.525229, acc.: 71.09%] [G loss: 1.143900]\n",
            "2331 [D loss: 0.521634, acc.: 71.58%] [G loss: 1.171704]\n",
            "2332 [D loss: 0.524481, acc.: 72.17%] [G loss: 1.096722]\n",
            "2333 [D loss: 0.515223, acc.: 72.85%] [G loss: 1.129684]\n",
            "2334 [D loss: 0.525079, acc.: 72.27%] [G loss: 1.083197]\n",
            "2335 [D loss: 0.533921, acc.: 71.29%] [G loss: 1.131069]\n",
            "2336 [D loss: 0.523859, acc.: 72.27%] [G loss: 1.081505]\n",
            "2337 [D loss: 0.519079, acc.: 72.75%] [G loss: 1.120387]\n",
            "2338 [D loss: 0.518692, acc.: 71.97%] [G loss: 1.140926]\n",
            "2339 [D loss: 0.518090, acc.: 71.97%] [G loss: 1.113603]\n",
            "2340 [D loss: 0.529449, acc.: 71.39%] [G loss: 1.110781]\n",
            "2341 [D loss: 0.531527, acc.: 71.48%] [G loss: 1.135818]\n",
            "2342 [D loss: 0.529024, acc.: 71.00%] [G loss: 1.091895]\n",
            "2343 [D loss: 0.523163, acc.: 72.46%] [G loss: 1.116262]\n",
            "2344 [D loss: 0.524105, acc.: 70.51%] [G loss: 1.123730]\n",
            "2345 [D loss: 0.525789, acc.: 72.07%] [G loss: 1.129117]\n",
            "2346 [D loss: 0.521594, acc.: 72.27%] [G loss: 1.152401]\n",
            "2347 [D loss: 0.524787, acc.: 71.48%] [G loss: 1.130384]\n",
            "2348 [D loss: 0.527950, acc.: 71.39%] [G loss: 1.109219]\n",
            "2349 [D loss: 0.531673, acc.: 72.27%] [G loss: 1.134212]\n",
            "2350 [D loss: 0.522799, acc.: 71.78%] [G loss: 1.111556]\n",
            "2351 [D loss: 0.512938, acc.: 72.66%] [G loss: 1.144590]\n",
            "2352 [D loss: 0.529023, acc.: 70.70%] [G loss: 1.131290]\n",
            "2353 [D loss: 0.525301, acc.: 72.66%] [G loss: 1.108941]\n",
            "2354 [D loss: 0.524151, acc.: 72.17%] [G loss: 1.109653]\n",
            "2355 [D loss: 0.522919, acc.: 72.07%] [G loss: 1.094617]\n",
            "2356 [D loss: 0.515518, acc.: 71.97%] [G loss: 1.126609]\n",
            "2357 [D loss: 0.520191, acc.: 72.07%] [G loss: 1.119471]\n",
            "2358 [D loss: 0.513449, acc.: 73.34%] [G loss: 1.096888]\n",
            "2359 [D loss: 0.528259, acc.: 72.07%] [G loss: 1.185927]\n",
            "2360 [D loss: 0.527006, acc.: 71.29%] [G loss: 1.124185]\n",
            "2361 [D loss: 0.530316, acc.: 71.39%] [G loss: 1.089077]\n",
            "2362 [D loss: 0.518537, acc.: 72.85%] [G loss: 1.169222]\n",
            "2363 [D loss: 0.521312, acc.: 73.24%] [G loss: 1.162712]\n",
            "2364 [D loss: 0.522566, acc.: 72.75%] [G loss: 1.114847]\n",
            "2365 [D loss: 0.516703, acc.: 72.36%] [G loss: 1.159033]\n",
            "2366 [D loss: 0.520604, acc.: 71.39%] [G loss: 1.136123]\n",
            "2367 [D loss: 0.507518, acc.: 74.02%] [G loss: 1.143180]\n",
            "2368 [D loss: 0.530110, acc.: 71.88%] [G loss: 1.112393]\n",
            "2369 [D loss: 0.531339, acc.: 71.39%] [G loss: 1.098165]\n",
            "2370 [D loss: 0.531874, acc.: 71.19%] [G loss: 1.116663]\n",
            "2371 [D loss: 0.526287, acc.: 70.70%] [G loss: 1.087399]\n",
            "2372 [D loss: 0.525437, acc.: 72.27%] [G loss: 1.105981]\n",
            "2373 [D loss: 0.520984, acc.: 72.17%] [G loss: 1.100078]\n",
            "2374 [D loss: 0.525294, acc.: 71.88%] [G loss: 1.137516]\n",
            "2375 [D loss: 0.523176, acc.: 71.97%] [G loss: 1.121424]\n",
            "2376 [D loss: 0.518478, acc.: 71.88%] [G loss: 1.107482]\n",
            "2377 [D loss: 0.524782, acc.: 72.46%] [G loss: 1.148346]\n",
            "2378 [D loss: 0.519156, acc.: 71.48%] [G loss: 1.120511]\n",
            "2379 [D loss: 0.513064, acc.: 73.14%] [G loss: 1.114742]\n",
            "2380 [D loss: 0.512635, acc.: 72.36%] [G loss: 1.147216]\n",
            "2381 [D loss: 0.519126, acc.: 71.78%] [G loss: 1.120414]\n",
            "2382 [D loss: 0.519343, acc.: 71.78%] [G loss: 1.110717]\n",
            "2383 [D loss: 0.526505, acc.: 71.78%] [G loss: 1.170500]\n",
            "2384 [D loss: 0.527462, acc.: 71.58%] [G loss: 1.096123]\n",
            "2385 [D loss: 0.521062, acc.: 72.66%] [G loss: 1.103585]\n",
            "2386 [D loss: 0.516618, acc.: 72.27%] [G loss: 1.151882]\n",
            "2387 [D loss: 0.519291, acc.: 71.29%] [G loss: 1.088168]\n",
            "2388 [D loss: 0.518542, acc.: 72.27%] [G loss: 1.097876]\n",
            "2389 [D loss: 0.522139, acc.: 71.39%] [G loss: 1.169156]\n",
            "2390 [D loss: 0.523376, acc.: 71.09%] [G loss: 1.088133]\n",
            "2391 [D loss: 0.528611, acc.: 71.39%] [G loss: 1.141096]\n",
            "2392 [D loss: 0.524455, acc.: 71.09%] [G loss: 1.138188]\n",
            "2393 [D loss: 0.529277, acc.: 71.78%] [G loss: 1.116693]\n",
            "2394 [D loss: 0.524487, acc.: 72.27%] [G loss: 1.123308]\n",
            "2395 [D loss: 0.531780, acc.: 69.73%] [G loss: 1.112640]\n",
            "2396 [D loss: 0.525365, acc.: 71.68%] [G loss: 1.091089]\n",
            "2397 [D loss: 0.526163, acc.: 72.07%] [G loss: 1.148326]\n",
            "2398 [D loss: 0.523756, acc.: 72.36%] [G loss: 1.120946]\n",
            "2399 [D loss: 0.519712, acc.: 71.88%] [G loss: 1.107702]\n",
            "2400 [D loss: 0.529838, acc.: 71.78%] [G loss: 1.160857]\n",
            "2401 [D loss: 0.523337, acc.: 72.07%] [G loss: 1.108966]\n",
            "2402 [D loss: 0.525343, acc.: 72.75%] [G loss: 1.130020]\n",
            "2403 [D loss: 0.525821, acc.: 72.07%] [G loss: 1.116701]\n",
            "2404 [D loss: 0.520679, acc.: 73.63%] [G loss: 1.110597]\n",
            "2405 [D loss: 0.515413, acc.: 73.24%] [G loss: 1.125057]\n",
            "2406 [D loss: 0.515482, acc.: 72.46%] [G loss: 1.127750]\n",
            "2407 [D loss: 0.515982, acc.: 73.54%] [G loss: 1.089627]\n",
            "2408 [D loss: 0.517027, acc.: 72.66%] [G loss: 1.147702]\n",
            "2409 [D loss: 0.526486, acc.: 71.58%] [G loss: 1.155391]\n",
            "2410 [D loss: 0.522152, acc.: 72.85%] [G loss: 1.077318]\n",
            "2411 [D loss: 0.508889, acc.: 73.54%] [G loss: 1.151048]\n",
            "2412 [D loss: 0.521411, acc.: 73.24%] [G loss: 1.072023]\n",
            "2413 [D loss: 0.519925, acc.: 73.14%] [G loss: 1.094371]\n",
            "2414 [D loss: 0.516209, acc.: 74.22%] [G loss: 1.144148]\n",
            "2415 [D loss: 0.520232, acc.: 71.78%] [G loss: 1.107366]\n",
            "2416 [D loss: 0.523048, acc.: 71.48%] [G loss: 1.117061]\n",
            "2417 [D loss: 0.522673, acc.: 72.56%] [G loss: 1.139924]\n",
            "2418 [D loss: 0.513806, acc.: 72.27%] [G loss: 1.157571]\n",
            "2419 [D loss: 0.518715, acc.: 73.34%] [G loss: 1.111114]\n",
            "2420 [D loss: 0.515971, acc.: 72.07%] [G loss: 1.144945]\n",
            "2421 [D loss: 0.530648, acc.: 71.88%] [G loss: 1.125185]\n",
            "2422 [D loss: 0.515891, acc.: 73.24%] [G loss: 1.168318]\n",
            "2423 [D loss: 0.522348, acc.: 72.46%] [G loss: 1.126405]\n",
            "2424 [D loss: 0.513782, acc.: 72.46%] [G loss: 1.109973]\n",
            "2425 [D loss: 0.531305, acc.: 71.19%] [G loss: 1.188827]\n",
            "2426 [D loss: 0.520292, acc.: 71.78%] [G loss: 1.127160]\n",
            "2427 [D loss: 0.517150, acc.: 73.05%] [G loss: 1.121427]\n",
            "2428 [D loss: 0.516328, acc.: 72.75%] [G loss: 1.133495]\n",
            "2429 [D loss: 0.524785, acc.: 72.36%] [G loss: 1.092320]\n",
            "2430 [D loss: 0.519020, acc.: 72.36%] [G loss: 1.131524]\n",
            "2431 [D loss: 0.514246, acc.: 72.85%] [G loss: 1.095964]\n",
            "2432 [D loss: 0.512495, acc.: 73.05%] [G loss: 1.140225]\n",
            "2433 [D loss: 0.507435, acc.: 73.34%] [G loss: 1.108771]\n",
            "2434 [D loss: 0.497625, acc.: 73.54%] [G loss: 1.194992]\n",
            "2435 [D loss: 0.504004, acc.: 73.73%] [G loss: 1.107073]\n",
            "2436 [D loss: 0.519902, acc.: 72.27%] [G loss: 1.124082]\n",
            "2437 [D loss: 0.517942, acc.: 72.85%] [G loss: 1.130359]\n",
            "2438 [D loss: 0.517308, acc.: 71.78%] [G loss: 1.148685]\n",
            "2439 [D loss: 0.513966, acc.: 72.75%] [G loss: 1.128957]\n",
            "2440 [D loss: 0.513628, acc.: 72.95%] [G loss: 1.177804]\n",
            "2441 [D loss: 0.522699, acc.: 72.27%] [G loss: 1.137927]\n",
            "2442 [D loss: 0.514298, acc.: 73.14%] [G loss: 1.119310]\n",
            "2443 [D loss: 0.515771, acc.: 72.75%] [G loss: 1.170007]\n",
            "2444 [D loss: 0.522962, acc.: 71.39%] [G loss: 1.127355]\n",
            "2445 [D loss: 0.511866, acc.: 71.78%] [G loss: 1.140674]\n",
            "2446 [D loss: 0.520504, acc.: 71.48%] [G loss: 1.121681]\n",
            "2447 [D loss: 0.514859, acc.: 72.46%] [G loss: 1.134382]\n",
            "2448 [D loss: 0.520661, acc.: 72.46%] [G loss: 1.087124]\n",
            "2449 [D loss: 0.510557, acc.: 73.34%] [G loss: 1.181784]\n",
            "2450 [D loss: 0.528583, acc.: 70.61%] [G loss: 1.110425]\n",
            "2451 [D loss: 0.519123, acc.: 71.78%] [G loss: 1.102443]\n",
            "2452 [D loss: 0.517887, acc.: 71.78%] [G loss: 1.144390]\n",
            "2453 [D loss: 0.515248, acc.: 72.27%] [G loss: 1.146720]\n",
            "2454 [D loss: 0.519720, acc.: 72.07%] [G loss: 1.111208]\n",
            "2455 [D loss: 0.500876, acc.: 74.61%] [G loss: 1.184731]\n",
            "2456 [D loss: 0.538148, acc.: 71.19%] [G loss: 1.097029]\n",
            "2457 [D loss: 0.521187, acc.: 72.66%] [G loss: 1.156172]\n",
            "2458 [D loss: 0.515550, acc.: 73.54%] [G loss: 1.167905]\n",
            "2459 [D loss: 0.517864, acc.: 73.93%] [G loss: 1.118038]\n",
            "2460 [D loss: 0.514609, acc.: 72.56%] [G loss: 1.101299]\n",
            "2461 [D loss: 0.518720, acc.: 72.36%] [G loss: 1.129495]\n",
            "2462 [D loss: 0.513216, acc.: 72.46%] [G loss: 1.137531]\n",
            "2463 [D loss: 0.521698, acc.: 73.14%] [G loss: 1.073313]\n",
            "2464 [D loss: 0.517848, acc.: 73.05%] [G loss: 1.141760]\n",
            "2465 [D loss: 0.516144, acc.: 72.75%] [G loss: 1.158389]\n",
            "2466 [D loss: 0.505614, acc.: 74.02%] [G loss: 1.145390]\n",
            "2467 [D loss: 0.522692, acc.: 71.58%] [G loss: 1.105829]\n",
            "2468 [D loss: 0.509923, acc.: 72.85%] [G loss: 1.139142]\n",
            "2469 [D loss: 0.526181, acc.: 71.29%] [G loss: 1.117123]\n",
            "2470 [D loss: 0.519536, acc.: 72.95%] [G loss: 1.198043]\n",
            "2471 [D loss: 0.523940, acc.: 73.93%] [G loss: 1.111417]\n",
            "2472 [D loss: 0.517844, acc.: 73.14%] [G loss: 1.123599]\n",
            "2473 [D loss: 0.505411, acc.: 73.34%] [G loss: 1.196729]\n",
            "2474 [D loss: 0.512881, acc.: 73.34%] [G loss: 1.115493]\n",
            "2475 [D loss: 0.524470, acc.: 71.58%] [G loss: 1.107142]\n",
            "2476 [D loss: 0.519881, acc.: 72.27%] [G loss: 1.187064]\n",
            "2477 [D loss: 0.510098, acc.: 73.63%] [G loss: 1.146190]\n",
            "2478 [D loss: 0.523684, acc.: 71.88%] [G loss: 1.140606]\n",
            "2479 [D loss: 0.508600, acc.: 73.34%] [G loss: 1.239587]\n",
            "2480 [D loss: 0.517781, acc.: 73.54%] [G loss: 1.148041]\n",
            "2481 [D loss: 0.527464, acc.: 71.48%] [G loss: 1.118443]\n",
            "2482 [D loss: 0.514341, acc.: 73.05%] [G loss: 1.185513]\n",
            "2483 [D loss: 0.526236, acc.: 72.07%] [G loss: 1.161135]\n",
            "2484 [D loss: 0.518467, acc.: 72.46%] [G loss: 1.148217]\n",
            "2485 [D loss: 0.513395, acc.: 72.36%] [G loss: 1.179197]\n",
            "2486 [D loss: 0.506205, acc.: 72.56%] [G loss: 1.151079]\n",
            "2487 [D loss: 0.517776, acc.: 72.95%] [G loss: 1.190902]\n",
            "2488 [D loss: 0.518323, acc.: 72.17%] [G loss: 1.167360]\n",
            "2489 [D loss: 0.526433, acc.: 70.80%] [G loss: 1.099588]\n",
            "2490 [D loss: 0.527105, acc.: 72.07%] [G loss: 1.118339]\n",
            "2491 [D loss: 0.514020, acc.: 72.56%] [G loss: 1.165875]\n",
            "2492 [D loss: 0.522872, acc.: 72.56%] [G loss: 1.123238]\n",
            "2493 [D loss: 0.536919, acc.: 72.07%] [G loss: 1.160115]\n",
            "2494 [D loss: 0.521946, acc.: 71.48%] [G loss: 1.179722]\n",
            "2495 [D loss: 0.514441, acc.: 72.56%] [G loss: 1.165059]\n",
            "2496 [D loss: 0.521142, acc.: 72.66%] [G loss: 1.166043]\n",
            "2497 [D loss: 0.512484, acc.: 72.85%] [G loss: 1.172312]\n",
            "2498 [D loss: 0.509547, acc.: 73.05%] [G loss: 1.145914]\n",
            "2499 [D loss: 0.508444, acc.: 72.85%] [G loss: 1.136639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'GAN': ['GAN', False, synthesizer.generator]}"
      ],
      "metadata": {
        "id": "-7Jw7yMnt7tQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(17) # used for retestability \n",
        "\n",
        "noise = np.random.normal(0,1, (1170495, 32)) # number of benign samples to be generated\n",
        "\n",
        "[model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "X = generator_model.predict(noise)"
      ],
      "metadata": {
        "id": "Zeva0__roIuX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise1 = np.random.normal(0,1, (70302, 32)) # number of dos hulk samples to be generated\n",
        "noise2 = np.random.normal(0,1, (2922, 32)) # number of ssh patator samples to be generated\n",
        "noise3 = np.random.normal(0,1, (2332, 32)) # number of dos slow loris samples to be generated\n",
        "noise4 = np.random.normal(0,1, (11, 32)) # number of heartbleed samples to be generated\n",
        "noise5 = np.random.normal(0,1, (4, 32)) # number of infiltration samples to be generated\n",
        "\n",
        "[model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "X1 = generator_model.predict(noise1)\n",
        "X2 = generator_model.predict(noise2)\n",
        "X3 = generator_model.predict(noise3)\n",
        "X4 = generator_model.predict(noise4)\n",
        "X5 = generator_model.predict(noise5)\n"
      ],
      "metadata": {
        "id": "9-LwzI3Hu1_C"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe with generated samples, so can be used for evaluation later\n",
        "\n",
        "gen_benign_samples = pd.DataFrame(X, columns=data_columns)"
      ],
      "metadata": {
        "id": "AngnJX-wrs_7"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dosHulk_samples = pd.DataFrame(X1, columns=data_columns)\n",
        "gen_sshPatator_samples = pd.DataFrame(X2, columns=data_columns)\n",
        "gen_dosSlowloris_samples = pd.DataFrame(X3, columns=data_columns)\n",
        "gen_heartbleed_samples = pd.DataFrame(X4, columns=data_columns)\n",
        "gen_infiltration_samples = pd.DataFrame(X5, columns=data_columns)\n"
      ],
      "metadata": {
        "id": "2tbX5ih-vQVF"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_benign_samples.to_csv(r'drive/My Drive/gen_benign_samples.csv')\n"
      ],
      "metadata": {
        "id": "FxGq84cHrv4C"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dosHulk_samples.to_csv(r'drive/My Drive/gen_dosHulk_samples.csv')\n",
        "gen_sshPatator_samples.to_csv(r'drive/My Drive/gen_sshPatator_samples.csv')\n",
        "gen_dosSlowloris_samples.to_csv(r'drive/My Drive/gen_dosSlowloris_samples.csv')\n",
        "gen_heartbleed_samples.to_csv(r'drive/My Drive/gen_heartbleed_samples.csv')\n",
        "gen_infiltration_samples.to_csv(r'drive/My Drive/gen_infiltration_samples.csv')"
      ],
      "metadata": {
        "id": "taj1zK2xvfbj"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Results for Synthetic Data**"
      ],
      "metadata": {
        "id": "f1zjfU__r2uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "df_benign_samples = pd.read_csv('/content/drive/MyDrive/gen_benign_samples.csv')\n",
        "df_dosHulk_samples = pd.read_csv('/content/drive/MyDrive/gen_dosHulk_samples.csv')\n",
        "df_sshPatator_samples = pd.read_csv('/content/drive/MyDrive/gen_sshPatator_samples.csv')\n",
        "df_dosSlowloris_samples = pd.read_csv('/content/drive/MyDrive/gen_dosSlowloris_samples.csv')\n",
        "df_heartbleed_samples = pd.read_csv('/content/drive/MyDrive/gen_heartbleed_samples.csv')\n",
        "df_infiltration_samples = pd.read_csv('/content/drive/MyDrive/gen_infiltration_samples.csv')"
      ],
      "metadata": {
        "id": "778grqLkr0gm"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_benign_samples['label'] = 0\n",
        "df_dosHulk_samples['label'] = 1\n",
        "df_sshPatator_samples['label'] = 5\n",
        "df_dosSlowloris_samples['label'] = 2\n",
        "df_infiltration_samples['label'] = 4\n",
        "df_heartbleed_samples['label'] = 3"
      ],
      "metadata": {
        "id": "2rMcNAYwoFFG"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_benign_samples = df_benign_samples.drop('Unnamed: 0', axis=1)\n",
        "df_dosHulk_samples = df_dosHulk_samples.drop('Unnamed: 0', axis=1)\n",
        "df_sshPatator_samples = df_sshPatator_samples.drop('Unnamed: 0', axis=1)\n",
        "df_dosSlowloris_samples = df_dosSlowloris_samples.drop('Unnamed: 0', axis=1)\n",
        "df_infiltration_samples = df_infiltration_samples.drop('Unnamed: 0', axis=1)\n",
        "df_heartbleed_samples = df_heartbleed_samples.drop('Unnamed: 0', axis=1)"
      ],
      "metadata": {
        "id": "Fhxs-6-roH_p"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all generated samples into one dataframe for classification\n",
        "\n",
        "df_synthetic = pd.concat([df_benign_samples, df_dosHulk_samples, df_sshPatator_samples, df_dosSlowloris_samples, df_infiltration_samples,df_heartbleed_samples])"
      ],
      "metadata": {
        "id": "SYHxJLyVoKDk"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_real[' Label'] = df_real[' Label'].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "v8I4W08SodHe"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_synthetic['label'] = df_synthetic['label'].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "mDL75ETOogDz"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_synthetic['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbRC1tL03W1e",
        "outputId": "1d81f83d-93c0-4270-856e-6c062c58c96d"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1170495\n",
              "1      70302\n",
              "5       2922\n",
              "2       2332\n",
              "3         11\n",
              "4          4\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_real = df_real.drop(' Label', axis=1)\n",
        "y_real = df_real[' Label']\n",
        "X_syn = df_synthetic.drop('label', axis = 1)\n",
        "y_syn = df_synthetic['label']"
      ],
      "metadata": {
        "id": "pnxIyNBJoi5c"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)"
      ],
      "metadata": {
        "id": "Or4NAnSVok7R"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_real = pw.fit_transform(X_real[X_real.columns])"
      ],
      "metadata": {
        "id": "RGHDCyGUomqv"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_real = pd.DataFrame(X_real)"
      ],
      "metadata": {
        "id": "bWefYBOxoovT"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_syn = pw.fit_transform(X_syn[X_syn.columns])"
      ],
      "metadata": {
        "id": "NashLs26oqpH"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_syn = pd.DataFrame(X_syn)"
      ],
      "metadata": {
        "id": "Ok8DDfyiosqM"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_syn.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjJYnsh7wEaY",
        "outputId": "103973da-317d-4004-df8b-9e47102cbb58"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=10, step=1)"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_real.columns= [' Destination Port', ' Bwd Packet Length Std', 'Init_Win_bytes_forward',\n",
        "       'Bwd Packet Length Max', ' Bwd Packet Length Mean',\n",
        "       ' Avg Bwd Segment Size', ' Packet Length Std', ' Fwd IAT Std',\n",
        "       ' Packet Length Mean', ' Average Packet Size']"
      ],
      "metadata": {
        "id": "mcpPG80GouuS"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "y_real_oh = pd.DataFrame(y_real)\n",
        "y_syn_oh = pd.DataFrame(y_syn)\n",
        "\n",
        "y_real_oh = keras.utils.np_utils.to_categorical(y_real_oh)\n",
        "y_syn_oh = keras.utils.np_utils.to_categorical(y_syn_oh)\n"
      ],
      "metadata": {
        "id": "TmKzP24YpbIB"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_syn_train, X_syn_test, y_syn_oh_train, y_syn_oh_test = train_test_split(X_syn, y_syn_oh, test_size = 0.3, random_state = 5)\n"
      ],
      "metadata": {
        "id": "JRReJPjspb0X"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_real_train, X_real_test, y_real_oh_train, y_real_oh_test = train_test_split(X_real, y_real_oh, test_size = 0.3, random_state = 5)"
      ],
      "metadata": {
        "id": "6Ax0htpApevS"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_real_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "CEnlulaG5Z8X",
        "outputId": "56e6839d-8b2a-41aa-bb53-eee299976e15"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Destination Port   Bwd Packet Length Std  Init_Win_bytes_forward  \\\n",
              "464064           -1.749017                1.936163                1.479917   \n",
              "1097216          -0.792884               -0.520538               -0.969102   \n",
              "333738            1.695513               -0.520538                0.703548   \n",
              "144753            1.698815               -0.520538                0.709490   \n",
              "1236301          -0.792884               -0.520538               -0.969102   \n",
              "\n",
              "         Bwd Packet Length Max   Bwd Packet Length Mean  \\\n",
              "464064                1.202573                 0.321036   \n",
              "1097216               0.361529                 0.478626   \n",
              "333738               -1.385957                -1.397908   \n",
              "144753               -1.385957                -1.397908   \n",
              "1236301               0.599662                 0.787546   \n",
              "\n",
              "          Avg Bwd Segment Size   Packet Length Std   Fwd IAT Std  \\\n",
              "464064                0.321036            0.978595      1.615512   \n",
              "1097216               0.478626            0.386678     -0.646342   \n",
              "333738               -1.397908           -0.030412      1.345112   \n",
              "144753               -1.397908           -1.341993     -0.646342   \n",
              "1236301               0.787546            0.720178     -0.646342   \n",
              "\n",
              "          Packet Length Mean   Average Packet Size  \n",
              "464064              0.450685              0.326956  \n",
              "1097216             0.301433              0.299250  \n",
              "333738             -0.789251             -0.808128  \n",
              "144753             -1.638138             -1.656229  \n",
              "1236301             0.573696              0.586568  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aee9a2d5-5906-430d-b9c1-7d698815f1e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Average Packet Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>464064</th>\n",
              "      <td>-1.749017</td>\n",
              "      <td>1.936163</td>\n",
              "      <td>1.479917</td>\n",
              "      <td>1.202573</td>\n",
              "      <td>0.321036</td>\n",
              "      <td>0.321036</td>\n",
              "      <td>0.978595</td>\n",
              "      <td>1.615512</td>\n",
              "      <td>0.450685</td>\n",
              "      <td>0.326956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097216</th>\n",
              "      <td>-0.792884</td>\n",
              "      <td>-0.520538</td>\n",
              "      <td>-0.969102</td>\n",
              "      <td>0.361529</td>\n",
              "      <td>0.478626</td>\n",
              "      <td>0.478626</td>\n",
              "      <td>0.386678</td>\n",
              "      <td>-0.646342</td>\n",
              "      <td>0.301433</td>\n",
              "      <td>0.299250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333738</th>\n",
              "      <td>1.695513</td>\n",
              "      <td>-0.520538</td>\n",
              "      <td>0.703548</td>\n",
              "      <td>-1.385957</td>\n",
              "      <td>-1.397908</td>\n",
              "      <td>-1.397908</td>\n",
              "      <td>-0.030412</td>\n",
              "      <td>1.345112</td>\n",
              "      <td>-0.789251</td>\n",
              "      <td>-0.808128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144753</th>\n",
              "      <td>1.698815</td>\n",
              "      <td>-0.520538</td>\n",
              "      <td>0.709490</td>\n",
              "      <td>-1.385957</td>\n",
              "      <td>-1.397908</td>\n",
              "      <td>-1.397908</td>\n",
              "      <td>-1.341993</td>\n",
              "      <td>-0.646342</td>\n",
              "      <td>-1.638138</td>\n",
              "      <td>-1.656229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236301</th>\n",
              "      <td>-0.792884</td>\n",
              "      <td>-0.520538</td>\n",
              "      <td>-0.969102</td>\n",
              "      <td>0.599662</td>\n",
              "      <td>0.787546</td>\n",
              "      <td>0.787546</td>\n",
              "      <td>0.720178</td>\n",
              "      <td>-0.646342</td>\n",
              "      <td>0.573696</td>\n",
              "      <td>0.586568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aee9a2d5-5906-430d-b9c1-7d698815f1e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aee9a2d5-5906-430d-b9c1-7d698815f1e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aee9a2d5-5906-430d-b9c1-7d698815f1e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_syn_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sqhBbf3MphMX",
        "outputId": "8d5bc760-7b63-445f-e907-845e37dc0d34"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0         1         2         3         4         5         6  \\\n",
              "421050 -0.479392  0.088157 -0.569217  1.123717  1.148197  1.170982  1.252611   \n",
              "272176 -0.507991  0.635543 -0.464054  0.748882  0.714391  0.722093  0.938650   \n",
              "163708 -0.543512 -0.598552 -0.526557  0.290854  0.287121  0.283181  0.451665   \n",
              "569977 -0.563196 -0.110747 -0.591474  1.053950  1.047870  1.104165  1.230707   \n",
              "368906 -0.493640  0.329986 -0.472408  0.514125  0.469069  0.472105  0.554879   \n",
              "\n",
              "               7         8         9  \n",
              "421050  0.348018  1.077692  1.089368  \n",
              "272176  0.334526  0.538509  0.514023  \n",
              "163708 -0.364182  0.078443  0.040428  \n",
              "569977  0.489012  0.860418  0.891264  \n",
              "368906  0.053334  0.364550  0.324505  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ca0da6c-0f8f-41ef-b326-9235a4e47574\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>421050</th>\n",
              "      <td>-0.479392</td>\n",
              "      <td>0.088157</td>\n",
              "      <td>-0.569217</td>\n",
              "      <td>1.123717</td>\n",
              "      <td>1.148197</td>\n",
              "      <td>1.170982</td>\n",
              "      <td>1.252611</td>\n",
              "      <td>0.348018</td>\n",
              "      <td>1.077692</td>\n",
              "      <td>1.089368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272176</th>\n",
              "      <td>-0.507991</td>\n",
              "      <td>0.635543</td>\n",
              "      <td>-0.464054</td>\n",
              "      <td>0.748882</td>\n",
              "      <td>0.714391</td>\n",
              "      <td>0.722093</td>\n",
              "      <td>0.938650</td>\n",
              "      <td>0.334526</td>\n",
              "      <td>0.538509</td>\n",
              "      <td>0.514023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163708</th>\n",
              "      <td>-0.543512</td>\n",
              "      <td>-0.598552</td>\n",
              "      <td>-0.526557</td>\n",
              "      <td>0.290854</td>\n",
              "      <td>0.287121</td>\n",
              "      <td>0.283181</td>\n",
              "      <td>0.451665</td>\n",
              "      <td>-0.364182</td>\n",
              "      <td>0.078443</td>\n",
              "      <td>0.040428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569977</th>\n",
              "      <td>-0.563196</td>\n",
              "      <td>-0.110747</td>\n",
              "      <td>-0.591474</td>\n",
              "      <td>1.053950</td>\n",
              "      <td>1.047870</td>\n",
              "      <td>1.104165</td>\n",
              "      <td>1.230707</td>\n",
              "      <td>0.489012</td>\n",
              "      <td>0.860418</td>\n",
              "      <td>0.891264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368906</th>\n",
              "      <td>-0.493640</td>\n",
              "      <td>0.329986</td>\n",
              "      <td>-0.472408</td>\n",
              "      <td>0.514125</td>\n",
              "      <td>0.469069</td>\n",
              "      <td>0.472105</td>\n",
              "      <td>0.554879</td>\n",
              "      <td>0.053334</td>\n",
              "      <td>0.364550</td>\n",
              "      <td>0.324505</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ca0da6c-0f8f-41ef-b326-9235a4e47574')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ca0da6c-0f8f-41ef-b326-9235a4e47574 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ca0da6c-0f8f-41ef-b326-9235a4e47574');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=10, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#return model"
      ],
      "metadata": {
        "id": "dmfxARhUpkTL"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_real_oh_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Y_lAaL3Hah",
        "outputId": "b5cc99af-b4ce-4870-ca53-5b7927772fe1"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(871753, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "history = model.fit(X_real_train, y_real_oh_train, batch_size = 512, verbose=2, epochs = 50, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIoOrtFUpoNW",
        "outputId": "0c1a40bb-1409-4311-b883-f636268c418f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1363/1363 - 7s - loss: 0.1156 - accuracy: 0.9694 - val_loss: 0.0427 - val_accuracy: 0.9841 - 7s/epoch - 5ms/step\n",
            "Epoch 2/50\n",
            "1363/1363 - 5s - loss: 0.0385 - accuracy: 0.9822 - val_loss: 0.0350 - val_accuracy: 0.9841 - 5s/epoch - 4ms/step\n",
            "Epoch 3/50\n",
            "1363/1363 - 3s - loss: 0.0341 - accuracy: 0.9829 - val_loss: 0.0318 - val_accuracy: 0.9843 - 3s/epoch - 3ms/step\n",
            "Epoch 4/50\n",
            "1363/1363 - 3s - loss: 0.0311 - accuracy: 0.9837 - val_loss: 0.0307 - val_accuracy: 0.9847 - 3s/epoch - 2ms/step\n",
            "Epoch 5/50\n",
            "1363/1363 - 4s - loss: 0.0295 - accuracy: 0.9846 - val_loss: 0.0283 - val_accuracy: 0.9875 - 4s/epoch - 3ms/step\n",
            "Epoch 6/50\n",
            "1363/1363 - 6s - loss: 0.0284 - accuracy: 0.9845 - val_loss: 0.0280 - val_accuracy: 0.9851 - 6s/epoch - 4ms/step\n",
            "Epoch 7/50\n",
            "1363/1363 - 5s - loss: 0.0277 - accuracy: 0.9847 - val_loss: 0.0270 - val_accuracy: 0.9870 - 5s/epoch - 4ms/step\n",
            "Epoch 8/50\n",
            "1363/1363 - 4s - loss: 0.0273 - accuracy: 0.9848 - val_loss: 0.0268 - val_accuracy: 0.9866 - 4s/epoch - 3ms/step\n",
            "Epoch 9/50\n",
            "1363/1363 - 5s - loss: 0.0270 - accuracy: 0.9847 - val_loss: 0.0268 - val_accuracy: 0.9864 - 5s/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "1363/1363 - 3s - loss: 0.0268 - accuracy: 0.9850 - val_loss: 0.0268 - val_accuracy: 0.9817 - 3s/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "1363/1363 - 3s - loss: 0.0266 - accuracy: 0.9852 - val_loss: 0.0268 - val_accuracy: 0.9830 - 3s/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "1363/1363 - 3s - loss: 0.0264 - accuracy: 0.9851 - val_loss: 0.0259 - val_accuracy: 0.9873 - 3s/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "1363/1363 - 3s - loss: 0.0264 - accuracy: 0.9852 - val_loss: 0.0255 - val_accuracy: 0.9872 - 3s/epoch - 2ms/step\n",
            "Epoch 14/50\n",
            "1363/1363 - 3s - loss: 0.0262 - accuracy: 0.9853 - val_loss: 0.0258 - val_accuracy: 0.9866 - 3s/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "1363/1363 - 3s - loss: 0.0260 - accuracy: 0.9853 - val_loss: 0.0254 - val_accuracy: 0.9864 - 3s/epoch - 2ms/step\n",
            "Epoch 16/50\n",
            "1363/1363 - 3s - loss: 0.0259 - accuracy: 0.9855 - val_loss: 0.0267 - val_accuracy: 0.9859 - 3s/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "1363/1363 - 4s - loss: 0.0258 - accuracy: 0.9854 - val_loss: 0.0270 - val_accuracy: 0.9856 - 4s/epoch - 3ms/step\n",
            "Epoch 18/50\n",
            "1363/1363 - 3s - loss: 0.0258 - accuracy: 0.9856 - val_loss: 0.0268 - val_accuracy: 0.9825 - 3s/epoch - 2ms/step\n",
            "Epoch 19/50\n",
            "1363/1363 - 5s - loss: 0.0258 - accuracy: 0.9854 - val_loss: 0.0275 - val_accuracy: 0.9826 - 5s/epoch - 3ms/step\n",
            "Epoch 20/50\n",
            "1363/1363 - 4s - loss: 0.0257 - accuracy: 0.9855 - val_loss: 0.0261 - val_accuracy: 0.9858 - 4s/epoch - 3ms/step\n",
            "Epoch 21/50\n",
            "1363/1363 - 3s - loss: 0.0258 - accuracy: 0.9855 - val_loss: 0.0257 - val_accuracy: 0.9823 - 3s/epoch - 2ms/step\n",
            "Epoch 22/50\n",
            "1363/1363 - 3s - loss: 0.0256 - accuracy: 0.9854 - val_loss: 0.0253 - val_accuracy: 0.9874 - 3s/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9856 - val_loss: 0.0256 - val_accuracy: 0.9875 - 3s/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9856 - val_loss: 0.0250 - val_accuracy: 0.9874 - 3s/epoch - 2ms/step\n",
            "Epoch 25/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9855 - val_loss: 0.0252 - val_accuracy: 0.9865 - 3s/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9856 - val_loss: 0.0251 - val_accuracy: 0.9881 - 3s/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "1363/1363 - 3s - loss: 0.0254 - accuracy: 0.9857 - val_loss: 0.0260 - val_accuracy: 0.9862 - 3s/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9855 - val_loss: 0.0250 - val_accuracy: 0.9843 - 3s/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "1363/1363 - 3s - loss: 0.0255 - accuracy: 0.9856 - val_loss: 0.0251 - val_accuracy: 0.9870 - 3s/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "1363/1363 - 3s - loss: 0.0253 - accuracy: 0.9854 - val_loss: 0.0251 - val_accuracy: 0.9835 - 3s/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "1363/1363 - 3s - loss: 0.0253 - accuracy: 0.9857 - val_loss: 0.0247 - val_accuracy: 0.9875 - 3s/epoch - 2ms/step\n",
            "Epoch 32/50\n",
            "1363/1363 - 3s - loss: 0.0252 - accuracy: 0.9859 - val_loss: 0.0248 - val_accuracy: 0.9881 - 3s/epoch - 2ms/step\n",
            "Epoch 33/50\n",
            "1363/1363 - 3s - loss: 0.0252 - accuracy: 0.9858 - val_loss: 0.0249 - val_accuracy: 0.9869 - 3s/epoch - 2ms/step\n",
            "Epoch 34/50\n",
            "1363/1363 - 3s - loss: 0.0251 - accuracy: 0.9860 - val_loss: 0.0259 - val_accuracy: 0.9830 - 3s/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "1363/1363 - 3s - loss: 0.0251 - accuracy: 0.9860 - val_loss: 0.0247 - val_accuracy: 0.9866 - 3s/epoch - 2ms/step\n",
            "Epoch 36/50\n",
            "1363/1363 - 3s - loss: 0.0251 - accuracy: 0.9857 - val_loss: 0.0251 - val_accuracy: 0.9831 - 3s/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "1363/1363 - 3s - loss: 0.0250 - accuracy: 0.9856 - val_loss: 0.0249 - val_accuracy: 0.9873 - 3s/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "1363/1363 - 3s - loss: 0.0250 - accuracy: 0.9859 - val_loss: 0.0246 - val_accuracy: 0.9877 - 3s/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "1363/1363 - 3s - loss: 0.0250 - accuracy: 0.9860 - val_loss: 0.0251 - val_accuracy: 0.9864 - 3s/epoch - 2ms/step\n",
            "Epoch 40/50\n",
            "1363/1363 - 4s - loss: 0.0249 - accuracy: 0.9858 - val_loss: 0.0248 - val_accuracy: 0.9869 - 4s/epoch - 3ms/step\n",
            "Epoch 41/50\n",
            "1363/1363 - 3s - loss: 0.0250 - accuracy: 0.9859 - val_loss: 0.0247 - val_accuracy: 0.9871 - 3s/epoch - 2ms/step\n",
            "Epoch 42/50\n",
            "1363/1363 - 3s - loss: 0.0248 - accuracy: 0.9860 - val_loss: 0.0254 - val_accuracy: 0.9863 - 3s/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "1363/1363 - 3s - loss: 0.0249 - accuracy: 0.9860 - val_loss: 0.0246 - val_accuracy: 0.9879 - 3s/epoch - 2ms/step\n",
            "Epoch 44/50\n",
            "1363/1363 - 3s - loss: 0.0248 - accuracy: 0.9862 - val_loss: 0.0250 - val_accuracy: 0.9787 - 3s/epoch - 2ms/step\n",
            "Epoch 45/50\n",
            "1363/1363 - 3s - loss: 0.0249 - accuracy: 0.9859 - val_loss: 0.0259 - val_accuracy: 0.9828 - 3s/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "1363/1363 - 3s - loss: 0.0248 - accuracy: 0.9859 - val_loss: 0.0255 - val_accuracy: 0.9827 - 3s/epoch - 2ms/step\n",
            "Epoch 47/50\n",
            "1363/1363 - 3s - loss: 0.0247 - accuracy: 0.9861 - val_loss: 0.0248 - val_accuracy: 0.9788 - 3s/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "1363/1363 - 3s - loss: 0.0249 - accuracy: 0.9859 - val_loss: 0.0246 - val_accuracy: 0.9875 - 3s/epoch - 2ms/step\n",
            "Epoch 49/50\n",
            "1363/1363 - 3s - loss: 0.0247 - accuracy: 0.9862 - val_loss: 0.0246 - val_accuracy: 0.9869 - 3s/epoch - 2ms/step\n",
            "Epoch 50/50\n",
            "1363/1363 - 3s - loss: 0.0248 - accuracy: 0.9859 - val_loss: 0.0250 - val_accuracy: 0.9861 - 3s/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_real_test, y_real_oh_test ,verbose=2, batch_size= 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDrQwdtspqqZ",
        "outputId": "8fdc6afa-f2ab-4385-c834-64c7d45ed433"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "730/730 - 1s - loss: 0.2510 - accuracy: 0.9388 - 874ms/epoch - 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_probs= model.predict(X_real_test)\n",
        "yhat_classes= np.argmax(yhat_probs, axis=1)"
      ],
      "metadata": {
        "id": "IerDOcBKpsy4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_classes = pd.DataFrame(yhat_classes)\n",
        "yhat_classes.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK4tIvbSpuhd",
        "outputId": "d0b8500d-d1f3-48c8-d1a1-e3d122dc10b3"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    347245\n",
              "1     25395\n",
              "2       525\n",
              "5       444\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_rounded = np.argmax(y_real_oh_test, axis=1)"
      ],
      "metadata": {
        "id": "1ATFrOOwpwPg"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test_rounded, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "precision = precision_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('Precision: %f' % precision)\n",
        "recall = recall_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('Recall: %f' % recall)\n",
        "f1 = f1_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apSTE21OpyXW",
        "outputId": "7ecc97c4-49d4-4176-9d95-027bd0265ee5"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.986087\n",
            "Precision: 0.987958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.986087\n",
            "F1 score: 0.986331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "# confusion matrix\n",
        "CM = confusion_matrix(y_test_rounded, yhat_classes)\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(6, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "CQia8EO0p1JF",
        "outputId": "94c04991-5cf7-484f-84fa-df77c55cbbc8"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHukGW9XvfYQ",
        "outputId": "62bff768-2d98-4644-f927-0c599019de27"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "874288"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_syn_oh_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfB8Af4321y3",
        "outputId": "6ce13456-b330-4bb6-bcb4-161433889150"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(874288, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_syn_train, y_syn_oh_train, batch_size = 512, epochs = 50, verbose = 2, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SggSccSap6OQ",
        "outputId": "67dad16e-7368-4236-c87a-53c10234add8"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1363/1363 - 4s - loss: 0.3065 - accuracy: 0.9360 - val_loss: 0.2536 - val_accuracy: 0.9404 - 4s/epoch - 3ms/step\n",
            "Epoch 2/50\n",
            "1363/1363 - 3s - loss: 0.2536 - accuracy: 0.9390 - val_loss: 0.2485 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 3/50\n",
            "1363/1363 - 3s - loss: 0.2512 - accuracy: 0.9390 - val_loss: 0.2472 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 4/50\n",
            "1363/1363 - 3s - loss: 0.2504 - accuracy: 0.9390 - val_loss: 0.2470 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 5/50\n",
            "1363/1363 - 3s - loss: 0.2498 - accuracy: 0.9390 - val_loss: 0.2463 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 6/50\n",
            "1363/1363 - 3s - loss: 0.2495 - accuracy: 0.9390 - val_loss: 0.2460 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 7/50\n",
            "1363/1363 - 3s - loss: 0.2493 - accuracy: 0.9390 - val_loss: 0.2458 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 8/50\n",
            "1363/1363 - 3s - loss: 0.2492 - accuracy: 0.9390 - val_loss: 0.2463 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 9/50\n",
            "1363/1363 - 3s - loss: 0.2491 - accuracy: 0.9390 - val_loss: 0.2457 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 10/50\n",
            "1363/1363 - 3s - loss: 0.2490 - accuracy: 0.9390 - val_loss: 0.2460 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "1363/1363 - 3s - loss: 0.2489 - accuracy: 0.9390 - val_loss: 0.2456 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "1363/1363 - 3s - loss: 0.2489 - accuracy: 0.9390 - val_loss: 0.2457 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "1363/1363 - 3s - loss: 0.2488 - accuracy: 0.9390 - val_loss: 0.2458 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 14/50\n",
            "1363/1363 - 3s - loss: 0.2488 - accuracy: 0.9390 - val_loss: 0.2460 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "1363/1363 - 3s - loss: 0.2487 - accuracy: 0.9390 - val_loss: 0.2454 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 16/50\n",
            "1363/1363 - 3s - loss: 0.2487 - accuracy: 0.9390 - val_loss: 0.2453 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "1363/1363 - 3s - loss: 0.2487 - accuracy: 0.9390 - val_loss: 0.2455 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 18/50\n",
            "1363/1363 - 3s - loss: 0.2486 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 19/50\n",
            "1363/1363 - 3s - loss: 0.2486 - accuracy: 0.9390 - val_loss: 0.2456 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 20/50\n",
            "1363/1363 - 3s - loss: 0.2486 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 21/50\n",
            "1363/1363 - 3s - loss: 0.2485 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 22/50\n",
            "1363/1363 - 3s - loss: 0.2485 - accuracy: 0.9390 - val_loss: 0.2451 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2459 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 25/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2451 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "1363/1363 - 3s - loss: 0.2484 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2455 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2453 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 32/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2453 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 33/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 34/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2459 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 36/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 40/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 41/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 42/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2451 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2450 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 44/50\n",
            "1363/1363 - 3s - loss: 0.2483 - accuracy: 0.9390 - val_loss: 0.2448 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 45/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 47/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2448 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 49/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2449 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n",
            "Epoch 50/50\n",
            "1363/1363 - 3s - loss: 0.2482 - accuracy: 0.9390 - val_loss: 0.2448 - val_accuracy: 0.9404 - 3s/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trained with synthetic data so now test with real data\n",
        "score, acc = model.evaluate(X_syn_test, y_syn_oh_test ,verbose=2, batch_size= 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVfn2ZOep8Wo",
        "outputId": "8418d0ab-0e28-4d3b-d6c9-96110fc048aa"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "731/731 - 1s - loss: 0.2464 - accuracy: 0.9395 - 900ms/epoch - 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_probs= model.predict(X_syn_test)\n",
        "yhat_classes= np.argmax(yhat_probs, axis=1)"
      ],
      "metadata": {
        "id": "lt0a0Uypp97D"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_classes = pd.DataFrame(yhat_classes)\n",
        "yhat_classes.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54Rt7FwYp_kK",
        "outputId": "ffa37e55-1d81-4084-ceed-f9c87a7cc902"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    373820\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_rounded = np.argmax(y_syn_oh_test, axis=1)"
      ],
      "metadata": {
        "id": "gj9VUobEqBGS"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_rounded, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "precision = precision_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('Precision: %f' % precision)\n",
        "recall = recall_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('Recall: %f' % recall)\n",
        "f1 = f1_score(y_test_rounded, yhat_classes, average='weighted')\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcK1QlukqCca",
        "outputId": "2da4a519-4cba-4f48-ae89-8849d8f44131"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.939498\n",
            "Precision: 0.882656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.939498\n",
            "F1 score: 0.910190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "#print(cm)\n",
        "\n",
        "# confusion matrix\n",
        "CM = confusion_matrix(y_test_rounded, yhat_classes)\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CM ,  figsize=(6, 8))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "Ejk-fcvTqEmS",
        "outputId": "2715b9e4-8a1c-49c5-924d-71ec6553ffc6"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "869iEOVZtqBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}